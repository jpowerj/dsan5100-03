---
title: "Week 12: Fancy Hypothesis Testing"
date: "Thursday, November 9, 2023"
date-format: full
lecnum: 12
categories:
  - "Class Sessions"
subtitle: "*DSAN 5100: Probabilistic Modeling and Statistical Computing*<br>Section 03"
author: "Jeff Jacobs"
institute: "<a href='mailto:jj1088@georgetown.edu' target='_blank'>`jj1088@georgetown.edu`</a>"
bibliography: "../_DSAN5100.bib"
execute:
  echo: true
format:
  revealjs:
    cache: false
    footer: "DSAN 5100-03 Week 12: Fancy Hypothesis Testing"
    output-file: slides.html
    df-print: kable
    code-fold: true
    html-math-method: mathjax
    scrollable: true
    slide-number: true
    section-divs: false
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar'><span style='position: absolute; left: 8; padding-left: 8px;'><a href='./index.html'>&larr; Return to Notes</a></span><ul class='menu'></ul></div>"
      scale: 0.5
    number-sections: false
    footnotes-hover: true
    tbl-cap-location: bottom
    theme: [default, "../_style-slides.scss"]
    revealjs-plugins:
      - simplemenu
    include-in-header:
      text: "<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css' integrity='sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65' crossorigin='anonymous'><link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css'><link href=\"https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined\" rel=\"stylesheet\" />"
  html:
    output-file: index.html
    cache: false
    html-math-method: mathjax
    number-sections: false
    code-fold: true
    footnotes-hover: true
    tbl-cap-location: bottom
---


::: {.content-visible unless-format="revealjs"}

<center>
<a class="h2" href="./slides.html" target="_blank">Open slides in new window &rarr;</a>
</center>

:::

# Schedule {.smaller .small-title .crunch-title data-name="Schedule"}

::: {.hidden}

```{r}
#| label: r-source-globals
source("../_globals.r")
```

{{< include ../_globals-tex.qmd >}}

:::

Today's Planned Schedule:

| | Start | End | Topic | Recording |
|:- |:- |:- |:- |:-:|
| **Lecture** | 12:30pm | 1:00pm | <a href="#method-of-moments">Types of Hypotheses &rarr;</a> | <a href="../recordings/recording-w12-1.html" target="_blank"><i class="bi bi-film"></i></a> |
| | 1:00pm | 1:30pm | <a href='#the-chi-squared-test-of-independence'>Chi-Squared Test of Independence &rarr;</a> | <a href='../recordings/recording-w12-2.html' target='_blank'><i class='bi bi-film'></i></a> |
| | 1:30pm | 2:00pm | <a href='#testing-hypotheses-about-proportions'>Testing Hypotheses About Proportions &rarr;</a> | <a href='../recordings/recording-w12-3.html' target='_blank'><i class='bi bi-film'></i></a> |
| **Break!** | 2:00pm | 2:10pm | | |
| | 2:10pm | 2:40pm | <a href="#bias-variance-tradeoff-introduction">Hypothesis Testing Lab &rarr;</a> | <a href="../recordings/recording-w11-2.html" target="_blank"><i class="bi bi-film"></i></a> |
| | 2:40pm | 3:00pm | Student Presentation | |

: {tbl-colwidths="[10,10,10,60,10]"}

# Types of Hypotheses {data-stack-name="Hypotheses"}

## Differences vs. Ratios {.smaller .crunch-title .crunch-ul .crunch-math}

* **Setting**: We have a sample $\mathbf{X}_1$ taken from a population $P_1$ and a sample $\mathbf{X}_2$ taken from a population $P_2$.
* We care about some variable $v$ that we measured as part of our sampling: in particular, we care about $\mu^{(v)}_1$, the mean of this variable in **population 1**, and $\mu^{(v)}_2$, the mean of this variable in **population 2**^[From here onwards I drop the $(v)$ superscript for brevity, but remember that we're always talking about $\mu^{(v)}_i$: the population mean of a **particular variable** $v$ in population $i$!]
* **Goal**: Test whether the **mean** of $v$ in population 1 is **equal to** the mean of $v$ in population 2. Formally, we test the hypothesis $\mathcal{H}$:

$$
\mathcal{H}: \mu_1 = \mu_2
$$

* It is ambiguous what **test statistic** we should use to check this(!), since

    $$
    \begin{align*}
    \mu_1 = \mu_2 &\iff \mu_1 - \mu_2 = 0 \\
    \mu_1 = \mu_2 &\iff \frac{\mu_1}{\mu_2} = 0
    \end{align*}
    $$

    (where we assume, for the second equivalence, that $\mu_2 \neq 0$)

## The Difference, In Math {.smaller .crunch-ul .crunch-code .crunch-math .crunch-quarto-layout-panel}

::: {layout="[1,1]" layout-valign="top"}

::: {#test-stats-left}

* Our test statistic is some function $t(\mu_1, \mu_2)$.
* Let $t_d(\mu_1, \mu_2) = \mu_1 - \mu_2$
* Let $t_r(\mu_1, \mu_2) = \frac{\mu_1}{\mu_2}$
* How **sensitive** are these two ways of defining $t$ to changes in the individual terms?

    $$
    t'_d(\mu_2) = \frac{\partial t^-(\mu_1, \mu_2)}{\partial \mu_1} = 1,
    $$

    whereas

    $$
    t'_r(\mu_2) = \frac{\partial t^\div(\mu_1, \mu_2)}{\partial \mu_1} = \frac{1}{\mu_2}
    $$

:::
::: {#test-stats-right}

```{r}
#| label: diff-plot
#| fig-height: 4
library(tidyverse)
library(latex2exp)
my_const <- function(x) 1
my_ratio <- function(x) 1/x
#data_df <- data_df |> mutate(
#  z = my_diff(x, y)
#)
#print(data_df)
x_label <- TeX("$\\mu_2$")
td_title <- TeX("Stability of $t_d$")
td_label <- TeX("$t'_d(\\mu_2)$")
base_plot <- ggplot(data=data.frame(x=c(-5,5)), aes(x=x)) +
  dsan_theme("quarter") +
  labs(
    x = x_label
  )
diff_plot <- base_plot + stat_function(
    fun = my_const,
    linewidth = g_linewidth
  ) + labs(
    title = td_title,
    y = td_label
  )
diff_plot
```

```{r}
#| label: ratio-plot
#| fig-height: 4
tr_title <- TeX("Stability of $t_r$")
tr_label <- TeX("$t'_r(\\mu_2)$")
ratio_plot <- base_plot + stat_function(
    fun = my_ratio,
    linewidth = g_linewidth
  ) + labs(
    title = tr_title,
    y = tr_label
  )
ratio_plot
```

:::
:::

## The Takeaway

* In scenarios where $\mu$ values are **far from zero**, both behave similarly
* But in scenarios where $\mu$ values are **close to zero**, need to be careful about using $t_r(\mu_1, \mu_2) = \frac{\mu_1}{\mu_2}$!
* If $\mu_2$ could feasibly **be** zero... $t_r(\mu_1, \mu_2) = ðŸ’€ðŸ˜µðŸ’€$

# The $t$-Test {data-stack-name="t-Test"}

## Moving From Known $\rightarrow$ Unknown $\sigma$ {.smaller}

* Last week: $z$-test for scenario where we (somehow) **know** population variance $\sigma$
* This week: More realistic case where we **don't** know $\sigma$, so we **estimate it** from our sample $\mathbf{X}$: $\widehat{\sigma^2} = s^2_{\mathbf{X}}$
* This means we have **recursive uncertainty** in our estimates!

$$
\begin{align*}
\widehat{\mu}(\mathbf{X}) = f(\mathbf{X}) &= \frac{1}{N}\sum_{i=1}^{N}X_i \\
\widehat{\sigma^2}(\mathbf{X}) = g(\mathbf{X}, \widehat{\mu}(\mathbf{X})) &= \frac{1}{N}\sum_{i=1}^N (X_i - \boxed{\widehat{\mu}(\mathbf{X})})^2 \\
&= \frac{1}{N}\sum_{i=1}^N\left(X_i - \boxed{\frac{1}{N}\sum_{j=1}^NX_j}\right)^2
\end{align*}
$$

## Errors Now Make Our Estimates *Exponentially Worse*(!) {.smaller .title-10 .crunch-title .crunch-ul}

* Imagine that when calculating $\widehat{\mu} = \frac{1}{N}\sum_{i=1}^nX_i$ (recall that this is an **unbiased** estimator for $\mu$), we accidentally add $\varepsilon$ to every value $X_i$, so that we instead compute $\widetilde{\mu} = \frac{1}{N}\sum_{i=1}^n(X_i+\varepsilon) = \widehat{\mu} + \varepsilon$
* This means our estimate of $\mu$ is now **biased** by some amount $\varepsilon$: while $\mathbb{E}[\widehat{\mu}] = \mu$, $\mathbb{E}[\widetilde{\mu}] = \mu + {\color{red}\varepsilon}$
* How does this affect subsequent estimates of the **variance** $\widehat{\sigma^2}$? Even if we use an **unbiased** estimator $\widehat{\sigma^2}$, so that $\mathbb{E}[\widehat{\sigma^2}] = \sigma^2$, we get something that looks like the following (sweeping some details under the rug):

$$
\mathbb{E}[\widehat{\sigma^2}] = \cdots = \sigma^2 + \mathbb{E}[\varepsilon^2] - 2\varepsilon\mathbb{E}[X_i-\mu] = \sigma^2 + {\color{red}\varepsilon^2}
$$

* While our estimate of the **mean** was off by ${\color{red}\varepsilon}$, our estimate of the **variance** is off by ${\color{red}\varepsilon^2}$!
* Taking square root to obtain the standard deviation $\widehat{\sigma}$ doesn't "fix" this, either, since $\sqrt{\sigma^2 + \varepsilon^2} \neq \sigma + \varepsilon$ in general (e.g., $\sqrt{2^2 + 3^2} = \sqrt{4+9} = \sqrt{13} \neq 2 + 3 = 5$)

## $z$-Test $\rightarrow$ $t$-Test {.smaller .crunch-title .crunch-ul .crunch-quarto-layout-panel}

::: {layout="[2,3]"}

::: {#t-test-left}

<center>
**The Takeaway:**
</center>

* When we **know** $\sigma^2$ but we **estimate** $\mu$ from a sample, we represent our uncertainty via test statistic $z \sim \mathcal{N}(\widehat{\mu}, \sigma^2)$
* When we estimate **both** $\mu$ and $\sigma^2$ from a sample, we use a test statistic $t$ with a *wider* **"Student's $t$"** Distribution in place of the **Normal** Distribution: $t \sim \mathcal{T}(\widehat{\mu}, \widehat{\sigma}^2, N)$
* As $N \rightarrow \infty$, $\mathcal{T}_N(\widehat{\mu},\widehat{\sigma}^2,N) \rightarrow \mathcal{N}(\widehat{\mu},\widehat{\sigma}^2)$

:::
::: {#t-test-right}

```{r}
#| label: normal-vs-t-dist
#| fig-cap: A comparison of the two distributions, showing how the Student's t Distribution has **greater variance**, representing the **greater uncertainty** when estimating two population parameters instead of just one
library(tidyverse)
my_normal <- function(x) dnorm(x)
my_st <- function(x) dt(x, 2)
ggplot(data=data.frame(x=c(-3,3)), aes(x=x)) +
  stat_function(
    aes(color='norm'),
    fun=my_normal,
    linewidth = g_linewidth
  ) +
  stat_function(
    aes(color='st'),
    fun=my_st,
    linewidth = g_linewidth
  ) +
  dsan_theme("half") +
  scale_color_manual(
    "Distribution",
    values=c('norm'=cbPalette[1],'st'=cbPalette[2]),
    labels=c('norm'="Standard Normal",'st'="Student's t (df=2)")
  ) +
  remove_legend_title() +
  labs(
    title = "Standard Normal vs. Student's t Distribution"
  )
```

:::
:::

## In R {.smaller .crunch-title}

* Let's create a tibble containing samples from **two populations**

```{r}
#| label: two-pops-tibble
library(tidyverse)
library(infer)
nl_height_mean <- 182.535
nl_height_sd <- 8
yemen_height_mean <- 159.887
yemen_height_sd <- 8
N <- 100
nl_sample <- rnorm(N, mean=nl_height_mean, sd = nl_height_sd)
nl_df <- tibble(height=nl_sample, Country="Netherlands")
yemen_sample <- rnorm(N, mean=yemen_height_mean, sd = yemen_height_sd)
yemen_df <- tibble(height=yemen_sample, Country="Yemen")
data_df <- bind_rows(nl_df, yemen_df)
ggplot(data_df, aes(x=height, fill=Country)) +
  geom_density(alpha=0.5) + 
  dsan_theme() +
  #xlim(150,200) +
  labs(
    title = "Mean Heights: Yemen (N=100) vs. Netherlands (N=100)",
    x = "Height (cm)",
    y = "Sample Probability Density"
  ) +
  scale_fill_manual("Country", values=c('Netherlands'=cbPalette[1], 'Yemen'=cbPalette[2]))
```

* $H_0: \mu_{\text{NL}} - \mu_{\text{Yem}} = 0$, $H_A: \mu_{\text{NL}} - \mu_{\text{Yem}} > 0$, $t = \overline{h}_{\text{NL}} - \overline{h}_{\text{Yem}}$

## The `infer` Package {.smaller}

* It's easy to find resources on built-in `t.test()`, harder to find resources on <a href='https://infer.netlify.app/reference/t_test.html' target='_blank'>newer, tidyverse-based `t_test()`</a>! ðŸ˜‰
* Syntax is `t_test(df, formula, order, alternative)`
* `order = c(pop1, pop2)` $\rightarrow H_0: \mu_1 - \mu_2 = 0$
* `alternative = {"two-sided", "greater", "less"}`
* `formula` slightly trickier, but tldr is:

    ```
    variable you're computing means for ~ variable splitting df into two populations
    ```

```{r}
#| label: tidy-t-test
#| code-fold: show
data_df |> t_test(formula = height ~ Country, order = c("Netherlands", "Yemen"), alternative = "greater")
```

# The Chi-Squared Test of Independence {.not-title-slide .crunch-title .crunch-quarto-layout-panel .title-10 data-stack-name="Chi-Squared Test"}

::: {layout="[1,1]" layout-valign="center"}

![$z$-Tests and $t$-Tests](images/pooh_nobg.png){fig-align="center" width="300"}

![The Chi-Squared Test](images/swole_pooh.jpeg){fig-align="center" width="400"}

:::

## What Hypothesis Are We Testing? {.crunch-title .crunch-math .crunch-ul .small-math .crunch-lists}

* $z$-Tests and $t$-Tests might feel a bit weak: they allow us to hypothesize about a single population parameter, which is nice, but throughout the course we have been talking about **distributions**, not just means or variances!
* The **Chi-Squared Test** allows us to hypothesize about distributions: for categorical random variables $X_1$ and $X_2$ we can test the null and alternative hypotheses:

$$
\begin{align*}
H_0: X_1 \perp X_2 &\iff \Pr(X_1 = v_1 \mid X_2 = v_2) = \Pr(X_1 = v_1) \\
H_A: X_1 \not\perp X_2 &\iff \Pr(X_1 = v_1 \mid X_2 = v_2) \neq \Pr(X_1 = v_1)
\end{align*}
$$

* $H_0$ in words: "learning the value of $X_2$ does not give me any information about the value of $X_1$, and vice-versa"

## Base Notation {.smaller}

* *(May seem tedious, but you will be thankful later if you use this notation!)*
* Let $N$ be the total number of samples we have in our dataset
* Let $K_1$ be the total number of **categories** that $X_1$ can take on: formally, $K_1 = |\mathcal{R}_{X_1}|$.
* Similarly, let $K_2 = |\mathcal{R}_{X_2}|$, the cardinality of the support of $X_2$.
* We will use a **lowercase** $k_{1,i}$ to represent the **$i$th possible value** of $X_1$, (i.e., the $i$th element of $\mathcal{R}_{X_1}$), and $k_{2,i}$ to represent the $i$th possible value of $\mathcal{R}_{X_2}$.
  * e.g., $k_{1,1}$ is the first element of $\mathcal{R}_{X_1}$, $k_{1,2}$ is the second element of $\mathcal{R}_{X_2}$, and so on.

## Joint and Marginal Frequencies {.smaller}

* Now we can define the central quantities that the chi-squared test involves: the **frequencies** (i.e., unnormalized probabilities) of observations with certain (categorical) variable values:

* $f_{i,j}$: The **number of observations** for which $X_1 = k_{1,i}$ (the $i$th value that $X_1$ can take on) and $X_2 = k_{2,j}$ (the $j$th value that $X_2$ can take on).
  * Example: $f_{1,3}$, represents the **number of observations** for which $X_1 = k_{1,1}$ and $X_2 = k_{2,3}$.
* Let $f_{i,\cdot} = \sum_{j=1}^{K_2}f_{i,j}$ be the **number of observations** for which $X_1 = k_{1,i}$, regardless of the value of $X_2$ (hence we call this the **marginal frequency** of $X_1 = k_{1,i}$). 
  * $f_{7,\cdot}$, for example, represents the **number of observations** for which $X_1 = k_{1,7}$, **regardless** of the values of $X_2$ across these observations.
  * This is called "dot notation", and makes it easy for us to notationally represent which marginal distributions we are talking about.
* Similarly, let $f_{\cdot, j} = \sum_{i=1}^{K_1}f_{i,j}$ be the **marginal frequency** with which songs have $V_2 = k_j$. $f_{\cdot, 2}$, for example, represents the **number of songs** for which the **valence** value is at level 2 (Moderate), **regardless** of its artist.

## We Made It

* We can finally compute the test statistic for the Chi-Squared Test, $Q$!

$$
Q = \sum_{k_1=1}^{K_1}\sum_{k_2=1}^{K_2}\frac{\left(f_{k_1,k_2} - \frac{f_{k_1,\cdot}f_{\cdot,k_2}}{N}\right)^2}{\frac{f_{k_1,\cdot}f_{\cdot, k_2}}{N}}
$$

* This test statistic has (by construction) a **Chi-Squared Distribution** with $(K_1 - 1)(K_2 - 1)$ degrees of freedom

## In R

We can compute this test statistic (somewhat laboriously) in R as follows: first let's create variables representing the **supports** $\mathcal{R}_{X_1}$ and $\mathcal{R}_{X_2}$ for our two random variables $X_1$ and $X_2$:

```{r}
#| label: chi-sq-tedious
#(L1 <- sort(unique(artist_df$artist_name)))
#K1 <- length(L1)
# Here we set this manually, rather than using unique(), so we can obtain a specific ordering that we want
#(L2 <- c("more negative", "Moderate", "more positive"))
#K2 <- length(L2)
```

## Computing Marginal Frequencies

* Next we pre-compute the **marginal frequencies** for all possible $X_1$ values (all elements of $\mathcal{R}_{X_1}$) and then for all possible $X_2$ values (all elements of $\mathcal{R}_{X_2}$):

```{r}
#| label: precompute-marginals
1
```

## Computing the Test Statistic

* Finally, we can compute the individual bin frequencies $f_{i,j}$ in a loop *(there are more efficient ways to do this, which we'd want to adopt instead if our dataset was much larger, for example)*:

```{r}
#| label: compute-chi-sq-stat
# q_sum <- 0
# for (k1 in 1:K1) {
#   artist_name_value <- L1[k1]
#   artist_marginal_freq <- L1_marginal_freqs[k1]
#   for (k2 in 1:K2) {
#     valence_value <- L2[k2]
#     valence_marginal_freq <- L2_marginal_freqs[k2]
#     print(paste0("(",artist_name_value,", ",valence_value,")"))
#     # Compute the frequency in this bin
#     bin_df <- artist_df |> filter(artist_name == artist_name_value & valence_C == valence_value)
#     print(nrow(bin_df))
#     # And now, since we precomputed the marginal frequencies, we have everything we need!
#     joint_freq <- nrow(bin_df)
#     marginal_ratio <- (artist_marginal_freq * valence_marginal_freq) / N
#     numer <- (joint_freq - marginal_ratio)^2
#     denom <- marginal_ratio
#     cur_q_val <- numer / denom
#     q_sum <- q_sum + cur_q_val
#   }
# }

# q_sum
```

## Interpretation

* Since we know this test statistic $Q$ follows (asymptotically/approximately) a chi-squared distribution with $(K_1 - 1)(K_2 - 1)$ degrees of freedom, we can compute the **probability** of obtaining a test statistic value this high or higher (14.162598), using R's built-in `dchisq()` function:

```{r}
#| label: chi-sq-p-value
#(test_df <- (K1 - 1) * (K2 - 2))
#dchisq(q_sum, test_df)
```

## Drawing Conclusions in Classical World

And so, finally: this tells us that under the **null hypothesis** that `artist_name` and `valence_C` represent draws from **independent** random variables, the likelihood of obtaining our dataset, or a dataset with a more extreme test statistic, is about 0.00042, or 0.042%.

Therefore, if we are evaluating our hypothesis at the 5% confidence level (or 1% or even 0.1%, because this is a **very low** probability value/percentage), we conclude that we should **reject** the null hypothesis that artist name and valence are independent. Therefore (switching from a frequentist to a Bayesian interpretation, mercifully), we **increase our degree of belief** in the hypothesis that the valence of a song **does** depend upon the artist making the song.

## Drawing Conclusions in Modern (Bayesian) World {.smaller}

This **technical** conclusion (which focuses only on the results of the statistical hypothesis test), combined with the Bayesian interpretation at the end, lets us say something about "what we've learned" about music:

* Our Bayesian **prior** on the null hypothesis, $\Pr(H_0) = \Pr(V \perp A)$, was probably quite small: before performing this analysis, we probably did not think that these two variables are independent, since we know that some artists tend to make more **sad** songs, while other artists tend to make more **happy** songs (though there are obviously exceptions: bands that tend to make a mix of sad and happy songs).
* Then we performed the analysis, and **failed to reject** the hypothesis that these two variables are independent.
* Therefore, our Bayesian **posterior** on $\Pr(V \perp A)$ should be slightly **lower**: If our prior probability on $H_0$ was some value $p$, our posterior probability now that we've performed the study and failed to reject the null hypothesis should be $p - \varepsilon$, for some appropriate value $\varepsilon$ representing the **relative balance** between (our view of) the veracity of the prior vs. the veracity of the test we performed.
* To make it concrete: if we previously believed that $p = \Pr(H_0) = 0.1$, perhaps now we can update our beliefs such that $p = \Pr(H_0) = 0.05$. This would represent the case where we lend **equal credence** in our prior and in our experiement, since in that case

$$
p_{post} = \frac{1}{2}\Pr(H_0) + \frac{1}{2}\text{Test Result} = \frac{1}{2}(0.1) + \frac{1}{2}(0) = 0.05.
$$

