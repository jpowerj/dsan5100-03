@book{agnesi_analytical_1801,
  title = {Analytical {{Institutions}} in {{Four Books}}: {{Originally Written}} in {{Italian}}},
  shorttitle = {Analytical {{Institutions}} in {{Four Books}}},
  author = {Agnesi, Maria Gaetana},
  year = {1801},
  publisher = {{Taylor and Wilks}},
  googlebooks = {o54AAAAAMAAJ},
  langid = {english}
}

@book{collins_golem_1993,
  title = {The {{Golem}}: {{What You Should Know About Science}}},
  shorttitle = {The {{Golem}}},
  author = {Collins, Harry M. and Pinch, Trevor},
  year = {1993},
  publisher = {{Cambridge University Press}},
  abstract = {Harry Collins and Trevor Pinch liken science to the Golem, a creature from Jewish mythology, powerful yet potentially dangerous, a gentle, helpful creature that may yet run amok at any moment. Through a series of intriguing case studies the authors debunk the traditional view that science is the straightforward result of competent theorisation, observation and experimentation. The very well-received first edition generated much debate, reflected in a substantial new Afterword in this second edition, which seeks to place the book in what have become known as 'the science wars'.},
  isbn = {978-1-107-39449-0},
  langid = {english}
}

@book{degroot_probability_2013,
  title = {Probability and {{Statistics}}},
  author = {DeGroot, Morris H. and Schervish, Mark J.},
  year = {2013},
  publisher = {{Pearson Education}},
  abstract = {The revision of this well-respected text presents a balanced approach of the classical and Bayesian methods and now includes a chapter on simulation (including Markov chain Monte Carlo and the Bootstrap), coverage of residual analysis in linear models, and many examples using real data. Probability \& Statistics, Fourth Edition, was written for a one- or two-semester probability and statistics course. This course is offered primarily at four-year institutions and taken mostly by sophomore and junior level students majoring in mathematics or statistics. Calculus is a prerequisite, and a familiarity with the concepts and elementary properties of vectors and matrices is a plus.},
  googlebooks = {hIPkngEACAAJ},
  isbn = {978-1-292-02504-9},
  langid = {english}
}

@book{efron_introduction_1994,
  title = {An {{Introduction}} to the {{Bootstrap}}},
  author = {Efron, Bradley and Tibshirani, R. J.},
  year = {1994},
  month = may,
  publisher = {{CRC Press}},
  abstract = {An Introduction to the Bootstrap arms scientists and engineers as well as statisticians with the computational techniques they need to analyze and understand complicated data sets. The bootstrap is a computer-based method of statistical inference that answers statistical questions without formulas and gives a direct appreciation of variance, bias, coverage, and other probabilistic phenomena. This book presents an overview of the bootstrap and related methods for assessing statistical accuracy, concentrating on the ideas rather than their mathematical justification. Not just for beginners, the presentation starts off slowly, but builds in both scope and depth to ideas that are quite sophisticated.},
  googlebooks = {MWC1DwAAQBAJ},
  isbn = {978-1-00-006498-8},
  langid = {english}
}

@book{gardner_colossal_2001,
  title = {Colossal {{Book}} of {{Mathematics}}: {{Classic Puzzles Paradoxes And Problems}}},
  shorttitle = {Colossal {{Book}} of {{Mathematics}}},
  author = {Gardner, Martin},
  year = {2001},
  publisher = {{W. W. Norton \& Company}},
  abstract = {No amateur or math authority can be without this ultimate compendium from America's best-loved mathematical expert.Whether discussing hexaflexagons or number theory, Klein bottles or the essence of "nothing," Martin Gardner has single-handedly created the field of "recreational mathematics." The Colossal Book of Mathematics collects together Gardner's most popular pieces from his legendary "Mathematical Games" column, which ran in Scientific American for twenty-five years. Gardner's array of absorbing puzzles and mind-twisting paradoxes opens mathematics up to the world at large, inspiring people to see past numbers and formulas and experience the application of mathematical principles to the mysterious world around them. With articles on topics ranging from simple algebra to the twisting surfaces of Mobius strips, from an endless game of Bulgarian solitaire to the unreachable dream of time travel, this volume comprises a substantial and definitive monument to Gardner's influence on mathematics, science, and culture.  In its twelve sections, The Colossal Book of Math explores a wide range of areas, each startlingly illuminated by Gardner's incisive expertise. Beginning with seemingly simple topics, Gardner expertly guides us through complicated and wondrous worlds: by way of basic algebra we contemplate the mesmerizing, often hilarious, linguistic and numerical possibilities of palindromes; using simple geometry, he dissects the principles of symmetry upon which the renowned mathematical artist M. C. Escher constructs his unique, dizzying universe. Gardner, like few thinkers today, melds a rigorous scientific skepticism with a profound artistic and imaginative impulse. His stunning exploration of "The Church of the Fourth Dimension," for example, bridges the disparate worlds of religion and science by brilliantly imagining the spatial possibility of God's presence in the world as a fourth dimension, at once "everywhere and nowhere."  With boundless wisdom and his trademark wit, Gardner allows the reader to further engage challenging topics like probability and game theory which have plagued clever gamblers, and famous mathematicians, for centuries. Whether debunking Pascal's wager with basic probability, making visual music with fractals, or uncoiling a "knotted doughnut" with introductory topology, Gardner continuously displays his fierce intelligence and gentle humor. His articles confront both the comfortingly mundane{\textemdash}"Generalized Ticktacktoe" and "Sprouts and Brussel Sprouts"{\textemdash}and the quakingly abstract{\textemdash}"Hexaflexagons," "Nothing," and "Everything." He navigates these staggeringly obscure topics with a deft intelligence and, with addendums and suggested reading lists, he informs these classic articles with new insight.  Admired by scientists and mathematicians, writers and readers alike, Gardner's vast knowledge and burning curiosity reveal themselves on every page. The culmination of a lifelong devotion to the wonders of mathematics, The Colossal Book of Mathematics is the largest and most comprehensive math book ever assembled by Gardner and remains an indispensable volume for the amateur and expert alike.},
  googlebooks = {orz0SDEakpYC},
  isbn = {978-0-393-02023-6},
  langid = {english}
}

@book{gelman_data_2007,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2007},
  publisher = {{Cambridge University Press}},
  abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout. Author resource page: http://www.stat.columbia.edu/{\textasciitilde}gelman/arm/},
  googlebooks = {lV3DIdV0F9AC},
  isbn = {978-0-521-68689-1},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Political Science / General,{Psychology / Assessment, Testing \& Measurement},Social Science / Research}
}

@article{hansen_large_1982,
  title = {Large Sample Properties of Generalized Method of Moments Estimators},
  author = {Hansen, Lars Peter},
  year = {1982},
  journal = {Econometrica: Journal of the econometric society},
  pages = {1029--1054},
  publisher = {{JSTOR}}
}

@book{hastie_elements_2013,
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}}},
  shorttitle = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2013},
  month = nov,
  publisher = {{Springer Science \& Business Media}},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
  googlebooks = {yPfZBwAAQBAJ},
  isbn = {978-0-387-21606-5},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Database Administration \& Management,Computers / Mathematical \& Statistical Software,Mathematics / Discrete Mathematics,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes,Science / Life Sciences / Biology,Science / Life Sciences / General}
}

@book{james_introduction_2013,
  title = {An {{Introduction}} to {{Statistical Learning}}: With {{Applications}} in {{R}}},
  shorttitle = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  month = jun,
  publisher = {{Springer Science \& Business Media}},
  abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
  googlebooks = {qcI\_AAAAQBAJ},
  isbn = {978-1-4614-7138-7},
  langid = {english}
}

@book{kahneman_thinking_2011,
  title = {Thinking, {{Fast}} and {{Slow}}},
  author = {Kahneman, Daniel},
  year = {2011},
  month = oct,
  publisher = {{Farrar, Straus and Giroux}},
  abstract = {Major New York Times bestsellerWinner of the National Academy of Sciences Best Book Award in 2012Selected by the New York Times Book Review as one of the ten best books of 2011A Globe and Mail Best Books of the Year 2011 TitleOne of The Economist's 2011 Books of the Year One of The Wall Street Journal's Best Nonfiction Books of the Year 20112013 Presidential Medal of Freedom RecipientKahneman's work with Amos Tversky is the subject of Michael Lewis's The Undoing Project: A Friendship That Changed Our MindsIn his mega bestseller, Thinking, Fast and Slow, Daniel Kahneman, the renowned psychologist and winner of the Nobel Prize in Economics, takes us on a groundbreaking tour of the mind and explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. The impact of overconfidence on corporate strategies, the difficulties of predicting what will make us happy in the future, the profound effect of cognitive biases on everything from playing the stock market to planning our next vacation{\textemdash}each of these can be understood only by knowing how the two systems shape our judgments and decisions.Engaging the reader in a lively conversation about how we think, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives{\textemdash}and how we can use different techniques to guard against the mental glitches that often get us into trouble. Winner of the National Academy of Sciences Best Book Award and the Los Angeles Times Book Prize and selected by The New York Times Book Review as one of the ten best books of 2011, Thinking, Fast and Slow is destined to be a classic.},
  googlebooks = {ZuKTvERuPG8C},
  isbn = {978-1-4299-6935-2},
  langid = {english}
}

@book{mcelreath_statistical_2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{STAN}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
  googlebooks = {FuLWDwAAQBAJ},
  isbn = {978-0-429-64231-9},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability \& Statistics / General,Science / Life Sciences / Biological Diversity}
}

@book{popper_logic_1934,
  title = {The {{Logic}} of {{Scientific Discovery}}},
  author = {Popper, Karl R.},
  year = {1934},
  publisher = {{Psychology Press}},
  abstract = {Described by the philosopher A.J. Ayer as a work of 'great originality and power', this book revolutionized contemporary thinking on science and knowledge. Ideas such as the now legendary doctrine of 'falsificationism' electrified the scientific community, influencing even working scientists, as well as post-war philosophy. This astonishing work ranks alongside The Open Society and Its Enemies as one of Popper's most enduring books and contains insights and arguments that demand to be read to this day.},
  googlebooks = {Yq6xeupNStMC},
  isbn = {978-0-415-27844-7},
  langid = {english}
}

@article{tharwat_parameter_2019,
  title = {Parameter Investigation of Support Vector Machine Classifier with Kernel Functions},
  author = {Tharwat, Alaa},
  year = {2019},
  month = dec,
  journal = {Knowledge and Information Systems},
  volume = {61},
  number = {3},
  pages = {1269--1302},
  issn = {0219-3116},
  doi = {10.1007/s10115-019-01335-4},
  urldate = {2023-10-19},
  abstract = {Support vector machine (SVM) is one of the well-known learning algorithms for classification and regression problems. SVM parameters such as kernel parameters and penalty parameter have a great influence on the complexity and performance of predicting models. Hence, the model selection in SVM involves the penalty parameter and kernel parameters. However, these parameters are usually selected and used as a black box, without understanding the internal details. In this paper, the behavior of the SVM classifier is analyzed when these parameters take different values. This analysis consists of illustrative examples, visualization, and mathematical and geometrical interpretations with the aim of providing the basics of kernel functions with SVM and to show how it works to serve as a comprehensive source for researchers who are interested in this field. This paper starts by highlighting the definition and underlying principles of SVM in details. Moreover, different kernel functions are introduced and the impact of each parameter in these kernel functions is explained from different perspectives.},
  langid = {english},
  file = {/Users/jpj/Zotero/storage/BX3VQ6AH/Tharwat - 2019 - Parameter investigation of support vector machine .pdf}
}
