[
  {
    "objectID": "cheatsheet-math.html",
    "href": "cheatsheet-math.html",
    "title": "DSAN 5100 Math Cheatsheet",
    "section": "",
    "text": "Proposition \\(p\\): A true-false statement like “1 + 1 = 2” or “\\(x\\) is greater than 5”. The latter would be written as \\(p(x)\\), since its true/false value depends on a given value of \\(x\\).\n\\(\\wedge\\): Logical “and”. \\(p \\wedge q\\) is true only if both \\(p\\) and \\(q\\) are true.\n\\(\\vee\\): Logical “or”. \\(p \\vee q\\) is true if \\(p\\) is true or \\(q\\) is true, or both are true.\n\\(\\neg\\): Logical negation: If \\(p\\) is true, \\(\\neg p\\) is false. If \\(p\\) is false, \\(\\neg p\\) is true.\nDeMorgan’s Laws: Logical identities which illustrate how the negation operator gets “distributed” in a logical statement, allowing conversion of “or” statements into “and” statements, and vice-versa: \\(\\neg(p \\wedge q) = \\neg(\\neg p \\vee \\neg q)\\)\n\\(\\implies\\): “Implies”. \\(a \\implies b\\) is true if, whenever \\(a\\) is true, \\(b\\) is also true.\n\\(\\iff\\): “If and only if”. \\(a \\iff b\\) is true if, \\(a\\) is only true when \\(b\\) is true, and \\(a\\) is only false when \\(b\\) is false.\n\\(\\exists\\): “There exists”. In the course this will be written as \\(\\exists x \\in S [p(x)]\\), which means that there exists some element \\(x\\) in the set \\(S\\) such that \\(p(x)\\) is true. Also called the “existential quantifier”.\n\\(\\forall\\): “For all”. In the course this will be written as \\(\\forall x \\in S [p(x)]\\), which means that for every element in a set \\(S\\) (with \\(x\\) representing some element arbitrarily taken from \\(S\\)), the proposition \\(p(x)\\) is true. Also called the “universal quantifier”.\n\nNote that the universal and existential quantifiers are closely related by a negation law (just like \\(\\wedge\\) and \\(\\vee\\) and their connection via DeMorgan’s Laws): \\(\\neg \\left( \\forall x \\in S[p(x)] \\right) \\iff \\exists x \\in S [\\neg p(x)]\\). The negation on the outside of the quantified proposition has moved inside of it, with the quantifier flipped.\nThe same holds true in reverse: \\(\\neg \\left( \\exists x \\in S [p(x)] \\right) \\iff \\forall x \\in S [\\neg p(x)]\\)\n\n\n\n\n\n\nA set \\(S\\) (denoted by a capital letter when possible) is a collection of elements (denoted by a lowercase letter when possible).\n\nFor example, if \\(S = \\{0,1,2,3\\}\\), then \\(0\\) is an element of \\(S\\).\n\n\\(a \\in S\\): The proposition that \\(a\\) is an element of the set \\(S\\).\n\nIf \\(S = \\{0,1,2,3\\}\\), then \\(2 \\in S\\) but \\(5 \\notin S\\).\n\n\\(A \\subseteq S\\): The proposition that \\(A\\) is a subset of the set \\(S\\).\n\nIf \\(S = \\{0, 1, 2, 3\\}\\), then \\(\\{1,3\\} \\subseteq S\\) but \\(\\{1,4\\} \\not\\subseteq S\\). Note that sets are defined to be subsets of themselves, so that \\(S \\subseteq S\\).\n\n\\(A \\subset S\\): The proposition that \\(A\\) is a proper subset of \\(S\\), meaning that \\(A \\subseteq S\\) but \\(A \\neq S\\).\n\nWhile sets are subsets of themselves, sets are not proper subsets of themselves, so that if \\(S = \\{0, 1, 2, 3\\}\\), then \\(\\{1,2,3\\} \\subset S\\) but \\(\\{0, 1, 2, 3\\} \\not\\subset S\\).\n\n\\(|S|\\): The cardinality of, or number of elements in, a set \\(S\\).\n\nIf \\(S = \\{1, 2, 3\\}\\), \\(|S| = 3\\).\nIf the set \\(S\\) has infinite cardinality, we can distinguish between two cases:\n\nIf elements of \\(S\\) can be put into one-to-one correspondence with the natural numbers \\(\\mathbb{N}\\), we say that \\(S\\) is countably infinite and has cardinality \\(\\aleph_0\\) (pronounced “aleph-null”): \\(|S| = \\aleph_0\\).\nIf elements of \\(S\\) cannot be put into one-to-one correspondence with the natural numbers \\(\\mathbb{N}\\), we say that \\(S\\) is uncountably infinite and has cardinality greater than \\(\\aleph_0\\): \\(|S| &gt; \\aleph_0\\).\n\n\n\\(\\mathcal{P}(S)\\): The power set of a set \\(S\\), which is the set of all possible subsets of \\(S\\).\n\nFor example, if \\(S = \\{1, 2, 3\\}\\), \\(\\mathcal{P}(S) = \\{\\varnothing, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\}\\}\\). Notice that \\(|\\mathcal{P}(S)| = 2^{|S|}\\), which is always true of the power set.\n\n\n\n\n\n\n\\(\\mathbb{N}\\): The set of all natural numbers, sometimes called the “counting numbers”: \\(\\{0, 1, 2, 3, \\ldots \\}\\) (This set is countably infinite).\n\\(\\mathbb{Z}\\): The set of all integers, which includes all of the natural numbers along with their negatives: \\(\\{\\ldots, -2, -1, 0, 1, 2, \\ldots\\}\\) (This set is also countably infinite)\n\\(\\mathbb{Q}\\): The set of all rational numbers, i.e., well-defined ratios of two integers. \\(x \\in \\mathbb{Q} \\iff x = \\frac{p}{q}\\) for two integers \\(p\\) and \\(q\\), and \\(q \\neq 0\\). (This set is, surprisingly, also countably infinite)\n\\(\\mathbb{R}\\): The set of all real numbers, which includes all integers as well as numbers such as \\(\\pi\\), \\(2.356\\), or \\(\\sqrt{2}\\) (This set is uncountably infinite).\nScalar: A single number from some set of numbers, like \\(-2.1 \\in \\mathbb{R}\\)\nVector: A \\(d\\)-dimensional vector \\(\\mathbf{v}\\) is a collection of \\(d\\) scalars in \\(\\mathbb{R}\\), \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_d)^\\top\\), which we can interpret as an arrow pointing \\(v_i\\) units in each dimension \\(i\\). For example, if \\(\\mathbf{v} = (3,5)\\), we can interpret \\(\\mathbf{v}\\) as representing an arrow pointing \\(3\\) units in the \\(x\\) direction and \\(5\\) units in the \\(y\\) direction.\n\nAs indicated above, however, we will assume that vectors are column vectors unless otherwise specified, though they will be written row-wise with a transpose symbol at the end. This means that, although it will be written like \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)^\\top\\), you should apply the transpose in your head: \\[\n\\mathbf{v} = (v_1, v_2, \\ldots, v_n)^\\top = \\begin{bmatrix}v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\\end{bmatrix}\n\\]\n\nMatrix: An \\(m \\times n\\) matrix \\(\\mathbf{M}_{[m \\times n]}\\) is an \\(m\\)-by-\\(n\\) grid of scalars, where the scalar in the \\(i\\)th row and \\(j\\)th column is denoted \\(m_{i,j}\\). In the class, we will be careful to add a subscript like \\(\\mathbf{M}_{[m \\times n]}\\) to indicate the number of rows (\\(m\\)) and columns (\\(n\\)) in the matrix, since operations like multiplication are only defined for matrices with particular dimensions.\nMatrix multiplication: For two matrices \\(\\mathbf{X}_{[a \\times b]}\\) and \\(\\mathbf{Y}_{[c \\times d]}\\), matrix multiplication is defined if \\(b = c\\), and produces the following \\(a \\times d\\) matrix \\(\\mathbf{XY}_{[a \\times d]}\\):\n\\[\n  \\mathbf{XY}_{[a \\times d]} = \\begin{bmatrix}\n  x_{1,1}y_{1,1} & \\cdots & x_{m,1}y_{1,n} \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  x_{1,n}y_{m,1} & \\cdots & x_{m,n}y_{m,n}\n  \\end{bmatrix}\n  \\]\n\\(\\sum_{i=1}^n f(i)\\): \\(\\Sigma\\) is the capitalized Greek letter “Sigma”, and stands for “Sum” in this case. This notation means: “the sum of \\(f(i)\\) from \\(i = 1\\) to \\(i = n\\)”. For example, \\(\\sum_{i=1}^3 i^2 = 1^2 + 2^2 + 3^2 = 14\\).\n\\(\\prod_{i=1}^n f(i)\\): \\(\\Pi\\) is the capitalized Greek letter “Pi”, and stands for “Product” in this case. This notation means: “the product of \\(f(i)\\) from \\(i = 1\\) to \\(i = n\\)”. For example, \\(\\prod_{i=1}^3 i^2 = 1^2 \\cdot 2^2 \\cdot 3^2 = 36\\).\n\n\n\n\n\n\\([a,b] \\in \\mathbb{R}\\): The closed interval between \\(a\\) and \\(b\\). That is, the subset of \\(\\mathbb{R}\\) containing all numbers between \\(a\\) and \\(b\\), including \\(a\\) and \\(b\\) themselves.\n\\((a,b) \\in \\mathbb{R}\\): The open interval between \\(a\\) and \\(b\\). That is, the subset of \\(\\mathbb{R}\\) containing all numbers between \\(a\\) and \\(b\\), excluding \\(a\\) and \\(b\\) themselves.\n\\([a, b)\\), \\((a, b]\\): The half-open interval between \\(a\\) and \\(b\\). In the first case, we include \\(a\\) but exclude \\(b\\), while in the second we exclude \\(a\\) but include \\(b\\).\n\\(\\int_{a}^b f(x)dx\\): The integral of the function \\(f(x)\\) between points \\(a\\) and \\(b\\). In this course, you just need to remember that this produces the area under the curve of \\(f(x)\\) between these points.\n\\(\\frac{d}{dx} \\left[ f(x) \\right]\\): The derivative of the function \\(f(x)\\). For this class, you just need to remember that the derivative is what transforms a Cumulative Density Function (CDF) into a Probability Density Function (PDF): if \\(F_X(v)\\) is the CDF of a random variable \\(X\\), then \\(\\frac{d}{dx}\\left[ F_X(v) \\right] = f_X(v)\\), the PDF of \\(X\\).\n\nFor functions of multiple variables, like the joint pdf \\(f_{X,Y}(v_X, v_Y)\\), we use the \\(\\partial\\) symbol instead of \\(d\\) to denote partial derivatives: for example, the change in \\(F_{X,Y}(v_X, v_Y)\\) as \\(v_X\\) changes is denoted \\(\\frac{\\partial}{\\partial v_X}\\left[ F_{X,Y}(v_X, v_Y) \\right]\\), while the change in \\(F_{X,Y}(v_X, v_Y)\\) as \\(v_Y\\) changes is denoted \\(\\frac{\\partial}{\\partial v_Y}\\left[ F_{X,Y}(v_X, v_Y) \\right]\\).",
    "crumbs": [
      "Math Cheatsheet"
    ]
  },
  {
    "objectID": "cheatsheet-math.html#math-preliminaries",
    "href": "cheatsheet-math.html#math-preliminaries",
    "title": "DSAN 5100 Math Cheatsheet",
    "section": "",
    "text": "Proposition \\(p\\): A true-false statement like “1 + 1 = 2” or “\\(x\\) is greater than 5”. The latter would be written as \\(p(x)\\), since its true/false value depends on a given value of \\(x\\).\n\\(\\wedge\\): Logical “and”. \\(p \\wedge q\\) is true only if both \\(p\\) and \\(q\\) are true.\n\\(\\vee\\): Logical “or”. \\(p \\vee q\\) is true if \\(p\\) is true or \\(q\\) is true, or both are true.\n\\(\\neg\\): Logical negation: If \\(p\\) is true, \\(\\neg p\\) is false. If \\(p\\) is false, \\(\\neg p\\) is true.\nDeMorgan’s Laws: Logical identities which illustrate how the negation operator gets “distributed” in a logical statement, allowing conversion of “or” statements into “and” statements, and vice-versa: \\(\\neg(p \\wedge q) = \\neg(\\neg p \\vee \\neg q)\\)\n\\(\\implies\\): “Implies”. \\(a \\implies b\\) is true if, whenever \\(a\\) is true, \\(b\\) is also true.\n\\(\\iff\\): “If and only if”. \\(a \\iff b\\) is true if, \\(a\\) is only true when \\(b\\) is true, and \\(a\\) is only false when \\(b\\) is false.\n\\(\\exists\\): “There exists”. In the course this will be written as \\(\\exists x \\in S [p(x)]\\), which means that there exists some element \\(x\\) in the set \\(S\\) such that \\(p(x)\\) is true. Also called the “existential quantifier”.\n\\(\\forall\\): “For all”. In the course this will be written as \\(\\forall x \\in S [p(x)]\\), which means that for every element in a set \\(S\\) (with \\(x\\) representing some element arbitrarily taken from \\(S\\)), the proposition \\(p(x)\\) is true. Also called the “universal quantifier”.\n\nNote that the universal and existential quantifiers are closely related by a negation law (just like \\(\\wedge\\) and \\(\\vee\\) and their connection via DeMorgan’s Laws): \\(\\neg \\left( \\forall x \\in S[p(x)] \\right) \\iff \\exists x \\in S [\\neg p(x)]\\). The negation on the outside of the quantified proposition has moved inside of it, with the quantifier flipped.\nThe same holds true in reverse: \\(\\neg \\left( \\exists x \\in S [p(x)] \\right) \\iff \\forall x \\in S [\\neg p(x)]\\)\n\n\n\n\n\n\nA set \\(S\\) (denoted by a capital letter when possible) is a collection of elements (denoted by a lowercase letter when possible).\n\nFor example, if \\(S = \\{0,1,2,3\\}\\), then \\(0\\) is an element of \\(S\\).\n\n\\(a \\in S\\): The proposition that \\(a\\) is an element of the set \\(S\\).\n\nIf \\(S = \\{0,1,2,3\\}\\), then \\(2 \\in S\\) but \\(5 \\notin S\\).\n\n\\(A \\subseteq S\\): The proposition that \\(A\\) is a subset of the set \\(S\\).\n\nIf \\(S = \\{0, 1, 2, 3\\}\\), then \\(\\{1,3\\} \\subseteq S\\) but \\(\\{1,4\\} \\not\\subseteq S\\). Note that sets are defined to be subsets of themselves, so that \\(S \\subseteq S\\).\n\n\\(A \\subset S\\): The proposition that \\(A\\) is a proper subset of \\(S\\), meaning that \\(A \\subseteq S\\) but \\(A \\neq S\\).\n\nWhile sets are subsets of themselves, sets are not proper subsets of themselves, so that if \\(S = \\{0, 1, 2, 3\\}\\), then \\(\\{1,2,3\\} \\subset S\\) but \\(\\{0, 1, 2, 3\\} \\not\\subset S\\).\n\n\\(|S|\\): The cardinality of, or number of elements in, a set \\(S\\).\n\nIf \\(S = \\{1, 2, 3\\}\\), \\(|S| = 3\\).\nIf the set \\(S\\) has infinite cardinality, we can distinguish between two cases:\n\nIf elements of \\(S\\) can be put into one-to-one correspondence with the natural numbers \\(\\mathbb{N}\\), we say that \\(S\\) is countably infinite and has cardinality \\(\\aleph_0\\) (pronounced “aleph-null”): \\(|S| = \\aleph_0\\).\nIf elements of \\(S\\) cannot be put into one-to-one correspondence with the natural numbers \\(\\mathbb{N}\\), we say that \\(S\\) is uncountably infinite and has cardinality greater than \\(\\aleph_0\\): \\(|S| &gt; \\aleph_0\\).\n\n\n\\(\\mathcal{P}(S)\\): The power set of a set \\(S\\), which is the set of all possible subsets of \\(S\\).\n\nFor example, if \\(S = \\{1, 2, 3\\}\\), \\(\\mathcal{P}(S) = \\{\\varnothing, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\}\\}\\). Notice that \\(|\\mathcal{P}(S)| = 2^{|S|}\\), which is always true of the power set.\n\n\n\n\n\n\n\\(\\mathbb{N}\\): The set of all natural numbers, sometimes called the “counting numbers”: \\(\\{0, 1, 2, 3, \\ldots \\}\\) (This set is countably infinite).\n\\(\\mathbb{Z}\\): The set of all integers, which includes all of the natural numbers along with their negatives: \\(\\{\\ldots, -2, -1, 0, 1, 2, \\ldots\\}\\) (This set is also countably infinite)\n\\(\\mathbb{Q}\\): The set of all rational numbers, i.e., well-defined ratios of two integers. \\(x \\in \\mathbb{Q} \\iff x = \\frac{p}{q}\\) for two integers \\(p\\) and \\(q\\), and \\(q \\neq 0\\). (This set is, surprisingly, also countably infinite)\n\\(\\mathbb{R}\\): The set of all real numbers, which includes all integers as well as numbers such as \\(\\pi\\), \\(2.356\\), or \\(\\sqrt{2}\\) (This set is uncountably infinite).\nScalar: A single number from some set of numbers, like \\(-2.1 \\in \\mathbb{R}\\)\nVector: A \\(d\\)-dimensional vector \\(\\mathbf{v}\\) is a collection of \\(d\\) scalars in \\(\\mathbb{R}\\), \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_d)^\\top\\), which we can interpret as an arrow pointing \\(v_i\\) units in each dimension \\(i\\). For example, if \\(\\mathbf{v} = (3,5)\\), we can interpret \\(\\mathbf{v}\\) as representing an arrow pointing \\(3\\) units in the \\(x\\) direction and \\(5\\) units in the \\(y\\) direction.\n\nAs indicated above, however, we will assume that vectors are column vectors unless otherwise specified, though they will be written row-wise with a transpose symbol at the end. This means that, although it will be written like \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)^\\top\\), you should apply the transpose in your head: \\[\n\\mathbf{v} = (v_1, v_2, \\ldots, v_n)^\\top = \\begin{bmatrix}v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\\end{bmatrix}\n\\]\n\nMatrix: An \\(m \\times n\\) matrix \\(\\mathbf{M}_{[m \\times n]}\\) is an \\(m\\)-by-\\(n\\) grid of scalars, where the scalar in the \\(i\\)th row and \\(j\\)th column is denoted \\(m_{i,j}\\). In the class, we will be careful to add a subscript like \\(\\mathbf{M}_{[m \\times n]}\\) to indicate the number of rows (\\(m\\)) and columns (\\(n\\)) in the matrix, since operations like multiplication are only defined for matrices with particular dimensions.\nMatrix multiplication: For two matrices \\(\\mathbf{X}_{[a \\times b]}\\) and \\(\\mathbf{Y}_{[c \\times d]}\\), matrix multiplication is defined if \\(b = c\\), and produces the following \\(a \\times d\\) matrix \\(\\mathbf{XY}_{[a \\times d]}\\):\n\\[\n  \\mathbf{XY}_{[a \\times d]} = \\begin{bmatrix}\n  x_{1,1}y_{1,1} & \\cdots & x_{m,1}y_{1,n} \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  x_{1,n}y_{m,1} & \\cdots & x_{m,n}y_{m,n}\n  \\end{bmatrix}\n  \\]\n\\(\\sum_{i=1}^n f(i)\\): \\(\\Sigma\\) is the capitalized Greek letter “Sigma”, and stands for “Sum” in this case. This notation means: “the sum of \\(f(i)\\) from \\(i = 1\\) to \\(i = n\\)”. For example, \\(\\sum_{i=1}^3 i^2 = 1^2 + 2^2 + 3^2 = 14\\).\n\\(\\prod_{i=1}^n f(i)\\): \\(\\Pi\\) is the capitalized Greek letter “Pi”, and stands for “Product” in this case. This notation means: “the product of \\(f(i)\\) from \\(i = 1\\) to \\(i = n\\)”. For example, \\(\\prod_{i=1}^3 i^2 = 1^2 \\cdot 2^2 \\cdot 3^2 = 36\\).\n\n\n\n\n\n\\([a,b] \\in \\mathbb{R}\\): The closed interval between \\(a\\) and \\(b\\). That is, the subset of \\(\\mathbb{R}\\) containing all numbers between \\(a\\) and \\(b\\), including \\(a\\) and \\(b\\) themselves.\n\\((a,b) \\in \\mathbb{R}\\): The open interval between \\(a\\) and \\(b\\). That is, the subset of \\(\\mathbb{R}\\) containing all numbers between \\(a\\) and \\(b\\), excluding \\(a\\) and \\(b\\) themselves.\n\\([a, b)\\), \\((a, b]\\): The half-open interval between \\(a\\) and \\(b\\). In the first case, we include \\(a\\) but exclude \\(b\\), while in the second we exclude \\(a\\) but include \\(b\\).\n\\(\\int_{a}^b f(x)dx\\): The integral of the function \\(f(x)\\) between points \\(a\\) and \\(b\\). In this course, you just need to remember that this produces the area under the curve of \\(f(x)\\) between these points.\n\\(\\frac{d}{dx} \\left[ f(x) \\right]\\): The derivative of the function \\(f(x)\\). For this class, you just need to remember that the derivative is what transforms a Cumulative Density Function (CDF) into a Probability Density Function (PDF): if \\(F_X(v)\\) is the CDF of a random variable \\(X\\), then \\(\\frac{d}{dx}\\left[ F_X(v) \\right] = f_X(v)\\), the PDF of \\(X\\).\n\nFor functions of multiple variables, like the joint pdf \\(f_{X,Y}(v_X, v_Y)\\), we use the \\(\\partial\\) symbol instead of \\(d\\) to denote partial derivatives: for example, the change in \\(F_{X,Y}(v_X, v_Y)\\) as \\(v_X\\) changes is denoted \\(\\frac{\\partial}{\\partial v_X}\\left[ F_{X,Y}(v_X, v_Y) \\right]\\), while the change in \\(F_{X,Y}(v_X, v_Y)\\) as \\(v_Y\\) changes is denoted \\(\\frac{\\partial}{\\partial v_Y}\\left[ F_{X,Y}(v_X, v_Y) \\right]\\).",
    "crumbs": [
      "Math Cheatsheet"
    ]
  },
  {
    "objectID": "cheatsheet-math.html#conditional-probability",
    "href": "cheatsheet-math.html#conditional-probability",
    "title": "DSAN 5100 Math Cheatsheet",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nProbability Fundamentals\n\n\\(\\Omega\\): The set of all possible outcomes in a probability setting\n\\(\\mathcal{P}(\\Omega) = \\{ e \\mid e \\subseteq \\Omega\\}\\): The set of all subsets of \\(\\Omega\\), each of which is an event\n\n\n\nRandom Variables\n\n\\(X\\) is a random variable if it maps each element of \\(\\Omega\\) to a real number \\(o \\in \\mathbb{R}\\).\n\nFor example, if we’re rolling a die, we can create a random variable \\(X\\) which maps \\(\\text{roll a one}\\) to \\(1\\), \\(\\text{roll a two}\\) to \\(2\\), and so on. (Allows us to do math with probability spaces!)\n\n\\(X\\) is a discrete random variable if it maps outcomes to countable set of numbers, whether this means a finite set like \\(\\{1,2,3\\}\\) or a countably infinite set like \\(\\mathbb{N}\\).\n\\(X\\) is a continuous random variable if it maps outcomes to a non-countable set of numbers, typically \\(\\mathbb{R}\\).\n\\(\\mathcal{R}_X\\), the support of a random variable \\(X\\), is the set of all possible values that the random variable can map onto. For example, if \\(X\\) represents a dice roll, then \\(\\mathcal{R}_X = \\{1, 2, 3, 4, 5, 6\\}\\).\nCumulative Density Function (CDF): Given a random variable \\(X\\) (whether discrete or continuous), \\(F_X(v) = P(X \\leq v)\\) is its cumulative density function, which tells us the probability that \\(X\\) is realized as a number less than or equal to some value \\(v\\).\nProbability Mass Function (PMF): Given a discrete random variable \\(X\\), \\(p_X(v) = P(X = v)\\) is its probability mass function, which tells us the probability that \\(X\\) is realized as the value \\(v\\).\nProbability Density Function (PDF): Given a continuous random variable \\(X\\), \\(f_X(v)\\) is the unique function which allows us to determine, using integration, the probability that \\(X\\) is in some range \\([a,b]\\). That is, it is the unique function satisfying \\(P(X \\in [a,b]) = \\int_a^b f_X(x)dx\\).\n\nRemember: unlike in the discrete case where \\(p_X(v) = P(X = v)\\), \\(f_X(v)\\) is not the probability that \\(X\\) is realized as the value \\(v\\). \\(f_X(v) \\neq P(X = v)\\).",
    "crumbs": [
      "Math Cheatsheet"
    ]
  },
  {
    "objectID": "cheatsheet-math.html#expectation-variance-moments",
    "href": "cheatsheet-math.html#expectation-variance-moments",
    "title": "DSAN 5100 Math Cheatsheet",
    "section": "Expectation, Variance, Moments",
    "text": "Expectation, Variance, Moments\n\n\\(M_1(V)\\): The (“regular”) arithmetic mean of a set of values \\(V = \\{v_1, v_2, \\ldots, v_n\\}\\): \\(M_1(V) = (v_1 + v_2 + \\cdots + v_n)\\frac{1}{n} = \\left( \\sum_{i=1}^n v_i \\right)\\frac{1}{n}\\).\n\\(M_0(V)\\): The geometric mean of a set of values \\(V = \\{v_1, v_2, \\ldots, v_n\\}\\): \\(M_0(V) = (v_1\\cdot v_2 \\cdot \\cdots \\cdot v_n)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^n v_i \\right)^{\\frac{1}{n}}\\)\n\\(M_{-1}(V)\\): The harmonic mean of a set of values \\(V = \\{v_1, v_2, \\ldots, v_n\\}\\): \\(M_{-1}(V) = \\frac{n}{\\frac{1}{v_1} + \\frac{1}{v_2} + \\cdots + \\frac{1}{v_n}} = \\left( \\frac{\\sum_{i=1}^n v_i^{-1}}{n} \\right)^{-1}\\)\n\\(\\odot\\): The Hadamard product of matrices. For two matrices \\(\\mathbf{X}_{[m \\times n]}\\) and \\(\\mathbf{Y}_{[m \\times n]}\\) with equal dimensions:\n\\[\n  X_{[m \\times n]} = \\begin{bmatrix}\n      x_{1,1} & \\cdots & x_{1,n} \\\\\n      \\vdots & \\ddots & \\vdots \\\\\n      x_{m,1} & \\cdots & x_{m,n}\n  \\end{bmatrix}, Y_{[m \\times n]} = \\begin{bmatrix}\n      y_{1,1} & \\cdots & y_{1,n} \\\\\n      \\vdots & \\ddots & \\vdots \\\\\n      y_{m,1} & \\cdots & y_{m,n}\n  \\end{bmatrix}\n  \\]\nTheir Hadamard product \\(\\mathbf{X} \\odot \\mathbf{Y}\\) is another \\(m \\times n\\) matrix\n\\[\n  (\\mathbf{X} \\odot \\mathbf{Y})_{[m \\times n]} = \\begin{bmatrix}\n      x_{1,1}y_{1,1} & \\cdots & x_{1,n}y_{1,n} \\\\\n      \\vdots & \\ddots & \\vdots \\\\\n      x_{m,1}y_{m,1} & \\cdots & x_{m,n}y_{m,n}\n  \\end{bmatrix}\n  \\]",
    "crumbs": [
      "Math Cheatsheet"
    ]
  },
  {
    "objectID": "extra-videos/index.html",
    "href": "extra-videos/index.html",
    "title": "Extra Videos",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Last Updated - Oldest\n        \n         \n          Last Updated - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nLast Updated\n\n\n\n\n\n\nCorrected Bayes’ Theorem Example (W03.1)\n\n\nFriday Sep 8, 2023\n\n\n\n\nStatistics Fundamentals (W02.5)\n\n\nTuesday Sep 5, 2023\n\n\n\n\nProbability Fundamentals (W02.4)\n\n\nMonday Sep 4, 2023\n\n\n\n\nCombinatorics (W02.3)\n\n\nSunday Sep 3, 2023\n\n\n\n\nReview (W02.2)\n\n\nSaturday Sep 2, 2023\n\n\n\n\nAbout Me (W02.1)\n\n\nFriday Sep 1, 2023\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Extra Videos"
    ]
  },
  {
    "objectID": "final-presentations.html",
    "href": "final-presentations.html",
    "title": "Final Presentation Info",
    "section": "",
    "text": "The information here, on the order for the final presentations, is loaded from a Google Sheets document so that student names are not included in the site’s html code itself (and the Sheets document will be removed at the end of the semester). But, if you would like your name removed from the Sheets document for safety/security reasons please let me know!\nClick to open sheet in new tab →"
  },
  {
    "objectID": "writeups/index.html",
    "href": "writeups/index.html",
    "title": "Extra Writeups",
    "section": "",
    "text": "Order By\n       Default\n         \n          Last Updated - Oldest\n        \n         \n          Last Updated - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nLast Updated\n\n\nCategory\n\n\n\n\n\n\nTwo Ways to Compute Probabilities in R/Python\n\n\nMonday, September 2, 2024\n\n\nExtra Writeups\n\n\n\n\nDeriving a pdf from Scratch\n\n\nMonday, September 2, 2024\n\n\nExtra Writeups\n\n\n\n\nTwo Ways of Sampling from a Data Frame\n\n\nFriday, August 30, 2024\n\n\nExtra Writeups\n\n\n\n\nSimulating Sample Spaces With Tibbles\n\n\nFriday, August 30, 2024\n\n\nExtra Writeups\n\n\n\n\nMarginal Distributions and “Marginalizing Out” A Variable\n\n\nFriday, August 30, 2024\n\n\nExtra Writeups\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Extra Writeups"
    ]
  },
  {
    "objectID": "writeups/computing-probabilities/index.html",
    "href": "writeups/computing-probabilities/index.html",
    "title": "Two Ways to Compute Probabilities in R/Python",
    "section": "",
    "text": "# For slides\nlibrary(ggplot2)\ncbPalette &lt;- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\noptions(ggplot2.discrete.colour = cbPalette)\n# Theme generator, for given sizes\ndsan_theme &lt;- function(plot_type = \"full\") {\n    if (plot_type == \"full\") {\n        custom_base_size &lt;- 16\n    } else if (plot_type == \"half\") {\n        custom_base_size &lt;- 22\n    } else if (plot_type == \"quarter\") {\n        custom_base_size &lt;- 28\n    } else {\n        # plot_type == \"col\"\n        custom_base_size &lt;- 22\n    }\n    theme &lt;- theme_classic(base_size = custom_base_size) +\n        theme(\n            plot.title = element_text(hjust = 0.5),\n            plot.subtitle = element_text(hjust = 0.5),\n            legend.title = element_text(hjust = 0.5),\n            legend.box.background = element_rect(colour = \"black\")\n        )\n    return(theme)\n}\n\nknitr::opts_chunk$set(fig.align = \"center\")\ng_pointsize &lt;- 5\ng_linesize &lt;- 1\n# Technically it should always be linewidth\ng_linewidth &lt;- 1\ng_textsize &lt;- 14\n\nremove_legend_title &lt;- function() {\n    return(theme(\n        legend.title = element_blank(),\n        legend.spacing.y = unit(0, \"mm\")\n    ))\n}\nIn several of the assignments for DSAN5100, we ask you to “compute” a probability. This can be ambiguous, as we’ve already learned several different ways to compute probabilities, so here I will try to distinguish between two different approaches to computing a probability using a computer. Hopefully it will help you in general when asking us questions, and will help us clarify which of these approaches we are looking for when we ask you to compute a probability in future assignments."
  },
  {
    "objectID": "writeups/computing-probabilities/index.html#approach-1-using-r-like-a-fancy-spreadsheet-program",
    "href": "writeups/computing-probabilities/index.html#approach-1-using-r-like-a-fancy-spreadsheet-program",
    "title": "Two Ways to Compute Probabilities in R/Python",
    "section": "Approach 1: Using R Like a Fancy Spreadsheet Program",
    "text": "Approach 1: Using R Like a Fancy Spreadsheet Program\nThis approach implicitly assumes the naïve definition of probability, from early on in the course:\n\n\n\n\n\n\n Naïve Definition of Probability\n\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\nIt follows the following logic:\n\nIf we want to use our computer to compute a probability, and if the probability space we’re working in is finite, then\n\nWe can represent the entire sample space \\(\\Omega\\) as a big tibble, and\n\n\nWe can represent the event of interest \\(E\\) as a subset of this tibble1.\n\n\nAs a super simple example that hopefully illustrates this approach, consider the case of rolling a single die, so that the sample space is\n\\[\n\\Omega = \\{1, 2, 3, 4, 5, 6\\}\n\\]\nand then consider the event of interest \\(E\\) to be “the outcome of the roll is even”, so that\n\\[\nE = \\{2, 4, 6\\} \\subseteq \\Omega\n\\]\nIn this approach we could construct a tibble representing the entire sample space \\(\\Omega\\) like\n\nlibrary(tibble)\nsample_space_df &lt;- tibble(outcome=seq(1,6))\nsample_space_df\n\n\n\n\n\noutcome\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n\n\n\n\nThen we could extract a subset of this tibble corresponding to the event \\(E\\) by selecting only those rows whose outcome value is even2:\n\nlibrary(dplyr)\nevent_df &lt;- sample_space_df %&gt;% filter(outcome %% 2 == 0)\nevent_df\n\n\n\n\n\noutcome\n\n\n\n\n2\n\n\n4\n\n\n6\n\n\n\n\n\n\nThese two tibbles together allow us to compute a probability using the naïve definition:\n\nnrow(event_df) / nrow(sample_space_df)\n\n[1] 0.5\n\n\nIt’s a bit silly to use this approach to study dice, since we can figure out the probabilistic properties of dice pretty intuitively (see below), but this approach becomes more useful when we think about problems focusing on data analysis.\nFor example, we might give you a problem that says\nLoad this dataset of country/region GDP data, and keep just the observations for the year 2010. What is the probability that an entity in this dataset had a GDP lower than $10 billion, and a country code containing the letter Z, in this year?\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\ngdp_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/fecd437b96d0954893de727383f2eaf2/raw/fec58507f7095cb8341b229d6eb74ce53232d663/gdp_2010.csv\")\nbelow_10b_df &lt;- gdp_df %&gt;% filter(value &lt; 10000000000)\nbelow_10b_z_df &lt;- below_10b_df %&gt;% filter(str_detect(code, 'Z'))\nbelow_10b_z_df\n\n\n\n\n\nname\ncode\nyear\nvalue\n\n\n\n\nBelize\nBLZ\n2010\n1397113450\n\n\nKyrgyz Republic\nKGZ\n2010\n4794357795\n\n\nSwaziland\nSWZ\n2010\n4438778424\n\n\n\n\n\n\nSo that now we could compute the probability of a country having these properties:\n\np_event &lt;- nrow(below_10b_z_df) / nrow(gdp_df)\np_event\n\n[1] 0.01470588\n\n\nSo, this approach works well when we’re dealing with nice, simple, finite sample-spacing and outcomes which are easily definable using and or or (for example), but we won’t be able to use it to handle fancier cases like continuous probability distributions."
  },
  {
    "objectID": "writeups/computing-probabilities/index.html#approach-2-using-r-like-a-fancy-calculator",
    "href": "writeups/computing-probabilities/index.html#approach-2-using-r-like-a-fancy-calculator",
    "title": "Two Ways to Compute Probabilities in R/Python",
    "section": "Approach 2: Using R Like a Fancy Calculator",
    "text": "Approach 2: Using R Like a Fancy Calculator\nIn this approach, we basically just use R (or Python, or whatever else) as a glorified calculator. We take the hard part—reasoning about what the sample space might look like, for example, or whether events are independent—and do it in our heads, then use R to figure out what happens when we multiply/add/divide the things we already figured out.\nContinuing the dice example, for instance, this approach would go something like: we ask you\n\nWhat is the probability that you observe a roll greater than 4, followed by a roll less than or equal to 4, followed by a roll greater than 4?\n\nand you reason through this like,\n\nI already know that there are 6 outcomes, and that they’re all equally likely, so that the probability of seeing a particular outcome in \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\) is always \\(\\frac{1}{6}\\). Therefore I can define an event \\(E_&gt;\\) representing the event of observing an outcome greater than 4, and another event \\(E_\\leq\\) of observing an outcome less than or equal to 4. I know that\n\n\n\nSince the possible outcomes greater than 4 are 5 and 6, my event \\(E_&gt; = \\{5, 6\\}\\), and since \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\), I can compute the probability of \\(E_&gt;\\) as \\[\n\\Pr(E_&gt;) = \\frac{|E_&gt;|}{|\\Omega|} = \\frac{|\\{5, 6\\}|}{6} = \\frac{1}{3}\n\\]\n\n\n\n\nSince the possible outcomes less than or equal to 4 are 1, 2, 3, and 4, my event \\(E_\\leq = \\{1, 2, 3, 4\\}\\), so I can compute the probability of \\(E_\\leq\\) as \\[\n\\Pr(E_\\leq) = \\frac{|E_\\leq|}{|\\Omega|} = \\frac{|\\{1, 2, 3, 4|}{6} = \\frac{2}{3}\n\\]\n\n\n\nAnd now I can use R with these probabilities I computed in my head to derive the probabilities of them occurring in a particular sequence:\n\n\n# Encode probabilities of my two events\np_greater &lt;- 1/3\np_less_or_equal &lt;- 2/3\n# Use them to compute the probabilities of sequences\np_sequence &lt;- p_greater * p_less_or_equal * p_greater\np_sequence\n\n[1] 0.07407407\n\n\nThis approach may seem boring at first, since again we could have done this just on a calculator, but it becomes interesting when we ask you to vary the numbers to see what happens to the resulting probabilities.\nFor example, if you wrote the above code for some problem, then in the next problem we could say “It turns out the person providing you with the dice was cheating! They rigged it so that there’s actually a 99% chance of getting a number greater than 4! Go back and compute the probability of this sequence but using the rigged die”\nAnd then you could just change the first line of your code, and re-run the whole thing, to obtain the new probability:\n\np_greater &lt;- 0.99\np_less_or_equal &lt;- 0.01\np_sequence &lt;- p_greater * p_less_or_equal * p_greater\np_sequence\n\n[1] 0.009801\n\n\nFinally, we might then ask you something like:\n“Your friend actually invented a machine that can generated any rigged die at all, so that the probability of rolling a number greater than 4 is whatever they want it to be.\nYou are worried because you always play a game with this friend where you win if you roll something greater than 4, then less than or equal to 4, then greater than 4. So you want to see how the friend’s rigging machine could affect your likelihood of winning.\nWrite a function that computes your likelihood of winning (i.e., your likelihood of getting this sequence of three rolls), for any die with any probability of producing an outcome greater than 4.\nIn this case, you could now take your code and transform it into a function, like\n\ncompute_win_prob &lt;- function(p_greater) {\n    p_less_or_equal &lt;- 1 - p_greater\n    p_sequence &lt;- p_greater * p_less_or_equal * p_greater\n    return(p_sequence)\n}\n\nAnd then test it on some values:\n\ncompute_win_prob(0.01)\n\n[1] 9.9e-05\n\ncompute_win_prob(0.5)\n\n[1] 0.125\n\ncompute_win_prob(0.99)\n\n[1] 0.009801\n\n\nThen we could ask you to think about\nWhat value for p_greater do you think would maximize your probability of winning?\nand then\nProduce a plot showing your likelihood of winning for 1000 different values of p_greater, evenly spread between 0 and 1\nand you could use the function you wrote like\n\nlibrary(ggplot2)\np_greater_vals &lt;- seq(from = 0, to = 1, length.out = 1000)\n#p_greater_vals\nwin_prob_vals &lt;- sapply(p_greater_vals, compute_win_prob)\nwin_df &lt;- tibble(p_greater=p_greater_vals, win_prob=win_prob_vals)\nggplot(win_df, aes(x=p_greater, y=win_prob)) +\n  geom_line() +\n  dsan_theme(\"full\") +\n  labs(\n    title = \"Probability of Winning With Rigged Die\",\n    x = \"Pr(Result &gt; 4)\",\n    y = \"Pr(Win)\"\n  )\n\n\n\n\n\n\n\n\nAnd finally, we could ask you to draw a conclusion about this situation:\nLooking at the plot, approximately what value for the rigged die looks like it produces the maximum likelihood of winning? Now write code to compute the actual value, out of the 1000 evenly-spaced values you computed the win probability for, that maximizes your probability of winning. Does it match your guess from earlier?\nFor which you could write code like\n\nmax_prob &lt;- max(win_df$win_prob)\nmax_row &lt;- win_df %&gt;% filter(win_prob == max_prob)\nmax_row\n\n\n\n\n\np_greater\nwin_prob\n\n\n\n\n0.6666667\n0.1481481\n\n\n\n\n\n\nMy point in writing all this out is just to say: that was a whole other way to compute probabilities using a computer, where the focus wasn’t on making a big tibble and then extracting smaller chunks out of it and dividing their sizes. We did have to construct a tibble, in order to compute the win probabilities for a spectrum of values, but that was just so we could generate a plot (since ggplot() needed to know specifically what points to put at what coordinates).\nSo, in this approach, we don’t have a single tibble throughout the entire problem that serves as our sample space. We just use tibbles when we need them, as tools for doing math or making plots, while the probabilities themselves are still getting computed using simple multiplication.\nAnd, I mentioned above how the tibble-subsetting method doesn’t allow us to deal with continuous probability distributions. We’ve now seen, on the other hand, how this “fancy calculator” approach is able to deal with such distributions: we were really looking at the distribution of all possible rigged dice, meaning, the space of all dice with \\(\\Pr(\\text{roll greater than 4}) = p\\) for \\(p \\in [0,1]\\). We had to approximate this continuous space using a grid of 1000 samples, which brought us closer to the fancy-spreadsheet approach, but hopefully this can help make it a bit clearer how the fancy-calculator approach can help us when the fancy-spreadsheet approach can’t, even if it (the fancy-calculator approach) requires us to do a lot more math in our heads."
  },
  {
    "objectID": "writeups/computing-probabilities/index.html#footnotes",
    "href": "writeups/computing-probabilities/index.html#footnotes",
    "title": "Two Ways to Compute Probabilities in R/Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’re a Python absolutist, you can take tibble here and replace it with pd.DataFrame, for example. If you’re a base-R absolutist you can replace it with data.frame.↩︎\nThe %% operator in R performs modular division: a %% b produces the remainder after dividing a by b (for example, 7 %% 3 produces 1).↩︎"
  },
  {
    "objectID": "writeups/deriving-pdf/index.html",
    "href": "writeups/deriving-pdf/index.html",
    "title": "Deriving a pdf from Scratch",
    "section": "",
    "text": "Setting Random Seed\n\n\n\n(At the beginning of code where you’re dealing with distributions, make sure you set the random seed to 5100, or at least some constant value, to make sure results are reproducible across different computers at different times)\n\nset.seed(5100)\n# This just loads some global ggplot settings, you\n# can uncomment this if trying to run on your own/on Colab\nsource(\"../../dsan-globals/_globals.r\")\nOn the Lab 5 Assignment, I think a lot of students might be wrestling with what’s “going on” on Problem 1, so I wanted to make a quick writeup with some intuition around where to start on this problem.\nWhereas in Problem 2 you are given a CDF (there are unknown variables, but you have something concrete to start “doing math” with), in Problem 1 it may seem at first like you don’t have enough information to complete the problem, since you only have information about a constraint on the possible values that \\(X\\) and \\(Y\\) can take on, rather than (e.g.) the actual probability density that the pdf \\(f_{X,Y}(x,y)\\) should take on.\nIn reality, though, that’s part of the difficulty of the problem: namely, how to derive a valid pdf from only these tiny scraps of information!"
  },
  {
    "objectID": "writeups/deriving-pdf/index.html#necessary-vs.-sufficient-conditions",
    "href": "writeups/deriving-pdf/index.html#necessary-vs.-sufficient-conditions",
    "title": "Deriving a pdf from Scratch",
    "section": "Necessary vs. Sufficient Conditions",
    "text": "Necessary vs. Sufficient Conditions\nBefore we start, in case the distinction between necessary and sufficient conditions isn’t something you’ve learned before:\n\nIf [a predicate] \\(p\\) is necessary for [a predicate] \\(q\\), then if we know \\(p\\) is false then \\(q\\) must be false.\n\nFor example, let \\(p = [x &gt; 3]\\) and \\(q = [x &gt; 5]\\).\nThen \\(p\\) is a necessary condition for \\(q\\), because in this case if we know that \\(p\\) is false \\(q\\) must also be false.\nNote that the converse does not hold! That is, knowing that \\(p\\) is true does not tell us that \\(q\\) is true: if we know \\(p\\) (so we know that \\(x &gt; 3\\)), we still don’t know \\(q\\), since we could have e.g. \\(x = 4\\).\n\nIf \\(p\\) is sufficient for \\(q\\), then if we know \\(p\\) is true we also know that \\(q\\) is true.\n\nFor example, let \\(p = [x\\text{ is divisible by }4]\\) and \\(q = [x\\text{ is even}]\\)\nThen \\(p\\) is a sufficient condition for \\(q\\), since all numbers divisible by 4 are also even.\nAgain you have to be careful, because the converse does not hold: knowing that \\(q\\) is false in this example does not tell us that \\(p\\) is false: if we know \\(\\neg q\\), we know \\(x\\) is not divisible by 4, but \\(p\\) can still be true, since e.g. \\(x\\) could be 6 (an even number not divisible by 4).\n\n\nI think keeping this in mind is helpful for this specific problem, because we can think about the following two necessary conditions that must hold for a given function \\(f_Z(v)\\) to represent a valid pdf for the random variable \\(Z\\).\n\n\n\n\n\n\nGoing from Single-Valued to Multi-Valued pdfs\n\n\n\nI use \\(Z\\) to define the pdf \\(f_Z(v)\\) here to emphasize how pdfs are (at their core) defined for single random variables like \\(X\\), \\(Y\\), or \\(Z\\).\nNonetheless, as we enter into multivariable probability world we can consider \\(f_Z(v)\\) as the pdf for a vector-valued random variable \\(Z\\), so that \\(Z\\) can in fact represent a point that is decomposable into an \\(x\\)-coordinate (represented by a random variable \\(X\\)) and a \\(y\\)-coordinate (represented by a random variable \\(Y\\)). In this case, we can rewrite \\(Z\\) as \\((X,Y)\\) and call our pdf \\(f_{(X,Y)}(v_X, x_Y)\\), which for succinctness we usually shorten to just \\(f_{X,Y}(x,y)\\)."
  },
  {
    "objectID": "writeups/deriving-pdf/index.html#two-necessary-conditions-for-a-valid-pdf-which-are-jointly-sufficient",
    "href": "writeups/deriving-pdf/index.html#two-necessary-conditions-for-a-valid-pdf-which-are-jointly-sufficient",
    "title": "Deriving a pdf from Scratch",
    "section": "Two Necessary Conditions for a Valid pdf Which Are Jointly Sufficient",
    "text": "Two Necessary Conditions for a Valid pdf Which Are Jointly Sufficient\nGiven the above definitions+examples of necessary vs. sufficient conditions, here (if we call the first condition \\(NC_1\\) and the second \\(NC_2\\)) they are individually necessary but jointly sufficient: meaning that if we define a new condition \\(B\\) for “both”, such that \\(B = (NC_1 \\wedge NC_2)\\), then \\(B\\) is in fact a sufficient condition for \\(f\\) to be a valid pdf. If we can ensure that both of the necessary conditions given below are true, then we know that our function \\(f_Z\\) defines a valid pdf for a random variable \\(Z\\).\n\nNecessary Condition 1 (\\(NC_1\\), Single-Variable): The integral of the pdf \\(f_Z(v)\\) over \\(\\mathcal{R}_Z\\) (where \\(\\mathcal{R}_Z\\) is the support of \\(Z\\)) must equal 1.\nNecessary Condition 1 (\\(NC_1\\), Multi-Value): The double-integral of the pdf \\(f_{X,Y}(x,y)\\) over \\(\\mathcal{R}_X\\) and \\(\\mathcal{R}_Y\\) must equal 1.\nNecessary Condition 2 (\\(NC_2\\), Single-Value): If \\(Z\\) is defined over a range \\([a, b]\\), then the pdf \\(f_Z\\) must assign nonzero probability density to all non-empty one-dimensional intervals of radius \\(\\varepsilon\\) around \\(v \\in [a,b]\\), and zero probability density to all other intervals.\nNecessary Condition 2 (\\(NC_2\\), Multi-Value): If \\(X\\) is defined over a range \\([a_X, b_X]\\) and \\(Y\\) is defined over a range \\([a_Y, b_Y]\\), then the pdf \\(f_{X,Y}\\) must assign nonzero probability density to all non-empty two-dimensional circles of radius \\(\\varepsilon\\) around points \\(\\{ (x,y) \\mid x \\in [a_X,b_X]\\text{ and }y \\in [a_Y,b_Y]\\}\\).\n\nI know Necessary Condition 2 is extremely scary-looking and confusing, but it is mainly that way to handle extremely “weird” cases where we’d like to define probability distributions over bizarre sets like the Sierpiński triangle.\nSince in Problem 1 on the Lab Assignment we are working with a nicely-behaved set (the set that you plotted in part 1, which is just a triangle taking up the bottom 1/2 of the unit square), we can transform Necessary Condition 2 into a much more intuitive version\n\nNecessary Condition 2 (\\(NC_2\\), RVs Defined over “Well-Behaved” Spaces): If \\(X\\) is defined over a range \\([a_X, b_X]\\) and \\(Y\\) is defined over a range \\([a_Y, b_Y]\\), then the pdf \\(f_{X,Y}\\) must assign nonzero probability density to all points \\(\\{(x,y) \\mid x \\in [a_X,b_X]\\text{ and }y \\in [a_Y,b_Y]\\}\\).\n\nIf the difference between the “well-behaved” case and the general case above is not clear, compare the definitions closely (in the “simplified version” just given, we don’t have to worry about circles around points, just points themselves)."
  },
  {
    "objectID": "writeups/deriving-pdf/index.html#why-are-you-telling-us-all-this-how-does-it-help-solve-problem-1",
    "href": "writeups/deriving-pdf/index.html#why-are-you-telling-us-all-this-how-does-it-help-solve-problem-1",
    "title": "Deriving a pdf from Scratch",
    "section": "Why Are You Telling Us All This? How Does It Help Solve Problem 1?",
    "text": "Why Are You Telling Us All This? How Does It Help Solve Problem 1?\nIf you’ve made it all the way to this point, you may be frustrated that I still haven’t pointed exactly to how these two conditions help us solve Problem 1.\nSince I can’t give away the solution for that particular problem, here I will show the applicability of these two conditions to a similar problem, and then your job is to think about how the way I work through this problem can help you work through problem 1.\nThe example problem: Given two random variables \\(X \\sim \\mathcal{U}[0,1]\\) and \\(Y \\sim \\mathcal{U}[0,1]\\), consider the joint distribution of the vector-valued random variable \\(Z = (X,Y)\\), where the possible realizations of \\(Z\\) are restricted to only those values of \\(X\\) and \\(Y\\) such that they lie within a circle of radius \\(1\\) around the origin: that is, \\(X^2 + Y^2 &lt; 1\\). This means that we could create a plot which “fills in” the possible values of \\(Z\\) more and more as we sample more points, by generating values of \\(X\\) sampled from from \\(\\mathcal{U}[0,1]\\) and values of \\(Y\\) sampled from \\(\\mathcal{U}[0,1]\\), then placing the points on the plot if they are “admissible” (if they form a valid realization of \\(Z\\)) and throwing the points away otherwise:\n\nlibrary(tidyverse)\nN &lt;- 2500\nx_vals &lt;- runif(N, -1, 1)\ny_vals &lt;- runif(N, -1, 1)\nsample_df &lt;- tibble(x=x_vals, y=y_vals)\nsample_df &lt;- sample_df |&gt;\n  mutate(\n    admissible = x^2 + y^2 &lt; 1\n  )\nsample_df |&gt; head()\n\n\n\n\n\nx\ny\nadmissible\n\n\n\n\n0.0853844\n0.4159406\nTRUE\n\n\n-0.4991692\n-0.6671818\nTRUE\n\n\n-0.5449181\n0.9697597\nFALSE\n\n\n-0.2142207\n-0.8855359\nTRUE\n\n\n0.8015973\n0.2324585\nTRUE\n\n\n-0.6283825\n-0.0055527\nTRUE\n\n\n\n\n\n\nWe see that, given how admissible is defined, plotting only the points for which admissible == TRUE will mean that we are “filling out” the subset of all possible values in the square \\([-1,1] \\times [-1,1]\\) that satisfy our contstraint:\n\nadmissible_df &lt;- sample_df |&gt;\n  filter(admissible)\nggplot(admissible_df, aes(x=x, y=y)) +\n  geom_point() +\n  dsan_theme() +\n  # If you were wondering how to make it an actual perfect circle\n  coord_fixed()\n\n\n\n\n\n\n\n\nAnd now we can finally use our two necessary conditions, albeit in reverse order:\n\nApplying Necessary Condition 2 (Simplified Version) To This Case:\nTo satisfy this condition, let’s literally just define a pdf \\(f_{X,Y}\\) that has some non-zero value \\(c\\) for points within the circle in the above plot, and has value \\(0\\) otherwise:\n\\[\nf_{X,Y}(x,y) = \\begin{cases}\nc &\\text{if }x^2 + y^2 &lt; 1, \\\\\n0 &\\text{otherwise.}\n\\end{cases}\n\\]\nSo far, we’ve satisfied Necessary Condition 1 (\\(NC_1\\)) for \\(f_{X,Y}\\) to be a valid pdf. Now if we can figure out how to also make this function satisfy Necessary Condition 2 (\\(NC_2\\)), we’ll know that we have a valid pdf for \\(Z = (X,Y)\\).\n\n\nApplying Necessary Condition 1 To This Case\nGiven that \\(X\\) and \\(Y\\) are both defined (in a nicely-behaved way) over the range \\([-1,1]\\), the remaining condition \\(NC_1\\) is satisfied if the following equality holds:\n\\[\n\\int_{-1}^{1}\\int_{-1}^{1}f_{X,Y}(x,y)dxdy = 1\n\\tag{1}\\]\nIf you calculate these two definite integrals (you could do it using fancy math, like variable substitution, or just using geometry, e.g. by thinking about what the integral would be given what you know about the areas of circles), you will find that this integral-filled equality reduces to the equality\n\\[\nc\\pi = 1\n\\]\nSo that, finally, we can solve for the value of \\(c\\) which now lets us “fill in this detail”: that if we choose \\(c = \\frac{1}{\\pi}\\), so that\n\\[\nf_{X,Y}(x,y) = \\begin{cases}\n\\frac{1}{\\pi} &\\text{if }x^2 + y^2 &lt; 1, \\\\\n0 &\\text{otherwise.}\n\\end{cases}\n\\]\nthen we get a nice chain of realizations:\n\nThe equality Equation 1 holds, so\nWe have satisfied \\(NC_1\\), after already satisfying \\(NC_2\\) above, so\nWe have now satisfied both \\(NC_1\\) and \\(NC_2\\),\nThis means we have satisfied \\(B\\), so that finally\nWe have defined a valid pdf \\(f_{X,Y}(x,y)\\) (since \\(B\\) is a sufficient condition for valid pdfs).\n\nI hope that helps somewhat: I wrote things out in excruciating detail, using necessary and sufficient conditions and etc., to try and show how there is a general logic to these types of problems (or at least, a general logic for how we can use the information we’re given to construct a distribution which encodes this information)."
  },
  {
    "objectID": "w02/slides.html#prof.-jeff-introduction",
    "href": "w02/slides.html#prof.-jeff-introduction",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Prof. Jeff Introduction!",
    "text": "Prof. Jeff Introduction!\n\nBorn and raised in NW DC → high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Economics (2008-2012)"
  },
  {
    "objectID": "w02/slides.html#grad-school",
    "href": "w02/slides.html#grad-school",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grad School",
    "text": "Grad School\n\nStudied abroad in Beijing (Peking University/北大) → internship with Huawei in Hong Kong (HKUST)\n\n\n\n\nStanford for MS in Computer Science (2012-2014)\nResearch Economist at UC Berkeley (2014-2015)\n\n\n\n\n\nColumbia (NYC) for PhD[+Postdoc] in Political Science (2015-2023)"
  },
  {
    "objectID": "w02/slides.html#dissertation-political-science-history",
    "href": "w02/slides.html#dissertation-political-science-history",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Dissertation (Political Science + History)",
    "text": "Dissertation (Political Science + History)\n“Our Word is Our Weapon”: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada"
  },
  {
    "objectID": "w02/slides.html#research-labor-economics",
    "href": "w02/slides.html#research-labor-economics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Research (Labor Economics)",
    "text": "Research (Labor Economics)\n\n\n\n“Monopsony in Online Labor Markets”: Machine Learning to enhance causal estimates of the effect of job description language on uptake rate\n\n\n\n“Freedom as Non-Domination in the Labor Market”: Game-theoretic models of workers’ rights (monopsony vs. labor discipline)\n\n\n\n\n“Unsupervised Extraction of Workplace Rights and Duties from Collective Bargaining Agreements”: Linguistic (dependency) parses of contracts → time series of worker vs. employer rights and responsibilities over time"
  },
  {
    "objectID": "w02/slides.html#deterministic-processes",
    "href": "w02/slides.html#deterministic-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Deterministic Processes",
    "text": "Deterministic Processes\n\nGiven a set of inputs, we can compute the outcome exactly\nExample: Given the radius of a circle, we can compute its area without any uncertainty. \\(r \\mapsto \\pi r^2\\)\n(The fact that we can compute the outcome doesn’t mean that it’s easy to do so! See, e.g., the double pendulum)\n\n\nImage credit: Tenor.com\nThe pendulum example points to the fact that the notion of a chaotic system, one which is “sensitive to initial conditions”, is different from that of a stochastic system."
  },
  {
    "objectID": "w02/slides.html#holy-grail-deterministic-model-newtonian-physics",
    "href": "w02/slides.html#holy-grail-deterministic-model-newtonian-physics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "“Holy Grail” Deterministic Model: Newtonian Physics",
    "text": "“Holy Grail” Deterministic Model: Newtonian Physics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto F_g = G\\frac{m_1m_2}{r^2}\n\\]\n\n\nFigure 1: Newton’s Law of Universal Gravitation← Dr. Zirkel follows Newton’s famous steps. Coloured wood engraving. Wellcome Collection (Public Domain)"
  },
  {
    "objectID": "w02/slides.html#but-what-happens-when",
    "href": "w02/slides.html#but-what-happens-when",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "But What Happens When…",
    "text": "But What Happens When…\n\\[\n\\text{Outcome}\\left(\\text{Dice Roll}\\right) = \\; ?\\frac{?_1?_2}{?^2}\n\\]\n\n\n\nPre-Enlightenment\n\n\n\n\nHans Sebald Beham, Fortuna (1541), via Wikimedia Commons\n\n\n\n\nPost-Enlightenment\n\n\n\n\nBlaise Pascal, Traité du triangle arithmétique (1665), via Internet Archive"
  },
  {
    "objectID": "w02/slides.html#random-processes",
    "href": "w02/slides.html#random-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Processes",
    "text": "Random Processes\n\n\n\nCan’t compute the outcome exactly, but can still say something about potential outcomes!\nExample: randomly chosen radius \\(r \\in [0,1]\\), what can we say about \\(A = \\pi r^2\\)?\nUnif: \\([0,\\pi]\\) equally likely\nExp: closer to \\(0\\) more likely\n\n\n\n\n\nCode\nplot_circ_with_distr &lt;- function(N, radii, ptitle, alpha=0.1) {\n  theta &lt;- seq(0, 360, 4)\n  #hist(radii)\n  circ_df &lt;- expand.grid(x = theta, y = radii)\n  #circ_df\n  ggplot(circ_df, aes(x = x, y = y, group = y)) +\n      geom_path(alpha = alpha, color = cbPalette[1], linewidth=g_linesize) +\n      # Plot the full unit circle\n      geom_path(data = data.frame(x = theta, y = 1), aes(x = x), linewidth=g_linesize) +\n      geom_point(data = data.frame(x = 0, y = 0), aes(x = x), size = g_pointsize) +\n      coord_polar(theta = \"x\", start = -pi / 2, direction = -1) +\n      ylim(0, 1) +\n      # scale_x_continuous(limits=c(0,360), breaks=seq(0,360,by=45)) +\n      scale_x_continuous(limits = c(0, 360), breaks = NULL) +\n      dsan_theme(\"quarter\") +\n      labs(\n          title = ptitle,\n          x = NULL,\n          y = NULL\n      ) +\n      # See https://stackoverflow.com/a/19821839\n      theme(\n          axis.line = element_blank(),\n          axis.text = element_blank(),\n          axis.ticks = element_blank(),\n          axis.title = element_blank(),\n          panel.border = element_blank(),\n          panel.grid.major=element_blank(),\n          panel.grid.minor=element_blank(),\n          plot.margin = unit(c(0,0,0,0), \"cm\"),\n          title = element_text(size=18)\n      )\n}\nN &lt;- 500\nradii &lt;- runif(N, 0, 1)\ntitle &lt;- paste0(N, \" Uniformly-Distributed Radii\")\nalpha &lt;- 0.2\nplot_circ_with_distr(N, radii, title, alpha)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nN &lt;- 1000\nradii &lt;- rexp(N, 4)\ntitle &lt;- paste0(N, \" Exponentially-Distributed Radii\")\nplot_circ_with_distr(N, radii, title, alpha=0.15)"
  },
  {
    "objectID": "w02/slides.html#data-ground-truth-noise",
    "href": "w02/slides.html#data-ground-truth-noise",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Data = Ground Truth + Noise",
    "text": "Data = Ground Truth + Noise\n\nDepressing but true origin of statistics (as opposed to probability): the Plague 😷\n\n\n\n\n\n\nGround Truth: The Great Plague (Lord Have Mercy on London, Unknown Artist, circa 1665, via Wikimedia Commons)\n\n\n\n\n\n\nNoisy Data (Recorded amidst chaos): London Bill of Mortality, 1665 (Public Domain, Wellcome Collection)"
  },
  {
    "objectID": "w02/slides.html#random-variables",
    "href": "w02/slides.html#random-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nIn algebra, to solve problems we work with variables\nIn probability theory, to solve problems we work with random variables\nRecall the difference between random and deterministic: \\(A = \\pi r^2\\) tells us that, given a value of \\(r\\), we can solve for the unique value of \\(A\\)\nIn probability theory, however, there is no one “true” value of a random variable \\(X\\).\nLet \\(X = f(N)\\) mean that \\(X\\) is the result of a rolled die, where the die has \\(N\\) sides.\nPlugging in \\(N = 6\\) (standard 6-sided die) still doesn’t mean we know “the” value of \\(X\\). However, (if the die is fair) we do know\n\n\\[\n\\Pr(X = 1) = \\Pr(X = 2) = \\cdots = \\Pr(X = 6) = \\frac{1}{6}\n\\]"
  },
  {
    "objectID": "w02/slides.html#discrete-vs.-continuous",
    "href": "w02/slides.html#discrete-vs.-continuous",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Discrete vs. Continuous",
    "text": "Discrete vs. Continuous\n\nMany complicated definitions, often misleading or unintuitive!\nHow I want you to remember: How many possible values between two known values?\nDiscrete: e.g., number of siblings\n\nI have 2 siblings, you have 3 siblings… How many values (sibling counts) in between?\n\nContinuous: e.g., temperature\n\nIt is 27.0° C in my room, 28.0° C in your room… How many values (temperatures) in between?\n\nSo, if \\(X\\) is the result of a rolled die, is \\(X\\) discrete or continuous? How many values can be rolled between 3 and 4?"
  },
  {
    "objectID": "w02/slides.html#thinking-about-independence",
    "href": "w02/slides.html#thinking-about-independence",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Thinking About Independence",
    "text": "Thinking About Independence\n\nWe’ll define it formally later; for now, this is our working definition:\n\n\n\n\n\n Working Definition: Independence\n\n\nTwo random variables \\(X\\) and \\(Y\\) are independent if learning information about \\(X\\) does not give you information about the value of \\(Y\\), or vice-versa."
  },
  {
    "objectID": "w02/slides.html#naïve-definition-of-probability",
    "href": "w02/slides.html#naïve-definition-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Naïve Definition of Probability",
    "text": "Naïve Definition of Probability\n\nSample Space: The set of all possible outcomes of an experiment\nEvent: A subset of the sample space\n\n\n\n\n\n Naïve Definition of Probability\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]"
  },
  {
    "objectID": "w02/slides.html#example-flipping-two-coins",
    "href": "w02/slides.html#example-flipping-two-coins",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\n\n\n\n Naïve Definition of Probability\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\n\n\nSample space \\(S = \\{TT, TH, HT, HH\\}\\)\nEvent \\(E_1\\): Result of first flip is \\(H\\), result of second flip is \\(T\\) \\(\\implies\\) \\(E_1 = \\{HT\\}\\)\nEvent \\(E_2\\): At least one \\(H\\) \\(\\implies\\) \\(E_2 = \\{TH, HT, HH\\}\\)\n\n\\[\n\\begin{align*}\n\\Pr(E_1) &= \\frac{|\\{HT\\}|}{|S|} = \\frac{|\\{HT\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{1}{4} \\\\\n\\Pr(E_2) &= \\frac{|\\{TH, HT, HH\\}|}{|S|} = \\frac{|\\{TH, HT, HH\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{3}{4}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#events-neq-outcomes",
    "href": "w02/slides.html#events-neq-outcomes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Events \\(\\neq\\) Outcomes!",
    "text": "Events \\(\\neq\\) Outcomes!\n\nOutcomes are things, events are sets of things\nSubtle but extremely important distinction!\nIn the coin flip example:\n\nThe event \\(E_1 = \\{HT\\}\\) can be confused with the outcome \\(HT\\).\nSo, try to remember instead the event \\(E_2 = \\{TH, HT, HH\\}\\): it is more clear, in this case, how this event does not correspond to any individual outcome"
  },
  {
    "objectID": "w02/slides.html#back-to-the-naïve-definition",
    "href": "w02/slides.html#back-to-the-naïve-definition",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Back to the Naïve Definition",
    "text": "Back to the Naïve Definition\n\n\n\n\n Naïve Definition of Probability\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\n\n\nThe naïve definition tells us that probabilities are just ratios of counts:\n\nCount the number of ways the event \\(E\\) can happen, count the total number of things that can happen, and divide!\n\nThis is why we begin studying probability by studying combinatorics: the mathematics of counting"
  },
  {
    "objectID": "w02/slides.html#combinatorics-ice-cream-possibilities",
    "href": "w02/slides.html#combinatorics-ice-cream-possibilities",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Combinatorics: Ice Cream Possibilities",
    "text": "Combinatorics: Ice Cream Possibilities\n\n\n\n\n\nThe \\(6 = \\color{red}\\boxed{\\color{black}2 \\cdot 3}\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second.\n\n\n\n\n\n\nThe \\(6 = \\color{red}\\boxed{\\color{black}3 \\cdot 2}\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second."
  },
  {
    "objectID": "w02/slides.html#grouping-vs.-ordering",
    "href": "w02/slides.html#grouping-vs.-ordering",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grouping vs. Ordering",
    "text": "Grouping vs. Ordering\n\nIn standard statistics/combinatorics introductions you’ll learn different counting formulas for when order matters vs. when order doesn’t matter\nThis is not a mathematical distinction so much as a pragmatic distinction: what are you trying to accomplish by counting?\nProblems with extremely similar descriptions can differ in small detail, so that the units you need to distinguish between in one version differ from the units you need to distinguish between in the other."
  },
  {
    "objectID": "w02/slides.html#does-order-matter",
    "href": "w02/slides.html#does-order-matter",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Does Order Matter?",
    "text": "Does Order Matter?\n\n\n\n\n Example: Student Government vs. Student Sports\n\n\n\nConsider a school where students can either try out for swim team or run for a position in student government\nThe swim team has 4 slots, but slots aren’t differentiated: you’re either on the team (one of the chosen students) or not\nThe student government also has 4 slots, but there is a difference between the slots: first slot is President, second is Vice President, third is Secretary, and fourth is Treasurer.\n\n\n\n\n\n\nSimple case (for intuition): school only has 4 students. In this case, how many ways are there to form the swim team? What about the student government?\n\nSwim team: \\(1\\) way. You have only one choice, let all 4 students onto team\nStudent government: \\(4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\\) ways. You have to let all 4 in, but you have a choice of who is President, Vice President, Secretary, and Treasurer\n\nHow did we get \\(4 \\cdot 3 \\cdot 2 \\cdot 1\\)? (Think about the ice cream example…)\n\nStart by choosing the President: 4 choices\nNow choose the Vice President: only 3 students left to choose from\nNow choose the Secretary: only 2 students left to choose from\nNow choose the Treasurer: only 1 student left to choose from"
  },
  {
    "objectID": "w02/slides.html#permutations-vs.-combinations",
    "href": "w02/slides.html#permutations-vs.-combinations",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Permutations vs. Combinations",
    "text": "Permutations vs. Combinations\n\nPermutations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order within groups matters: \\(P_{n,k}\\) (sometimes written \\(_nP_k\\)).\n\nIn this case, we want to count \\((a,b)\\) and \\((b,a)\\) as two separate groups\n\nCombinations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order in the groups doesn’t matter: \\(C_{n,k}\\) (sometimes written \\(_nC_k,\\binom{n}{k}\\)).\n\nIn this case, we don’t want to count \\((a, b)\\) and \\((b, a)\\) as two separate groups…\n\n\n\\[\n\\begin{align*}\nP_{n,k} = \\frac{n!}{(n-k)!}, \\; C_{n,k} = \\frac{n!}{k!(n-k)!}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#no-need-to-memorize",
    "href": "w02/slides.html#no-need-to-memorize",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "No Need to Memorize!",
    "text": "No Need to Memorize!\n\nKey point: you don’t have to remember these as two separate formulas!\nThe number of combinations is based on the number of permutations, but corrected for double counting: e.g., corrected for the fact that \\((a,b) \\neq (b,a)\\) when counting permutations but \\((a,b) = (b,a)\\) when counting combinations.\n\n\\[\nC_{n,k} = \\frac{P_{n,k}}{k!} \\genfrac{}{}{0pt}{}{\\leftarrow \\text{Permutations}}{\\leftarrow \\text{Duplicate groups}}\n\\]\nWhere does \\(k!\\) come from? (How many different orderings can we make of the same group?)\n\n\\(k = 2\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 2\\)\n\\(k = 3\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 6\\)\n\\(k = 4\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{4 choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 24\\)"
  },
  {
    "objectID": "w02/slides.html#with-or-without-replacement",
    "href": "w02/slides.html#with-or-without-replacement",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "With or Without Replacement?",
    "text": "With or Without Replacement?\n\nBoils down to: can the same object be included in my sample more than once?\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\nMost statistical problems: “Check off” objects as you collect data about them, so that each observation in your data is unique\nSpecial (but important!) set of statistical problems: let objects appear in your sample multiple times, to “squeeze” more information out of the sample (called Bootstrapping—much more later in the course!)"
  },
  {
    "objectID": "w02/slides.html#how-many-possible-samples",
    "href": "w02/slides.html#how-many-possible-samples",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "How Many Possible Samples?",
    "text": "How Many Possible Samples?\nExample: From \\(N = 3\\) population, how many ways can we take samples of size \\(k = 2\\)?\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(3 \\cdot 2 = 6\\) ways (3 objects to choose from for first element of sample, 2 remaining objects to choose from for second element of sample)\n\\(3\\cdot 3 = 3^2 = 9\\) ways (3 objects to choose from for first element of sample, still 3 objects to choose from for second element of sample)\n\n\n\nGeneral Case: From population of size \\(N\\), how many ways can we take samples of size \\(k\\)? (Try to extrapolate from above example before looking at answer!)\n\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(\\displaystyle \\underbrace{N \\cdot (N-1) \\cdot \\cdots \\cdot (N - k + 1)}_{k\\text{ times}} = \\frac{N!}{(N - k )!}\\)(This formula should look somewhat familiar…)\n\\(\\displaystyle \\underbrace{N \\cdot N \\cdot \\cdots \\cdot N}_{k\\text{ times}} = N^k\\)"
  },
  {
    "objectID": "w02/slides.html#logic-sets-and-probability",
    "href": "w02/slides.html#logic-sets-and-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic, Sets, and Probability",
    "text": "Logic, Sets, and Probability\nThere is a deep connection1 between the objects and operations of logic, set theory, and probability:\n\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\nObjects\nPredicates\\(p, q \\in \\{T, F\\}\\)\nSets\\(S = \\{a, b, \\ldots\\}\\)\nEvents\\(E = \\{TH, HT, HH\\}\\)\n\n\nConjunction\nAnd (\\(\\wedge\\))\\(p \\wedge q\\)\nIntersection (\\(\\cap\\))\\(A \\cap B\\)\nMultiplication (\\(\\times\\)):\\(\\Pr(E_1 \\cap E_2) = \\Pr(E_1)\\times \\Pr(E_2)\\)\n\n\nDisjunction\nOr (\\(\\vee\\))\\(p \\vee q\\)\nUnion (\\(\\cup\\))\\(A \\cup B\\)\nAddition (\\(+\\)): \\(\\Pr(E_1 \\cup E_2) =\\)\\(\\Pr(E_1) + \\Pr(E_2) - \\Pr(E_1 \\wedge E_2)\\)\n\n\nNegation\nNot (\\(\\neg\\))\\(\\neg p\\)\nComplement (\\(^c\\))\\(S^c\\)\nSubtract from 1\\(\\Pr(A^c) = 1 - \\Pr(A)\\)\n\n\n\nFor math majors, an isomorphism"
  },
  {
    "objectID": "w02/slides.html#example-flipping-two-coins-1",
    "href": "w02/slides.html#example-flipping-two-coins-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\nLogic: We can define 4 predicates:\n\n\\(p_1\\) = “First result is \\(H\\)”, \\(q_1\\) = “First result is \\(T\\)”\n\\(p_2\\) = “Second result is \\(H\\)”, \\(q_2\\) = “Second result is \\(T\\)”\n\nLogical formulas:\n\n\\(f_1 = p_1 \\wedge q_2\\): “First result is \\(H\\) and second result is \\(T\\)”\n\\(f_2 = p_1 \\vee q_2\\): “First result is \\(H\\) or second result is \\(T\\)”\n\\(f_3 = \\neg p_1\\): “First result is not \\(H\\)”\n\nThe issue?: We don’t know, until after the coins have been flipped, whether these are true or false!\nBut, we should still be able to say something about their likelihood, for example, whether \\(f_1\\) or \\(f_2\\) is more likely to happen… Enter probability theory!"
  },
  {
    "objectID": "w02/slides.html#logic-rightarrow-probability",
    "href": "w02/slides.html#logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic \\(\\rightarrow\\) Probability",
    "text": "Logic \\(\\rightarrow\\) Probability\n\nProbability theory lets us reason about the uncertainty surrounding logical predicates like \\(p\\) and \\(q\\), by:\n\nencoding them as sets of possibilities \\(P\\) and \\(Q\\), and\nrepresenting uncertainty around a given possibility using a probability measure \\(\\Pr: S \\mapsto [0,1]\\),\n\nthus allowing us to reason about\n\nthe likelihood of these set-encoded predicates on their own: \\(\\Pr(P)\\) and \\(\\Pr(Q)\\), but also\ntheir logical connections: \\(\\Pr(p \\wedge q) = \\Pr(P \\cap Q)\\), \\(\\Pr(\\neg p) = \\Pr(P^c)\\), and so on."
  },
  {
    "objectID": "w02/slides.html#flipping-two-coins-logic-rightarrow-probability",
    "href": "w02/slides.html#flipping-two-coins-logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability",
    "text": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability\n\nReturning to the two coins example: we can look at the predicates and see that they exhaust all possibilities, so that we can define a sample space \\(S = \\{TT, TH, HT, HH\\}\\) of all possible outcomes of our coin-flipping experiment, noting that \\(|S| = 4\\), so there are 4 possible outcomes.\nThen we can associate each predicate with an event, a subset of the sample space, and use our naïve definition to compute the probability of these events:\n\n\n\n\n\n\n\n\n\nPredicate\nEvent\nProbability\n\n\n\n\n\\(p_1\\) = “First result is \\(H\\)”\n\\(P_1 = \\{HT, HH\\}\\)\n\\(\\Pr(P_1) = \\frac{|P_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_1\\) = “First result is \\(T\\)”\n\\(Q_1 = \\{TT, TH\\}\\)\n\\(\\Pr(Q_1) = \\frac{|Q_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(p_2\\) = “Second result is \\(H\\)”\n\\(P_2 = \\{TH, HH\\}\\)\n\\(\\Pr(P_2) = \\frac{|P_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_2\\) = “Second result is \\(T\\)”\n\\(Q_2 = \\{TT, HT\\}\\)\n\\(\\Pr(Q_2) = \\frac{|Q_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "w02/slides.html#moving-from-predicates-to-formulas",
    "href": "w02/slides.html#moving-from-predicates-to-formulas",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Moving from Predicates to Formulas",
    "text": "Moving from Predicates to Formulas\n\nNotice that, in the four rows of the previous table, we were only computing the probabilities of “simple” events: events corresponding to a single predicate\nBut we promised that probability theory lets us compute probabilities for logical formulas as well! …The magic of encoding events as sets becomes clear:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\[ \\begin{align*} F_1 &= P_1 \\cap Q_2 \\\\ &= \\{HT, HH\\} \\cap \\{TT, HT\\} \\\\ &= \\{HT\\} \\end{align*} \\]\n\\[\\begin{align*} \\Pr(F_1) &= \\Pr(\\{HT\\}) \\\\ &= \\frac{|\\{HT\\}|}{|S|} = \\frac{1}{4} \\phantom{= \\frac{1}{4} = \\frac{1}{4}} \\end{align*}\\]\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\[\\begin{align*} F_2 &= P_1 \\cup Q_2 \\\\ &= \\{HT, HH\\} \\cup \\{TT, HT\\} \\\\ &= \\{TT, HT, HH\\} \\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_2) &= \\Pr(\\{TT, HT, HH\\}) \\\\ &= \\frac{|\\{TT, HT, HH\\}|}{|S|} = \\frac{3}{4} \\end{align*}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\[\\begin{align*} F_3 &= P_1^c \\\\ &= \\{HT, HH\\}^c \\phantom{\\cup \\{TT, HT\\}} \\\\ &= \\{TT, TH\\}\\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_3) &= \\Pr(\\{TT, TH\\}) \\\\ &= \\frac{|\\{TT, TH\\}|}{|S|} = \\frac{2}{4} = \\frac{1}{2} \\end{align*}\\]"
  },
  {
    "objectID": "w02/slides.html#using-rules-of-probability",
    "href": "w02/slides.html#using-rules-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Using “Rules” of Probability",
    "text": "Using “Rules” of Probability\n\nHopefully, you found all this churning through set theory to be tedious… ☠️\nThis is where rules of probability come from! They simplify set-theoretic computations into simple multiplications, additions, and subtractions:\n\n\\(\\Pr(A \\cap B) = \\Pr(A) \\times \\Pr(B)\\)\n\\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\)\n\\(\\Pr(A^c) = 1 - \\Pr(A)\\)\n\nSince we know probabilities of the “simple” events \\(P_1\\), \\(Q_1\\), \\(P_2\\), \\(Q_2\\), we don’t need to “look inside them”! Just take the probabilities and multiply/add/subtract as needed:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\(F_1 = P_1 \\cap Q_2\\)\n\\(\\Pr(F_1) = \\Pr(P_1) \\times \\Pr(Q_2) = \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{4}\\)\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\(F_2 = P_1 \\cup Q_2\\)\n\\[\\textstyle{\\begin{align*} \\textstyle \\Pr(F_2) &= \\Pr(P_1) + \\Pr(Q_2) - \\Pr(P_1 \\cap Q_2) \\\\ \\textstyle &= \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{4} = \\frac{3}{4} \\end{align*}}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\(F_3 = P_1^c\\)\n\\(\\Pr(F_3) = 1 - \\Pr(P_1) = 1 - \\frac{1}{2} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "w02/slides.html#importing-results-from-logic",
    "href": "w02/slides.html#importing-results-from-logic",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "“Importing” Results from Logic",
    "text": "“Importing” Results from Logic\n\nThis deep connection between the three fields means that, if we have some useful theorem or formula from one field, we can immediately put it to use in another!\nFor example: DeMorgan’s Laws were developed in logic (DeMorgan was a 19th-century logician), and basically just tell us how to distribute logic operators:\n\n\\[\n\\begin{align*}\n\\underbrace{\\neg(p \\wedge q)}_{\\text{``}p\\text{ and }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\vee \\neg q}_{p\\text{ is not true or }q\\text{ is not true}} \\\\\n\\underbrace{\\neg(p \\vee q)}_{\\text{``}p\\text{ or }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\wedge \\neg q}_{p\\text{ is not true and }q\\text{ is not true}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#converting-to-probability-theory",
    "href": "w02/slides.html#converting-to-probability-theory",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Converting to Probability Theory",
    "text": "Converting to Probability Theory\n\nSo, using the same principles we used in our coin flipping examples, we can consider events \\(P\\) and \\(Q\\), and get the following “translation” of DeMorgan’s Laws:\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\n\\(\\neg(p \\wedge q) = \\neg p \\vee \\neg q\\)\n\\((P \\cap Q)^c = P^c \\cup Q^c\\)\n\\(\\Pr((P \\cap Q)^c) = \\Pr(P^c \\cup Q^c)\\)\n\n\n\\(\\neg(p \\vee q) = \\neg p \\wedge \\neg q\\)\n\\((P \\cup Q)^c = P^c \\cap Q^c\\)\n\\(\\Pr((P \\cup Q)^c) = \\Pr(P^c \\cap Q^c)\\)\n\n\n\n\nNote that, since these are isomorphic to one another, we could have derived DeMorgan’s Laws from within probability theory, rather than the other way around:\n\n\\[\n\\begin{align*}\n\\Pr((P \\cap Q)^c) &= 1 - \\Pr(P \\cap Q) = 1 - \\Pr(P)\\Pr(Q) \\\\\n&= 1 - (1-\\Pr(P^c))(1 - \\Pr(Q^c)) \\\\\n&= 1 - [1 - \\Pr(P^c) - \\Pr(Q^c) + \\Pr(P^c)\\Pr(Q^c)] \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c)\\Pr(Q^c) \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c \\cap Q^c) \\\\\n&= \\Pr(P^c \\cup Q^c) \\; ✅\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#random-variables-1",
    "href": "w02/slides.html#random-variables-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nRecall our discussion of random variables: used by analogy to algebra, since we can do math with them:\nJust as \\(2 \\cdot 3\\) is shorthand for \\(2 + 2 + 2\\), can define \\(X\\) as shorthand for possible outcomes of random process:\n\n\\[\n\\begin{align*}\nS = \\{ &\\text{result of dice roll is 1}, \\\\\n&\\text{result of dice roll is 2}, \\\\\n&\\text{result of dice roll is 3}, \\\\\n&\\text{result of dice roll is 4}, \\\\\n&\\text{result of dice roll is 5}, \\\\\n&\\text{result of dice roll is 6}\\} \\rightsquigarrow X \\in \\{1,\\ldots,6\\}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#random-variables-as-events",
    "href": "w02/slides.html#random-variables-as-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables as Events",
    "text": "Random Variables as Events\n\nEach value \\(v_X\\) that a random variable \\(X\\) can take on gives rise to an event \\(X = v_X\\): the event that the random variable \\(X\\) takes on value \\(v\\).\nSince \\(X = v_X\\) is an event, we can compute its probability \\(\\Pr(X = v_X)\\)!\n\n\n\n\nEvent in words\nEvent in terms of RV\n\n\n\n\nResult of dice roll is 1\n\\(X = 1\\)\n\n\nResult of dice roll is 2\n\\(X = 2\\)\n\n\nResult of dice roll is 3\n\\(X = 3\\)\n\n\nResult of dice roll is 4\n\\(X = 4\\)\n\n\nResult of dice roll is 5\n\\(X = 5\\)\n\n\nResult of dice roll is 6\n\\(X = 6\\)"
  },
  {
    "objectID": "w02/slides.html#doing-math-with-events",
    "href": "w02/slides.html#doing-math-with-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Doing Math with Events",
    "text": "Doing Math with Events\n\nWe’ve seen how \\(\\Pr()\\) “encodes” logical expressions involving uncertain outcomes.\nEven more powerful when paired with the notion of random variables: lets us also “encode” mathematical expressions involving uncertain quantities!\nConsider an experiment where we roll two dice. Let \\(X\\) be the RV encoding the outcome of the first roll, and \\(Y\\) be the RV encoding the outcome of the second roll.\nWe can compute probabilities involving \\(X\\) and \\(Y\\) separately, e.g., \\(\\Pr(X = 1) = \\frac{1}{6}\\), but we can also reason probabilistically about mathematical expressions involving \\(X\\) and \\(Y\\)! For example, we can reason about their sum:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{rolls sum to 10}) &= \\Pr(X + Y = 10) \\\\\n&= \\Pr(Y = 10 - X)\n\\end{align*}\n\\]\n\nOr about how the outcome of one roll will relate to the outcome of the other:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{first roll above mean}) &= \\Pr\\left(X &gt; \\frac{X+Y}{2}\\right) \\\\\n&= \\Pr(2X &gt; X+Y) = \\Pr(X &gt; Y)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w02/slides.html#are-random-variables-all-powerful",
    "href": "w02/slides.html#are-random-variables-all-powerful",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Are Random Variables All-Powerful??",
    "text": "Are Random Variables All-Powerful??\n\nJust remember that probability \\(P(\\cdot)\\) is always probability of an event—random variables are just shorthand for quantifiable events.\nNot all events can be simplified via random variables!\n\n\\(\\text{catch a fish} \\mapsto P(\\text{trout}), P(\\text{bass}), \\ldots\\)\n\nWhat types of events can be quantified like this?\n\n(Hint: It has to do with a key topic in the early weeks of both DSAN 5000 and 5100…)\n\n\n\nThe answer is, broadly, any situation where you’re modeling things, like dice rolls, where mathematical operations like addition, multiplication, etc. make sense. So, if we’re modeling dice, it makes sense to say e.g. “result is 6” + “result is 3” = “total is 9”. More on the next page!"
  },
  {
    "objectID": "w02/slides.html#recall-types-of-variables",
    "href": "w02/slides.html#recall-types-of-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Recall: Types of Variables",
    "text": "Recall: Types of Variables\n\nCategorical\n\nNo meaningful way to order values: \\(\\{\\text{trout}, \\text{bass}, \\ldots \\}\\)\n\nOrdinal\n\nCan place in order (bigger, smaller), though gaps aren’t meaningful: \\(\\{{\\color{orange}\\text{great}},{\\color{orange}\\text{greater}},{\\color{orange}\\text{greatest}}\\}\\)\n\\({\\color{orange}\\text{greater}} \\overset{?}{=} 2\\cdot {\\color{orange}\\text{great}} - 1\\)\n\nCardinal\n\nCan place in order, and gaps are meaningful \\(\\implies\\) can do “standard” math with them! Example: \\(\\{{\\color{blue}1},{\\color{blue}2},\\ldots,{\\color{blue}10}\\}\\)\n\\({\\color{blue}7} \\overset{\\text{~✅~}}{=} 2 \\cdot {\\color{blue}4} - 1\\)\nIf events have this structure (meaningful way to define multiplication, addition, subtraction), then we can analyze them as random variables"
  },
  {
    "objectID": "w02/slides.html#visualizing-discrete-rvs",
    "href": "w02/slides.html#visualizing-discrete-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing Discrete RVs",
    "text": "Visualizing Discrete RVs\n\nUltimate Probability Pro-Tip: When you hear “discrete distribution”, think of a bar graph: \\(x\\)-axis = events, bar height = probability of events\nTwo coins example: \\(X\\) = RV representing number of heads obtained in two coin flips"
  },
  {
    "objectID": "w02/slides.html#preview-visualizing-continuous-rvs",
    "href": "w02/slides.html#preview-visualizing-continuous-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "(Preview:) Visualizing Continuous RVs",
    "text": "(Preview:) Visualizing Continuous RVs\n\nThis works even for continuous distributions, if you focus on the area under the curve instead of the height:"
  },
  {
    "objectID": "w02/slides.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "href": "w02/slides.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Probability Theory Gives Us Distributions for RVs, not Numbers!",
    "text": "Probability Theory Gives Us Distributions for RVs, not Numbers!\n\nWe’re going beyond “base” probability theory if we want to summarize these distributions\nHowever, we can understand a lot about the full distribution by looking at some basic summary statistics. Most common way to summarize:\n\n\n\n\n\n\n\n\n\n\\(\\underbrace{\\text{point estimate}}_{\\text{mean/median}}\\)\n\\(\\pm\\)\n\\(\\underbrace{\\text{uncertainty}}_{\\text{variance/standard deviation}}\\)"
  },
  {
    "objectID": "w02/slides.html#example-game-reviews",
    "href": "w02/slides.html#example-game-reviews",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Game Reviews",
    "text": "Example: Game Reviews\n\n(Data from Metacritic)"
  },
  {
    "objectID": "w02/slides.html#adding-a-single-line",
    "href": "w02/slides.html#adding-a-single-line",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Adding a Single Line",
    "text": "Adding a Single Line\n\n(Data from Metacritic)"
  },
  {
    "objectID": "w02/slides.html#or-a-single-ribbon",
    "href": "w02/slides.html#or-a-single-ribbon",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Or a Single Ribbon",
    "text": "Or a Single Ribbon"
  },
  {
    "objectID": "w02/slides.html#example-the-normal-distribution",
    "href": "w02/slides.html#example-the-normal-distribution",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: The Normal Distribution",
    "text": "Example: The Normal Distribution\n\n\n\n(The distribution you saw a few slides ago)\n\n\n\n\n\n\n\n\n\n\n\n\n“RV \\(X\\) is normally distributed with mean \\({\\color{purple}\\mu}\\) and standard deviation \\({\\color{purple}\\sigma}\\)”\n\nTranslates to \\(X \\sim \\mathcal{N}(\\color{purple}{\\mu},\\color{purple}{\\sigma})\\)1\n\\(\\color{purple}{\\mu}\\) and \\(\\color{purple}{\\sigma}\\) are parameters2: the “knobs” or “sliders” which change the location/shape of the distribution\n\n\n\n\nThe parameters in this case give natural summaries of the data:\n\n\\({\\color{\\purple}\\mu}\\) = center (mean), \\({\\color{purple}\\sigma}\\) = [square root of] variance around center\n\nMean can usually be interpreted intuitively; for standard deviation, can use the 68-95-99.7 rule, which will make more sense relative to some real-world data…\n\nThroughout the course, this “calligraphic” font \\(\\mathcal{N}\\), \\(\\mathcal{D}\\), etc., will be used to denote distributionsThroughout the course, remember, purrple is for purrameters"
  },
  {
    "objectID": "w02/slides.html#real-data-and-the-68-95-99.7-rule",
    "href": "w02/slides.html#real-data-and-the-68-95-99.7-rule",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Real Data and the 68-95-99.7 Rule",
    "text": "Real Data and the 68-95-99.7 Rule\n\n\n\n\nCode\nlibrary(readr)\nheight_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/9a23807fb71a5f6b6c2f37c09eb92ab3/raw/89fc6b8f0c57e41ebf4ce5cdf2b3cad6b2dd798c/sports_heights.csv\")\nmean_height &lt;- mean(height_df$height_cm)\nsd_height &lt;- sd(height_df$height_cm)\nheight_density &lt;- function(x) dnorm(x, mean_height, sd_height)\nm2_sd &lt;- mean_height - 2 * sd_height\nm1_sd &lt;- mean_height - 1 * sd_height\np1_sd &lt;- mean_height + 1 * sd_height\np2_sd &lt;- mean_height + 2 * sd_height\nvlines_data &lt;- tibble::tribble(\n  ~x, ~xend, ~y, ~yend, ~Params,\n  mean_height, mean_height, 0, height_density(mean_height), \"Mean\",\n  m2_sd, m2_sd, 0, height_density(m2_sd), \"SD\",\n  m1_sd, m1_sd, 0, height_density(m1_sd), \"SD\",\n  p1_sd, p1_sd, 0, height_density(p1_sd), \"SD\",\n  p2_sd, p2_sd, 0, height_density(p2_sd), \"SD\"\n)\nggplot(height_df, aes(x = height_cm)) +\n    geom_histogram(aes(y = after_stat(density)), binwidth = 5.0) +\n    #stat_function(fun = height_density, linewidth = g_linewidth) +\n    geom_area(stat = \"function\", fun = height_density, color=\"black\", linewidth = g_linewidth, fill = cbPalette[1], alpha=0.2) +\n    geom_segment(data=vlines_data, aes(x=x, xend=xend, y=y, yend=yend, linetype = Params), linewidth = g_linewidth, color=cbPalette[2]) +\n    labs(\n      title=paste0(\"Distribution of heights (cm), N=\",nrow(height_df),\" athletes\\nMean=\",round(mean_height,2),\", SD=\",round(sd_height,2)),\n      x=\"Height (cm)\",\n      y=\"Probability Density\"\n    ) +\n    dsan_theme(\"full\")\n\n\n\n\n\n\n\n\nFigure 2: Heights (cm) for 18K professional athletes\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: 68-95-99.7 Rule visualized (Wiki Commons)\n\n\n\n\nThe point estimate \\({\\color{purple}\\mu} = 186.48\\) is straightforward: the average height of the athletes is 186.48cm. Using the 68-95-99.7 Rule to interpret the SD, \\({\\color{purple}\\sigma} = 9.7\\), we get:\n\n\n\nAbout 68% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 1\\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 1\\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 1 · 9.7\nand\n186.48 + 1 · 9.7]\n\n\n[176.78\nand\n196.18]\n\n\n\n\n\nAbout 95% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 2 \\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 2 \\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 2 · 9.7\nand\n186.48 + 2 · 9.7]\n\n\n[167.08\nand\n205.88]"
  },
  {
    "objectID": "w02/slides.html#boxplots-comparing-multiple-distributions",
    "href": "w02/slides.html#boxplots-comparing-multiple-distributions",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Boxplots: Comparing Multiple Distributions",
    "text": "Boxplots: Comparing Multiple Distributions\n\n\n\n\n\nFrom Wikimedia Commons\n\n\n\n\n\n\nFrom Wikimedia Commons"
  },
  {
    "objectID": "w02/slides.html#another-option-joyplots",
    "href": "w02/slides.html#another-option-joyplots",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Another Option: Joyplots",
    "text": "Another Option: Joyplots\n\n\n\n\n\n\n\n\nFigure 4: (Iconic album cover)\n\n\n\n\n\n\n\n\n\n\nFigure 5: (Tooting my own horn)"
  },
  {
    "objectID": "w02/slides.html#multivariate-distributions-preview",
    "href": "w02/slides.html#multivariate-distributions-preview",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Multivariate Distributions: Preview",
    "text": "Multivariate Distributions: Preview\n\nThe bivariate normal distribution represents the distribution of two normally-distributed RVs \\(\\mathbf{X} = [\\begin{smallmatrix} X_1 & X_2\\end{smallmatrix}]\\), which may or may not be correlated:\n\n\\[\n\\mathbf{X} = \\begin{bmatrix}X_1 \\\\ X_2\\end{bmatrix}, \\; \\boldsymbol{\\mu} =\n%\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}\n\\begin{bmatrix}\\smash{\\overbrace{\\mu_1}^{\\mathbb{E}[X_1]}} \\\\ \\smash{\\underbrace{\\mu_2}_{\\mathbb{E}[X_2]}}\\end{bmatrix}\n, \\; \\mathbf{\\Sigma} = \\begin{bmatrix}\\smash{\\overbrace{\\sigma_1^2}^{\\text{Var}[X_1]}} & \\smash{\\overbrace{\\rho\\sigma_1\\sigma_2}^{\\text{Cov}[X_1,X_2]}} \\\\ \\smash{\\underbrace{\\rho\\sigma_2\\sigma_1}_{\\text{Cov}[X_2,X_1]}} & \\smash{\\underbrace{\\sigma_2^2}_{\\text{Var}[X_2]}}\\end{bmatrix}\n% \\begin{bmatrix}\\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_2\\sigma_1 & \\sigma_2^2 \\end{bmatrix}\n% = \\begin{bmatrix}\\text{Var}[X_1] & \\text{Cov}[X_1,X_2] \\\\ \\text{Cov}[X_2,X_1] & \\text{Var}[X_2] \\end{bmatrix}\n\\]\n\nBy squishing all this information intro matrices, we can specify the parameters of multivariate-normally-distributed vectors of RVs similarly to how we specify single-dimensional normally-distributed RVs:\n\n\n\\[\n\\begin{align*}\n\\overbrace{X}^{\\mathclap{\\text{scalar}}} &\\sim \\mathcal{N}\\phantom{_k}(\\overbrace{\\mu}^{\\text{scalar}}, \\overbrace{\\sigma}^{\\text{scalar}}) \\tag{Univariate} \\\\\n\\underbrace{\\mathbf{X}}_{\\text{vector}} &\\sim \\boldsymbol{\\mathcal{N}}_k(\\smash{\\underbrace{\\boldsymbol{\\mu}}_{\\text{vector}}}, \\underbrace{\\mathbf{\\Sigma}}_{\\text{matrix}}) \\tag{Multivariate}\n\\end{align*}\n\\]\n\n\nNote: In the future I’ll use the notation \\(\\mathbf{X}_{[a \\times b]}\\) to denote the dimensions of the vectors/matrices, like \\(\\mathbf{X}_{[k \\times 1]} \\sim \\boldsymbol{\\mathcal{N}}_k(\\boldsymbol{\\mu}_{[k \\times 1]}, \\mathbf{\\Sigma}_{[k \\times k]})\\)"
  },
  {
    "objectID": "w02/slides.html#visualizing-3d-distributions-projection",
    "href": "w02/slides.html#visualizing-3d-distributions-projection",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Projection",
    "text": "Visualizing 3D Distributions: Projection\n\nMost of our intuitions about plots come from 2D \\(\\Rightarrow\\) super helpful exercise to take a 3D plot like this and imagine “projecting” it onto different 2D surfaces:\n\n\n(Adapted via LaTeX from StackExchange discussion)"
  },
  {
    "objectID": "w02/slides.html#visualizing-3d-distributions-contours",
    "href": "w02/slides.html#visualizing-3d-distributions-contours",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\nFrom Prof. Hickman’s slides!"
  },
  {
    "objectID": "w02/slides.html#visualizing-3d-distributions-contours-1",
    "href": "w02/slides.html#visualizing-3d-distributions-contours-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\n\n\n\nAlso from Prof. Hickman’s slides!"
  },
  {
    "objectID": "w02/slides.html#link-to-colab-notebook",
    "href": "w02/slides.html#link-to-colab-notebook",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Link to Colab Notebook",
    "text": "Link to Colab Notebook\n\nDSAN 5100-03 Lab 02"
  },
  {
    "objectID": "w02/slides.html#references",
    "href": "w02/slides.html#references",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nDSAN 5100-03 W02: Probabilistic Modeling"
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "",
    "text": "Open slides in new window →",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#prof.-jeff-introduction",
    "href": "w02/index.html#prof.-jeff-introduction",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Prof. Jeff Introduction!",
    "text": "Prof. Jeff Introduction!\n\nBorn and raised in NW DC → high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Economics (2008-2012)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#grad-school",
    "href": "w02/index.html#grad-school",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grad School",
    "text": "Grad School\n\nStudied abroad in Beijing (Peking University/北大) → internship with Huawei in Hong Kong (HKUST)\n\n\n\n\nStanford for MS in Computer Science (2012-2014)\nResearch Economist at UC Berkeley (2014-2015)\n\n\n\n\n\n\nColumbia (NYC) for PhD[+Postdoc] in Political Science (2015-2023)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#dissertation-political-science-history",
    "href": "w02/index.html#dissertation-political-science-history",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Dissertation (Political Science + History)",
    "text": "Dissertation (Political Science + History)\n“Our Word is Our Weapon”: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#research-labor-economics",
    "href": "w02/index.html#research-labor-economics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Research (Labor Economics)",
    "text": "Research (Labor Economics)\n\n\n\n“Monopsony in Online Labor Markets”: Machine Learning to enhance causal estimates of the effect of job description language on uptake rate\n\n\n\n“Freedom as Non-Domination in the Labor Market”: Game-theoretic models of workers’ rights (monopsony vs. labor discipline)\n\n\n\n\n\n“Unsupervised Extraction of Workplace Rights and Duties from Collective Bargaining Agreements”: Linguistic (dependency) parses of contracts → time series of worker vs. employer rights and responsibilities over time",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#deterministic-processes",
    "href": "w02/index.html#deterministic-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Deterministic Processes",
    "text": "Deterministic Processes\n\nGiven a set of inputs, we can compute the outcome exactly\nExample: Given the radius of a circle, we can compute its area without any uncertainty. \\(r \\mapsto \\pi r^2\\)\n(The fact that we can compute the outcome doesn’t mean that it’s easy to do so! See, e.g., the double pendulum)\n\n\n\n\nImage credit: Tenor.com\n\n\n\nThe pendulum example points to the fact that the notion of a chaotic system, one which is “sensitive to initial conditions”, is different from that of a stochastic system.",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#holy-grail-deterministic-model-newtonian-physics",
    "href": "w02/index.html#holy-grail-deterministic-model-newtonian-physics",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "“Holy Grail” Deterministic Model: Newtonian Physics",
    "text": "“Holy Grail” Deterministic Model: Newtonian Physics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto F_g = G\\frac{m_1m_2}{r^2}\n\\]\n\n\nFigure 1: Newton’s Law of Universal Gravitation← Dr. Zirkel follows Newton’s famous steps. Coloured wood engraving. Wellcome Collection (Public Domain)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#but-what-happens-when",
    "href": "w02/index.html#but-what-happens-when",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "But What Happens When…",
    "text": "But What Happens When…\n\\[\n\\text{Outcome}\\left(\\text{Dice Roll}\\right) = \\; ?\\frac{?_1?_2}{?^2}\n\\]\n\n\n\nPre-Enlightenment\n\n\n\n\nHans Sebald Beham, Fortuna (1541), via Wikimedia Commons\n\n\n\n\nPost-Enlightenment\n\n\n\n\nBlaise Pascal, Traité du triangle arithmétique (1665), via Internet Archive",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#random-processes",
    "href": "w02/index.html#random-processes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Processes",
    "text": "Random Processes\n\n\n\nCan’t compute the outcome exactly, but can still say something about potential outcomes!\nExample: randomly chosen radius \\(r \\in [0,1]\\), what can we say about \\(A = \\pi r^2\\)?\nUnif: \\([0,\\pi]\\) equally likely\nExp: closer to \\(0\\) more likely\n\n\n\n\n\nCode\nplot_circ_with_distr &lt;- function(N, radii, ptitle, alpha=0.1) {\n  theta &lt;- seq(0, 360, 4)\n  #hist(radii)\n  circ_df &lt;- expand.grid(x = theta, y = radii)\n  #circ_df\n  ggplot(circ_df, aes(x = x, y = y, group = y)) +\n      geom_path(alpha = alpha, color = cbPalette[1], linewidth=g_linesize) +\n      # Plot the full unit circle\n      geom_path(data = data.frame(x = theta, y = 1), aes(x = x), linewidth=g_linesize) +\n      geom_point(data = data.frame(x = 0, y = 0), aes(x = x), size = g_pointsize) +\n      coord_polar(theta = \"x\", start = -pi / 2, direction = -1) +\n      ylim(0, 1) +\n      # scale_x_continuous(limits=c(0,360), breaks=seq(0,360,by=45)) +\n      scale_x_continuous(limits = c(0, 360), breaks = NULL) +\n      dsan_theme(\"quarter\") +\n      labs(\n          title = ptitle,\n          x = NULL,\n          y = NULL\n      ) +\n      # See https://stackoverflow.com/a/19821839\n      theme(\n          axis.line = element_blank(),\n          axis.text = element_blank(),\n          axis.ticks = element_blank(),\n          axis.title = element_blank(),\n          panel.border = element_blank(),\n          panel.grid.major=element_blank(),\n          panel.grid.minor=element_blank(),\n          plot.margin = unit(c(0,0,0,0), \"cm\"),\n          title = element_text(size=18)\n      )\n}\nN &lt;- 500\nradii &lt;- runif(N, 0, 1)\ntitle &lt;- paste0(N, \" Uniformly-Distributed Radii\")\nalpha &lt;- 0.2\nplot_circ_with_distr(N, radii, title, alpha)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nN &lt;- 1000\nradii &lt;- rexp(N, 4)\ntitle &lt;- paste0(N, \" Exponentially-Distributed Radii\")\nplot_circ_with_distr(N, radii, title, alpha=0.15)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#data-ground-truth-noise",
    "href": "w02/index.html#data-ground-truth-noise",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Data = Ground Truth + Noise",
    "text": "Data = Ground Truth + Noise\n\nDepressing but true origin of statistics (as opposed to probability): the Plague 😷\n\n\n\n\n\n\nGround Truth: The Great Plague (Lord Have Mercy on London, Unknown Artist, circa 1665, via Wikimedia Commons)\n\n\n\n\n\n\nNoisy Data (Recorded amidst chaos): London Bill of Mortality, 1665 (Public Domain, Wellcome Collection)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#random-variables",
    "href": "w02/index.html#random-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nIn algebra, to solve problems we work with variables\nIn probability theory, to solve problems we work with random variables\nRecall the difference between random and deterministic: \\(A = \\pi r^2\\) tells us that, given a value of \\(r\\), we can solve for the unique value of \\(A\\)\nIn probability theory, however, there is no one “true” value of a random variable \\(X\\).\nLet \\(X = f(N)\\) mean that \\(X\\) is the result of a rolled die, where the die has \\(N\\) sides.\nPlugging in \\(N = 6\\) (standard 6-sided die) still doesn’t mean we know “the” value of \\(X\\). However, (if the die is fair) we do know\n\n\\[\n\\Pr(X = 1) = \\Pr(X = 2) = \\cdots = \\Pr(X = 6) = \\frac{1}{6}\n\\]",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#discrete-vs.-continuous",
    "href": "w02/index.html#discrete-vs.-continuous",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Discrete vs. Continuous",
    "text": "Discrete vs. Continuous\n\nMany complicated definitions, often misleading or unintuitive!\nHow I want you to remember: How many possible values between two known values?\nDiscrete: e.g., number of siblings\n\nI have 2 siblings, you have 3 siblings… How many values (sibling counts) in between?\n\nContinuous: e.g., temperature\n\nIt is 27.0° C in my room, 28.0° C in your room… How many values (temperatures) in between?\n\nSo, if \\(X\\) is the result of a rolled die, is \\(X\\) discrete or continuous? How many values can be rolled between 3 and 4?",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#thinking-about-independence",
    "href": "w02/index.html#thinking-about-independence",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Thinking About Independence",
    "text": "Thinking About Independence\n\nWe’ll define it formally later; for now, this is our working definition:\n\n\n\n\n\n\n\n Working Definition: Independence\n\n\n\nTwo random variables \\(X\\) and \\(Y\\) are independent if learning information about \\(X\\) does not give you information about the value of \\(Y\\), or vice-versa.",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#naïve-definition-of-probability",
    "href": "w02/index.html#naïve-definition-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Naïve Definition of Probability",
    "text": "Naïve Definition of Probability\n\nSample Space: The set of all possible outcomes of an experiment\nEvent: A subset of the sample space\n\n\n\n\n\n\n\n Naïve Definition of Probability\n\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#example-flipping-two-coins",
    "href": "w02/index.html#example-flipping-two-coins",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\n\n\n\n\n\n Naïve Definition of Probability\n\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\nSample space \\(S = \\{TT, TH, HT, HH\\}\\)\nEvent \\(E_1\\): Result of first flip is \\(H\\), result of second flip is \\(T\\) \\(\\implies\\) \\(E_1 = \\{HT\\}\\)\nEvent \\(E_2\\): At least one \\(H\\) \\(\\implies\\) \\(E_2 = \\{TH, HT, HH\\}\\)\n\n\\[\n\\begin{align*}\n\\Pr(E_1) &= \\frac{|\\{HT\\}|}{|S|} = \\frac{|\\{HT\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{1}{4} \\\\\n\\Pr(E_2) &= \\frac{|\\{TH, HT, HH\\}|}{|S|} = \\frac{|\\{TH, HT, HH\\}|}{|\\{TT, TH, HT, HH\\}|} = \\frac{3}{4}\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#events-neq-outcomes",
    "href": "w02/index.html#events-neq-outcomes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Events \\(\\neq\\) Outcomes!",
    "text": "Events \\(\\neq\\) Outcomes!\n\nOutcomes are things, events are sets of things\nSubtle but extremely important distinction!\nIn the coin flip example:\n\nThe event \\(E_1 = \\{HT\\}\\) can be confused with the outcome \\(HT\\).\nSo, try to remember instead the event \\(E_2 = \\{TH, HT, HH\\}\\): it is more clear, in this case, how this event does not correspond to any individual outcome",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#back-to-the-naïve-definition",
    "href": "w02/index.html#back-to-the-naïve-definition",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Back to the Naïve Definition",
    "text": "Back to the Naïve Definition\n\n\n\n\n\n\n Naïve Definition of Probability\n\n\n\nGiven a sample space \\(S\\), and an event \\(E \\subset S\\),\n\\[\n\\Pr(\\underbrace{E}_{\\text{event}}) = \\frac{\\text{\\# Favorable Outcomes}}{\\text{\\# Possible Outcomes}} = \\frac{|E|}{|S|}\n\\]\n\n\n\nThe naïve definition tells us that probabilities are just ratios of counts:\n\nCount the number of ways the event \\(E\\) can happen, count the total number of things that can happen, and divide!\n\nThis is why we begin studying probability by studying combinatorics: the mathematics of counting",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#combinatorics-ice-cream-possibilities",
    "href": "w02/index.html#combinatorics-ice-cream-possibilities",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Combinatorics: Ice Cream Possibilities",
    "text": "Combinatorics: Ice Cream Possibilities\n\n\n\n\n\nThe \\(6 = \\color{red}\\boxed{\\color{black}2 \\cdot 3}\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second.\n\n\n\n\n\n\nThe \\(6 = \\color{red}\\boxed{\\color{black}3 \\cdot 2}\\) possible cone+flavor combinations which can result from choosing a flavor first and a cone type second.",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#grouping-vs.-ordering",
    "href": "w02/index.html#grouping-vs.-ordering",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Grouping vs. Ordering",
    "text": "Grouping vs. Ordering\n\nIn standard statistics/combinatorics introductions you’ll learn different counting formulas for when order matters vs. when order doesn’t matter\nThis is not a mathematical distinction so much as a pragmatic distinction: what are you trying to accomplish by counting?\nProblems with extremely similar descriptions can differ in small detail, so that the units you need to distinguish between in one version differ from the units you need to distinguish between in the other.",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#does-order-matter",
    "href": "w02/index.html#does-order-matter",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Does Order Matter?",
    "text": "Does Order Matter?\n\n\n\n\n\n\n Example: Student Government vs. Student Sports\n\n\n\n\nConsider a school where students can either try out for swim team or run for a position in student government\nThe swim team has 4 slots, but slots aren’t differentiated: you’re either on the team (one of the chosen students) or not\nThe student government also has 4 slots, but there is a difference between the slots: first slot is President, second is Vice President, third is Secretary, and fourth is Treasurer.\n\n\n\n\nSimple case (for intuition): school only has 4 students. In this case, how many ways are there to form the swim team? What about the student government?\n\nSwim team: \\(1\\) way. You have only one choice, let all 4 students onto team\nStudent government: \\(4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\\) ways. You have to let all 4 in, but you have a choice of who is President, Vice President, Secretary, and Treasurer\n\nHow did we get \\(4 \\cdot 3 \\cdot 2 \\cdot 1\\)? (Think about the ice cream example…)\n\nStart by choosing the President: 4 choices\nNow choose the Vice President: only 3 students left to choose from\nNow choose the Secretary: only 2 students left to choose from\nNow choose the Treasurer: only 1 student left to choose from",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#permutations-vs.-combinations",
    "href": "w02/index.html#permutations-vs.-combinations",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Permutations vs. Combinations",
    "text": "Permutations vs. Combinations\n\nPermutations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order within groups matters: \\(P_{n,k}\\) (sometimes written \\(_nP_k\\)).\n\nIn this case, we want to count \\((a,b)\\) and \\((b,a)\\) as two separate groups\n\nCombinations: How many ways can I choose groups of size \\(k\\) out of \\(n\\) total objects, where order in the groups doesn’t matter: \\(C_{n,k}\\) (sometimes written \\(_nC_k,\\binom{n}{k}\\)).\n\nIn this case, we don’t want to count \\((a, b)\\) and \\((b, a)\\) as two separate groups…\n\n\n\\[\n\\begin{align*}\nP_{n,k} = \\frac{n!}{(n-k)!}, \\; C_{n,k} = \\frac{n!}{k!(n-k)!}\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#no-need-to-memorize",
    "href": "w02/index.html#no-need-to-memorize",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "No Need to Memorize!",
    "text": "No Need to Memorize!\n\nKey point: you don’t have to remember these as two separate formulas!\nThe number of combinations is based on the number of permutations, but corrected for double counting: e.g., corrected for the fact that \\((a,b) \\neq (b,a)\\) when counting permutations but \\((a,b) = (b,a)\\) when counting combinations.\n\n\\[\nC_{n,k} = \\frac{P_{n,k}}{k!} \\genfrac{}{}{0pt}{}{\\leftarrow \\text{Permutations}}{\\leftarrow \\text{Duplicate groups}}\n\\]\nWhere does \\(k!\\) come from? (How many different orderings can we make of the same group?)\n\n\\(k = 2\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 2\\)\n\\(k = 3\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 choices}},\\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 6\\)\n\\(k = 4\\): \\((\\underbrace{\\boxed{\\phantom{a}}}_{\\text{4 choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{3 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{2 remaining choices}}, \\underbrace{\\boxed{\\phantom{a}}}_{\\text{1 remaining choice}}) \\implies 24\\)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#with-or-without-replacement",
    "href": "w02/index.html#with-or-without-replacement",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "With or Without Replacement?",
    "text": "With or Without Replacement?\n\nBoils down to: can the same object be included in my sample more than once?\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\nMost statistical problems: “Check off” objects as you collect data about them, so that each observation in your data is unique\nSpecial (but important!) set of statistical problems: let objects appear in your sample multiple times, to “squeeze” more information out of the sample (called Bootstrapping—much more later in the course!)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#how-many-possible-samples",
    "href": "w02/index.html#how-many-possible-samples",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "How Many Possible Samples?",
    "text": "How Many Possible Samples?\nExample: From \\(N = 3\\) population, how many ways can we take samples of size \\(k = 2\\)?\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(3 \\cdot 2 = 6\\) ways (3 objects to choose from for first element of sample, 2 remaining objects to choose from for second element of sample)\n\\(3\\cdot 3 = 3^2 = 9\\) ways (3 objects to choose from for first element of sample, still 3 objects to choose from for second element of sample)\n\n\n\nGeneral Case: From population of size \\(N\\), how many ways can we take samples of size \\(k\\)? (Try to extrapolate from above example before looking at answer!)\n\n\n\n\n\n\n\n\nWithout Replacement\nWith Replacement\n\n\n\n\n\\(\\displaystyle \\underbrace{N \\cdot (N-1) \\cdot \\cdots \\cdot (N - k + 1)}_{k\\text{ times}} = \\frac{N!}{(N - k )!}\\)(This formula should look somewhat familiar…)\n\\(\\displaystyle \\underbrace{N \\cdot N \\cdot \\cdots \\cdot N}_{k\\text{ times}} = N^k\\)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#logic-sets-and-probability",
    "href": "w02/index.html#logic-sets-and-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic, Sets, and Probability",
    "text": "Logic, Sets, and Probability\nThere is a deep connection1 between the objects and operations of logic, set theory, and probability:\n\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\nObjects\nPredicates\\(p, q \\in \\{T, F\\}\\)\nSets\\(S = \\{a, b, \\ldots\\}\\)\nEvents\\(E = \\{TH, HT, HH\\}\\)\n\n\nConjunction\nAnd (\\(\\wedge\\))\\(p \\wedge q\\)\nIntersection (\\(\\cap\\))\\(A \\cap B\\)\nMultiplication (\\(\\times\\)):\\(\\Pr(E_1 \\cap E_2) = \\Pr(E_1)\\times \\Pr(E_2)\\)\n\n\nDisjunction\nOr (\\(\\vee\\))\\(p \\vee q\\)\nUnion (\\(\\cup\\))\\(A \\cup B\\)\nAddition (\\(+\\)): \\(\\Pr(E_1 \\cup E_2) =\\)\\(\\Pr(E_1) + \\Pr(E_2) - \\Pr(E_1 \\wedge E_2)\\)\n\n\nNegation\nNot (\\(\\neg\\))\\(\\neg p\\)\nComplement (\\(^c\\))\\(S^c\\)\nSubtract from 1\\(\\Pr(A^c) = 1 - \\Pr(A)\\)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#example-flipping-two-coins-1",
    "href": "w02/index.html#example-flipping-two-coins-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Flipping Two Coins",
    "text": "Example: Flipping Two Coins\n\nLogic: We can define 4 predicates:\n\n\\(p_1\\) = “First result is \\(H\\)”, \\(q_1\\) = “First result is \\(T\\)”\n\\(p_2\\) = “Second result is \\(H\\)”, \\(q_2\\) = “Second result is \\(T\\)”\n\nLogical formulas:\n\n\\(f_1 = p_1 \\wedge q_2\\): “First result is \\(H\\) and second result is \\(T\\)”\n\\(f_2 = p_1 \\vee q_2\\): “First result is \\(H\\) or second result is \\(T\\)”\n\\(f_3 = \\neg p_1\\): “First result is not \\(H\\)”\n\nThe issue?: We don’t know, until after the coins have been flipped, whether these are true or false!\nBut, we should still be able to say something about their likelihood, for example, whether \\(f_1\\) or \\(f_2\\) is more likely to happen… Enter probability theory!",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#logic-rightarrow-probability",
    "href": "w02/index.html#logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Logic \\(\\rightarrow\\) Probability",
    "text": "Logic \\(\\rightarrow\\) Probability\n\nProbability theory lets us reason about the uncertainty surrounding logical predicates like \\(p\\) and \\(q\\), by:\n\nencoding them as sets of possibilities \\(P\\) and \\(Q\\), and\nrepresenting uncertainty around a given possibility using a probability measure \\(\\Pr: S \\mapsto [0,1]\\),\n\nthus allowing us to reason about\n\nthe likelihood of these set-encoded predicates on their own: \\(\\Pr(P)\\) and \\(\\Pr(Q)\\), but also\ntheir logical connections: \\(\\Pr(p \\wedge q) = \\Pr(P \\cap Q)\\), \\(\\Pr(\\neg p) = \\Pr(P^c)\\), and so on.",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#flipping-two-coins-logic-rightarrow-probability",
    "href": "w02/index.html#flipping-two-coins-logic-rightarrow-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability",
    "text": "Flipping Two Coins: Logic \\(\\rightarrow\\) Probability\n\nReturning to the two coins example: we can look at the predicates and see that they exhaust all possibilities, so that we can define a sample space \\(S = \\{TT, TH, HT, HH\\}\\) of all possible outcomes of our coin-flipping experiment, noting that \\(|S| = 4\\), so there are 4 possible outcomes.\nThen we can associate each predicate with an event, a subset of the sample space, and use our naïve definition to compute the probability of these events:\n\n\n\n\n\n\n\n\n\nPredicate\nEvent\nProbability\n\n\n\n\n\\(p_1\\) = “First result is \\(H\\)”\n\\(P_1 = \\{HT, HH\\}\\)\n\\(\\Pr(P_1) = \\frac{|P_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_1\\) = “First result is \\(T\\)”\n\\(Q_1 = \\{TT, TH\\}\\)\n\\(\\Pr(Q_1) = \\frac{|Q_1|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(p_2\\) = “Second result is \\(H\\)”\n\\(P_2 = \\{TH, HH\\}\\)\n\\(\\Pr(P_2) = \\frac{|P_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)\n\n\n\\(q_2\\) = “Second result is \\(T\\)”\n\\(Q_2 = \\{TT, HT\\}\\)\n\\(\\Pr(Q_2) = \\frac{|Q_2|}{|S|} = \\frac{2}{4} = \\frac{1}{2}\\)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#moving-from-predicates-to-formulas",
    "href": "w02/index.html#moving-from-predicates-to-formulas",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Moving from Predicates to Formulas",
    "text": "Moving from Predicates to Formulas\n\nNotice that, in the four rows of the previous table, we were only computing the probabilities of “simple” events: events corresponding to a single predicate\nBut we promised that probability theory lets us compute probabilities for logical formulas as well! …The magic of encoding events as sets becomes clear:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\[ \\begin{align*} F_1 &= P_1 \\cap Q_2 \\\\ &= \\{HT, HH\\} \\cap \\{TT, HT\\} \\\\ &= \\{HT\\} \\end{align*} \\]\n\\[\\begin{align*} \\Pr(F_1) &= \\Pr(\\{HT\\}) \\\\ &= \\frac{|\\{HT\\}|}{|S|} = \\frac{1}{4} \\phantom{= \\frac{1}{4} = \\frac{1}{4}} \\end{align*}\\]\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\[\\begin{align*} F_2 &= P_1 \\cup Q_2 \\\\ &= \\{HT, HH\\} \\cup \\{TT, HT\\} \\\\ &= \\{TT, HT, HH\\} \\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_2) &= \\Pr(\\{TT, HT, HH\\}) \\\\ &= \\frac{|\\{TT, HT, HH\\}|}{|S|} = \\frac{3}{4} \\end{align*}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\[\\begin{align*} F_3 &= P_1^c \\\\ &= \\{HT, HH\\}^c \\phantom{\\cup \\{TT, HT\\}} \\\\ &= \\{TT, TH\\}\\end{align*}\\]\n\\[\\begin{align*} \\Pr(F_3) &= \\Pr(\\{TT, TH\\}) \\\\ &= \\frac{|\\{TT, TH\\}|}{|S|} = \\frac{2}{4} = \\frac{1}{2} \\end{align*}\\]",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#using-rules-of-probability",
    "href": "w02/index.html#using-rules-of-probability",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Using “Rules” of Probability",
    "text": "Using “Rules” of Probability\n\nHopefully, you found all this churning through set theory to be tedious… ☠️\nThis is where rules of probability come from! They simplify set-theoretic computations into simple multiplications, additions, and subtractions:\n\n\\(\\Pr(A \\cap B) = \\Pr(A) \\times \\Pr(B)\\)\n\\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\)\n\\(\\Pr(A^c) = 1 - \\Pr(A)\\)\n\nSince we know probabilities of the “simple” events \\(P_1\\), \\(Q_1\\), \\(P_2\\), \\(Q_2\\), we don’t need to “look inside them”! Just take the probabilities and multiply/add/subtract as needed:\n\n\n\n\n\n\n\n\n\nFormula\nEvent\nProbability\n\n\n\n\n\\(f_1 = p_1 \\wedge q_2\\)\n\\(F_1 = P_1 \\cap Q_2\\)\n\\(\\Pr(F_1) = \\Pr(P_1) \\times \\Pr(Q_2) = \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{4}\\)\n\n\n\\(f_2 = p_1 \\vee q_2\\)\n\\(F_2 = P_1 \\cup Q_2\\)\n\\[\\textstyle{\\begin{align*} \\textstyle \\Pr(F_2) &= \\Pr(P_1) + \\Pr(Q_2) - \\Pr(P_1 \\cap Q_2) \\\\ \\textstyle &= \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{4} = \\frac{3}{4} \\end{align*}}\\]\n\n\n\\(f_3 = \\neg p_1\\)\n\\(F_3 = P_1^c\\)\n\\(\\Pr(F_3) = 1 - \\Pr(P_1) = 1 - \\frac{1}{2} = \\frac{1}{2}\\)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#importing-results-from-logic",
    "href": "w02/index.html#importing-results-from-logic",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "“Importing” Results from Logic",
    "text": "“Importing” Results from Logic\n\nThis deep connection between the three fields means that, if we have some useful theorem or formula from one field, we can immediately put it to use in another!\nFor example: DeMorgan’s Laws were developed in logic (DeMorgan was a 19th-century logician), and basically just tell us how to distribute logic operators:\n\n\\[\n\\begin{align*}\n\\underbrace{\\neg(p \\wedge q)}_{\\text{``}p\\text{ and }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\vee \\neg q}_{p\\text{ is not true or }q\\text{ is not true}} \\\\\n\\underbrace{\\neg(p \\vee q)}_{\\text{``}p\\text{ or }q\\text{'' is not true}} &\\iff \\underbrace{\\neg p \\wedge \\neg q}_{p\\text{ is not true and }q\\text{ is not true}}\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#converting-to-probability-theory",
    "href": "w02/index.html#converting-to-probability-theory",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Converting to Probability Theory",
    "text": "Converting to Probability Theory\n\nSo, using the same principles we used in our coin flipping examples, we can consider events \\(P\\) and \\(Q\\), and get the following “translation” of DeMorgan’s Laws:\n\n\n\n\n\n\n\n\n\nLogic\nSet Theory\nProbability Theory\n\n\n\n\n\\(\\neg(p \\wedge q) = \\neg p \\vee \\neg q\\)\n\\((P \\cap Q)^c = P^c \\cup Q^c\\)\n\\(\\Pr((P \\cap Q)^c) = \\Pr(P^c \\cup Q^c)\\)\n\n\n\\(\\neg(p \\vee q) = \\neg p \\wedge \\neg q\\)\n\\((P \\cup Q)^c = P^c \\cap Q^c\\)\n\\(\\Pr((P \\cup Q)^c) = \\Pr(P^c \\cap Q^c)\\)\n\n\n\n\nNote that, since these are isomorphic to one another, we could have derived DeMorgan’s Laws from within probability theory, rather than the other way around:\n\n\\[\n\\begin{align*}\n\\Pr((P \\cap Q)^c) &= 1 - \\Pr(P \\cap Q) = 1 - \\Pr(P)\\Pr(Q) \\\\\n&= 1 - (1-\\Pr(P^c))(1 - \\Pr(Q^c)) \\\\\n&= 1 - [1 - \\Pr(P^c) - \\Pr(Q^c) + \\Pr(P^c)\\Pr(Q^c)] \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c)\\Pr(Q^c) \\\\\n&= \\Pr(P^c) + \\Pr(Q^c) - \\Pr(P^c \\cap Q^c) \\\\\n&= \\Pr(P^c \\cup Q^c) \\; ✅\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#random-variables-1",
    "href": "w02/index.html#random-variables-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables",
    "text": "Random Variables\n\nRecall our discussion of random variables: used by analogy to algebra, since we can do math with them:\nJust as \\(2 \\cdot 3\\) is shorthand for \\(2 + 2 + 2\\), can define \\(X\\) as shorthand for possible outcomes of random process:\n\n\\[\n\\begin{align*}\nS = \\{ &\\text{result of dice roll is 1}, \\\\\n&\\text{result of dice roll is 2}, \\\\\n&\\text{result of dice roll is 3}, \\\\\n&\\text{result of dice roll is 4}, \\\\\n&\\text{result of dice roll is 5}, \\\\\n&\\text{result of dice roll is 6}\\} \\rightsquigarrow X \\in \\{1,\\ldots,6\\}\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#random-variables-as-events",
    "href": "w02/index.html#random-variables-as-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Random Variables as Events",
    "text": "Random Variables as Events\n\nEach value \\(v_X\\) that a random variable \\(X\\) can take on gives rise to an event \\(X = v_X\\): the event that the random variable \\(X\\) takes on value \\(v\\).\nSince \\(X = v_X\\) is an event, we can compute its probability \\(\\Pr(X = v_X)\\)!\n\n\n\n\nEvent in words\nEvent in terms of RV\n\n\n\n\nResult of dice roll is 1\n\\(X = 1\\)\n\n\nResult of dice roll is 2\n\\(X = 2\\)\n\n\nResult of dice roll is 3\n\\(X = 3\\)\n\n\nResult of dice roll is 4\n\\(X = 4\\)\n\n\nResult of dice roll is 5\n\\(X = 5\\)\n\n\nResult of dice roll is 6\n\\(X = 6\\)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#doing-math-with-events",
    "href": "w02/index.html#doing-math-with-events",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Doing Math with Events",
    "text": "Doing Math with Events\n\nWe’ve seen how \\(\\Pr()\\) “encodes” logical expressions involving uncertain outcomes.\nEven more powerful when paired with the notion of random variables: lets us also “encode” mathematical expressions involving uncertain quantities!\nConsider an experiment where we roll two dice. Let \\(X\\) be the RV encoding the outcome of the first roll, and \\(Y\\) be the RV encoding the outcome of the second roll.\nWe can compute probabilities involving \\(X\\) and \\(Y\\) separately, e.g., \\(\\Pr(X = 1) = \\frac{1}{6}\\), but we can also reason probabilistically about mathematical expressions involving \\(X\\) and \\(Y\\)! For example, we can reason about their sum:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{rolls sum to 10}) &= \\Pr(X + Y = 10) \\\\\n&= \\Pr(Y = 10 - X)\n\\end{align*}\n\\]\n\nOr about how the outcome of one roll will relate to the outcome of the other:\n\n\\[\n\\begin{align*}\n\\Pr(\\text{first roll above mean}) &= \\Pr\\left(X &gt; \\frac{X+Y}{2}\\right) \\\\\n&= \\Pr(2X &gt; X+Y) = \\Pr(X &gt; Y)\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#are-random-variables-all-powerful",
    "href": "w02/index.html#are-random-variables-all-powerful",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Are Random Variables All-Powerful??",
    "text": "Are Random Variables All-Powerful??\n\nJust remember that probability \\(P(\\cdot)\\) is always probability of an event—random variables are just shorthand for quantifiable events.\nNot all events can be simplified via random variables!\n\n\\(\\text{catch a fish} \\mapsto P(\\text{trout}), P(\\text{bass}), \\ldots\\)\n\nWhat types of events can be quantified like this?\n\n(Hint: It has to do with a key topic in the early weeks of both DSAN 5000 and 5100…)\n\n\n\nThe answer is, broadly, any situation where you’re modeling things, like dice rolls, where mathematical operations like addition, multiplication, etc. make sense. So, if we’re modeling dice, it makes sense to say e.g. “result is 6” + “result is 3” = “total is 9”. More on the next page!",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#recall-types-of-variables",
    "href": "w02/index.html#recall-types-of-variables",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Recall: Types of Variables",
    "text": "Recall: Types of Variables\n\nCategorical\n\nNo meaningful way to order values: \\(\\{\\text{trout}, \\text{bass}, \\ldots \\}\\)\n\nOrdinal\n\nCan place in order (bigger, smaller), though gaps aren’t meaningful: \\(\\{{\\color{orange}\\text{great}},{\\color{orange}\\text{greater}},{\\color{orange}\\text{greatest}}\\}\\)\n\\({\\color{orange}\\text{greater}} \\overset{?}{=} 2\\cdot {\\color{orange}\\text{great}} - 1\\)\n\nCardinal\n\nCan place in order, and gaps are meaningful \\(\\implies\\) can do “standard” math with them! Example: \\(\\{{\\color{blue}1},{\\color{blue}2},\\ldots,{\\color{blue}10}\\}\\)\n\\({\\color{blue}7} \\overset{\\text{~✅~}}{=} 2 \\cdot {\\color{blue}4} - 1\\)\nIf events have this structure (meaningful way to define multiplication, addition, subtraction), then we can analyze them as random variables",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#visualizing-discrete-rvs",
    "href": "w02/index.html#visualizing-discrete-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing Discrete RVs",
    "text": "Visualizing Discrete RVs\n\nUltimate Probability Pro-Tip: When you hear “discrete distribution”, think of a bar graph: \\(x\\)-axis = events, bar height = probability of events\nTwo coins example: \\(X\\) = RV representing number of heads obtained in two coin flips",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#preview-visualizing-continuous-rvs",
    "href": "w02/index.html#preview-visualizing-continuous-rvs",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "(Preview:) Visualizing Continuous RVs",
    "text": "(Preview:) Visualizing Continuous RVs\n\nThis works even for continuous distributions, if you focus on the area under the curve instead of the height:\n\n\nfuncShaded &lt;- function(x, lower_bound, upper_bound) {\n    y &lt;- dnorm(x)\n    y[x &lt; lower_bound | x &gt; upper_bound] &lt;- NA\n    return(y)\n}\nfuncShadedBound1 &lt;- function(x) funcShaded(x, -Inf, 0)\nfuncShadedBound2 &lt;- function(x) funcShaded(x, 0.2, 1.8)\nfuncShadedBound3 &lt;- function(x) funcShaded(x, 2, Inf)\n\nnorm_plot &lt;- ggplot(data.frame(x=c(-3,3)), aes(x = x)) +\n    stat_function(fun = dnorm) +\n    labs(\n      title=\"Probability Density, X Normally Distributed\",\n      x=\"Possible Values of X\",\n      y=\"Probability Density\"\n    ) +\n    dsan_theme(\"half\") +\n    theme(legend.position = \"none\") +\n    coord_cartesian(clip = \"off\")\nlabel_df &lt;- tribble(\n  ~x, ~y, ~label,\n  -0.8, 0.1, \"Pr(X &lt; 0) = 0.5\",\n  1.0, 0.05, \"Pr(0.2 &lt; X &lt; 1.8)\\n= 0.385\",\n  2.5,0.1,\"Pr(X &gt; 1.96)\\n= 0.025\"\n)\nshaded_plot &lt;- norm_plot +\n  stat_function(fun = funcShadedBound1, geom = \"area\", fill=cbPalette[1], alpha = 0.5) +\n  stat_function(fun = funcShadedBound2, geom = \"area\", fill=cbPalette[2], alpha = 0.5) +\n  stat_function(fun = funcShadedBound3, geom = \"area\", fill=cbPalette[3], alpha = 0.5) +\n  geom_text(label_df, mapping=aes(x = x, y = y, label = label), size=6)\nshaded_plot",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "href": "w02/index.html#probability-theory-gives-us-distributions-for-rvs-not-numbers",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Probability Theory Gives Us Distributions for RVs, not Numbers!",
    "text": "Probability Theory Gives Us Distributions for RVs, not Numbers!\n\nWe’re going beyond “base” probability theory if we want to summarize these distributions\nHowever, we can understand a lot about the full distribution by looking at some basic summary statistics. Most common way to summarize:\n\n\n\n\n\n\n\n\n\n\\(\\underbrace{\\text{point estimate}}_{\\text{mean/median}}\\)\n\\(\\pm\\)\n\\(\\underbrace{\\text{uncertainty}}_{\\text{variance/standard deviation}}\\)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#example-game-reviews",
    "href": "w02/index.html#example-game-reviews",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: Game Reviews",
    "text": "Example: Game Reviews\n\nlibrary(readr)\nfig_title &lt;- \"Reviews for a Popular Nintendo Switch Game\"\n#score_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/8b2b6a50cef5a682db640e874a14646b/raw/bbe07891a90874d1fe624224c1b82212b1ac8378/totk_scores.csv\")\nscore_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/8b2b6a50cef5a682db640e874a14646b/raw/e3c2b9d258380e817289fbb64f91ba9ed4357d62/totk_scores.csv\")\nmean_score &lt;- mean(score_df$score)\nlibrary(ggplot2)\nggplot(score_df, aes(x=score)) +\n  geom_histogram() +\n  #geom_vline(xintercept=mean_score) +\n  labs(\n    title=fig_title,\n    x=\"Review Score\",\n    y=\"Number of Reviews\"\n  ) +\n  dsan_theme(\"full\")\n\n\n\n\n(Data from Metacritic)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#adding-a-single-line",
    "href": "w02/index.html#adding-a-single-line",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Adding a Single Line",
    "text": "Adding a Single Line\n\nlibrary(readr)\nmean_score &lt;- mean(score_df$score)\nmean_score_label &lt;- sprintf(\"%0.2f\", mean_score)\nlibrary(ggplot2)\nggplot(score_df, aes(x=score)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept=mean_score, linetype=\"dashed\"), color=\"purple\", size=1) +\n  scale_linetype_manual(\"\", values=c(\"dashed\"=\"dashed\"), labels=c(\"dashed\"=\"Mean Score\")) +\n  # Add single additional tick\n  scale_x_continuous(breaks=c(60, 70, 80, 90, mean_score, 100), labels=c(\"60\",\"70\",\"80\",\"90\",mean_score_label,\"100\")) +\n  labs(\n    title=fig_title,\n    x=\"Review Score\",\n    y=\"Number of Reviews\"\n  ) +\n  dsan_theme(\"full\") +\n  theme(\n    legend.title = element_blank(),\n    legend.spacing.y = unit(0, \"mm\")\n  ) +\n  theme(axis.text.x = element_text(colour = c('black', 'black','black', 'black', 'purple', 'black')))\n\n\n\n\n(Data from Metacritic)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#or-a-single-ribbon",
    "href": "w02/index.html#or-a-single-ribbon",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Or a Single Ribbon",
    "text": "Or a Single Ribbon\nlibrary(tibble)\nN &lt;- 10\n# Each x value gets 10 y values\nx &lt;- sort(rep(seq(1,10),10))\ny &lt;- x + rnorm(length(x), 0, 5)\ndf &lt;- tibble(x=x,y=y)\ntotal_N &lt;- nrow(df)\nggplot(df, aes(x=x,y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"column\") +\n  labs(\n    title=paste0(\"N=\",total_N,\" Randomly-Generated Points\")\n  )\n# This time, just the means\nlibrary(dplyr)\nmean_df &lt;- df %&gt;% group_by(x) %&gt;% summarize(mean=mean(y), min=min(y), max=max(y))\nggplot(mean_df, aes(x=x, y=mean)) +\n  geom_ribbon(aes(ymin=min, ymax=max, fill=\"ribbon\"), alpha=0.5) +\n  geom_point(aes(color=\"mean\"), size=g_pointsize) +\n  geom_line(size=g_linesize) +\n  dsan_theme(\"half\") +\n  scale_color_manual(\"\", values=c(\"mean\"=\"black\"), labels=c(\"mean\"=\"Mean\")) +\n  scale_fill_manual(\"\", values=c(\"ribbon\"=cbPalette[1]), labels=c(\"ribbon\"=\"Range\")) +\n  remove_legend_title() +\n  labs(\n    title=paste0(\"Means of N=\",total_N,\" Randomly-Generated Points\")\n  )\nlibrary(tibble)\nN &lt;- 100\n# Each x value gets 10 y values\nx &lt;- sort(rep(seq(1,10),10))\ny &lt;- x + rnorm(length(x), 0, 1)\ndf &lt;- tibble(x=x,y=y)\ntotal_N &lt;- nrow(df)\nggplot(df, aes(x=x,y=y)) +\n  geom_point(size=g_pointsize) +\n  dsan_theme(\"column\") +\n  labs(\n    title=paste0(\"N=\",total_N,\" Randomly-Generated Points\")\n  )\n# This time, just the means\nlibrary(dplyr)\nmean_df &lt;- df %&gt;% group_by(x) %&gt;% summarize(mean=mean(y), min=min(y), max=max(y))\nggplot(mean_df, aes(x=x, y=mean)) +\n  geom_ribbon(aes(ymin=min, ymax=max, fill=\"ribbon\"), alpha=0.5) +\n  geom_point(aes(color=\"mean\"), size=g_pointsize) +\n  geom_line(size=g_linesize) +\n  dsan_theme(\"half\") +\n  scale_color_manual(\"\", values=c(\"mean\"=\"black\"), labels=c(\"mean\"=\"Mean\")) +\n  scale_fill_manual(\"\", values=c(\"ribbon\"=cbPalette[1]), labels=c(\"ribbon\"=\"Range\")) +\n  remove_legend_title() +\n  labs(\n    title=paste0(\"Means of N=\",total_N,\" Randomly-Generated Points\")\n  )",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#example-the-normal-distribution",
    "href": "w02/index.html#example-the-normal-distribution",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Example: The Normal Distribution",
    "text": "Example: The Normal Distribution\n\n\n\n(The distribution you saw a few slides ago)\n\n\nvlines_std_normal &lt;- tibble::tribble(\n  ~x, ~xend, ~y, ~yend, ~Params,\n  0, 0, 0, dnorm(0), \"Mean\",\n  -2, -2, 0, dnorm(-2), \"SD\",\n  -1, -1, 0, dnorm(-1), \"SD\",\n  1, 1, 0, dnorm(1), \"SD\",\n  2, 2, 0, dnorm(2), \"SD\"\n)\nggplot(data.frame(x = c(-3, 3)), aes(x = x)) +\n    stat_function(fun = dnorm, linewidth = g_linewidth) +\n    geom_segment(data=vlines_std_normal, aes(x=x, xend=xend, y=y, yend=yend, linetype = Params), linewidth = g_linewidth, color=\"purple\") +\n    geom_area(stat = \"function\", fun = dnorm, fill = cbPalette[1], xlim = c(-3, 3), alpha=0.2) +\n    #geom_area(stat = \"function\", fun = dnorm, fill = \"blue\", xlim = c(0, 2))\n    dsan_theme(\"quarter\") +\n    labs(\n      x = \"v\",\n      y = \"Density f(v)\"\n    )\n\n\n\n\n\n\n\n\n\n\n“RV \\(X\\) is normally distributed with mean \\({\\color{purple}\\mu}\\) and standard deviation \\({\\color{purple}\\sigma}\\)”\n\nTranslates to \\(X \\sim \\mathcal{N}(\\color{purple}{\\mu},\\color{purple}{\\sigma})\\)2\n\\(\\color{purple}{\\mu}\\) and \\(\\color{purple}{\\sigma}\\) are parameters3: the “knobs” or “sliders” which change the location/shape of the distribution\n\n\n\n\n\nThe parameters in this case give natural summaries of the data:\n\n\\({\\color{\\purple}\\mu}\\) = center (mean), \\({\\color{purple}\\sigma}\\) = [square root of] variance around center\n\nMean can usually be interpreted intuitively; for standard deviation, can use the 68-95-99.7 rule, which will make more sense relative to some real-world data…",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#real-data-and-the-68-95-99.7-rule",
    "href": "w02/index.html#real-data-and-the-68-95-99.7-rule",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Real Data and the 68-95-99.7 Rule",
    "text": "Real Data and the 68-95-99.7 Rule\n\n\n\n\nCode\nlibrary(readr)\nheight_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/9a23807fb71a5f6b6c2f37c09eb92ab3/raw/89fc6b8f0c57e41ebf4ce5cdf2b3cad6b2dd798c/sports_heights.csv\")\nmean_height &lt;- mean(height_df$height_cm)\nsd_height &lt;- sd(height_df$height_cm)\nheight_density &lt;- function(x) dnorm(x, mean_height, sd_height)\nm2_sd &lt;- mean_height - 2 * sd_height\nm1_sd &lt;- mean_height - 1 * sd_height\np1_sd &lt;- mean_height + 1 * sd_height\np2_sd &lt;- mean_height + 2 * sd_height\nvlines_data &lt;- tibble::tribble(\n  ~x, ~xend, ~y, ~yend, ~Params,\n  mean_height, mean_height, 0, height_density(mean_height), \"Mean\",\n  m2_sd, m2_sd, 0, height_density(m2_sd), \"SD\",\n  m1_sd, m1_sd, 0, height_density(m1_sd), \"SD\",\n  p1_sd, p1_sd, 0, height_density(p1_sd), \"SD\",\n  p2_sd, p2_sd, 0, height_density(p2_sd), \"SD\"\n)\nggplot(height_df, aes(x = height_cm)) +\n    geom_histogram(aes(y = after_stat(density)), binwidth = 5.0) +\n    #stat_function(fun = height_density, linewidth = g_linewidth) +\n    geom_area(stat = \"function\", fun = height_density, color=\"black\", linewidth = g_linewidth, fill = cbPalette[1], alpha=0.2) +\n    geom_segment(data=vlines_data, aes(x=x, xend=xend, y=y, yend=yend, linetype = Params), linewidth = g_linewidth, color=cbPalette[2]) +\n    labs(\n      title=paste0(\"Distribution of heights (cm), N=\",nrow(height_df),\" athletes\\nMean=\",round(mean_height,2),\", SD=\",round(sd_height,2)),\n      x=\"Height (cm)\",\n      y=\"Probability Density\"\n    ) +\n    dsan_theme(\"full\")\n\n\n\n\n\n\n\n\nFigure 2: Heights (cm) for 18K professional athletes\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: 68-95-99.7 Rule visualized (Wiki Commons)\n\n\n\n\n\nThe point estimate \\({\\color{purple}\\mu} = 186.48\\) is straightforward: the average height of the athletes is 186.48cm. Using the 68-95-99.7 Rule to interpret the SD, \\({\\color{purple}\\sigma} = 9.7\\), we get:\n\n\n\nAbout 68% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 1\\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 1\\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 1 · 9.7\nand\n186.48 + 1 · 9.7]\n\n\n[176.78\nand\n196.18]\n\n\n\n\n\nAbout 95% of the heights fall between\n\n\n\n\n\n\n\n\n\n[\\({\\color{purple}\\mu} - 2 \\cdot {\\color{purple}\\sigma}\\)\nand\n\\({\\color{purple}\\mu} + 2 \\cdot {\\color{purple}\\sigma}\\)]\n\n\n[186.48 - 2 · 9.7\nand\n186.48 + 2 · 9.7]\n\n\n[167.08\nand\n205.88]",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#boxplots-comparing-multiple-distributions",
    "href": "w02/index.html#boxplots-comparing-multiple-distributions",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Boxplots: Comparing Multiple Distributions",
    "text": "Boxplots: Comparing Multiple Distributions\n\n\n\n\n\nFrom Wikimedia Commons\n\n\n\n\n\n\nFrom Wikimedia Commons",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#another-option-joyplots",
    "href": "w02/index.html#another-option-joyplots",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Another Option: Joyplots",
    "text": "Another Option: Joyplots\n\n\n\n\n\n\n\n\nFigure 4: (Iconic album cover)\n\n\n\n\n\n\n\n\n\n\nFigure 5: (Tooting my own horn)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#multivariate-distributions-preview",
    "href": "w02/index.html#multivariate-distributions-preview",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Multivariate Distributions: Preview",
    "text": "Multivariate Distributions: Preview\n\nThe bivariate normal distribution represents the distribution of two normally-distributed RVs \\(\\mathbf{X} = [\\begin{smallmatrix} X_1 & X_2\\end{smallmatrix}]\\), which may or may not be correlated:\n\n\\[\n\\mathbf{X} = \\begin{bmatrix}X_1 \\\\ X_2\\end{bmatrix}, \\; \\boldsymbol{\\mu} =\n%\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}\n\\begin{bmatrix}\\smash{\\overbrace{\\mu_1}^{\\mathbb{E}[X_1]}} \\\\ \\smash{\\underbrace{\\mu_2}_{\\mathbb{E}[X_2]}}\\end{bmatrix}\n, \\; \\mathbf{\\Sigma} = \\begin{bmatrix}\\smash{\\overbrace{\\sigma_1^2}^{\\text{Var}[X_1]}} & \\smash{\\overbrace{\\rho\\sigma_1\\sigma_2}^{\\text{Cov}[X_1,X_2]}} \\\\ \\smash{\\underbrace{\\rho\\sigma_2\\sigma_1}_{\\text{Cov}[X_2,X_1]}} & \\smash{\\underbrace{\\sigma_2^2}_{\\text{Var}[X_2]}}\\end{bmatrix}\n% \\begin{bmatrix}\\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_2\\sigma_1 & \\sigma_2^2 \\end{bmatrix}\n% = \\begin{bmatrix}\\text{Var}[X_1] & \\text{Cov}[X_1,X_2] \\\\ \\text{Cov}[X_2,X_1] & \\text{Var}[X_2] \\end{bmatrix}\n\\]\n\nBy squishing all this information intro matrices, we can specify the parameters of multivariate-normally-distributed vectors of RVs similarly to how we specify single-dimensional normally-distributed RVs:\n\n\n\\[\n\\begin{align*}\n\\overbrace{X}^{\\mathclap{\\text{scalar}}} &\\sim \\mathcal{N}\\phantom{_k}(\\overbrace{\\mu}^{\\text{scalar}}, \\overbrace{\\sigma}^{\\text{scalar}}) \\tag{Univariate} \\\\\n\\underbrace{\\mathbf{X}}_{\\text{vector}} &\\sim \\boldsymbol{\\mathcal{N}}_k(\\smash{\\underbrace{\\boldsymbol{\\mu}}_{\\text{vector}}}, \\underbrace{\\mathbf{\\Sigma}}_{\\text{matrix}}) \\tag{Multivariate}\n\\end{align*}\n\\]\n\n\nNote: In the future I’ll use the notation \\(\\mathbf{X}_{[a \\times b]}\\) to denote the dimensions of the vectors/matrices, like \\(\\mathbf{X}_{[k \\times 1]} \\sim \\boldsymbol{\\mathcal{N}}_k(\\boldsymbol{\\mu}_{[k \\times 1]}, \\mathbf{\\Sigma}_{[k \\times k]})\\)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#visualizing-3d-distributions-projection",
    "href": "w02/index.html#visualizing-3d-distributions-projection",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Projection",
    "text": "Visualizing 3D Distributions: Projection\n\nMost of our intuitions about plots come from 2D \\(\\Rightarrow\\) super helpful exercise to take a 3D plot like this and imagine “projecting” it onto different 2D surfaces:\n\n\n\n\n(Adapted via LaTeX from StackExchange discussion)",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#visualizing-3d-distributions-contours",
    "href": "w02/index.html#visualizing-3d-distributions-contours",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\n\n\nFrom Prof. Hickman’s slides!",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#visualizing-3d-distributions-contours-1",
    "href": "w02/index.html#visualizing-3d-distributions-contours-1",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Visualizing 3D Distributions: Contours",
    "text": "Visualizing 3D Distributions: Contours\n\n\n\nAlso from Prof. Hickman’s slides!",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#link-to-colab-notebook",
    "href": "w02/index.html#link-to-colab-notebook",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Link to Colab Notebook",
    "text": "Link to Colab Notebook\n\nDSAN 5100-03 Lab 02",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#references",
    "href": "w02/index.html#references",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "w02/index.html#footnotes",
    "href": "w02/index.html#footnotes",
    "title": "Week 2: Introduction to Probabilistic Modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor math majors, an isomorphism↩︎\nThroughout the course, this “calligraphic” font \\(\\mathcal{N}\\), \\(\\mathcal{D}\\), etc., will be used to denote distributions↩︎\nThroughout the course, remember, purrple is for purrameters↩︎",
    "crumbs": [
      "Week 2: Sep 3"
    ]
  },
  {
    "objectID": "recordings-archive/index.html",
    "href": "recordings-archive/index.html",
    "title": "Lecture Recordings Archive",
    "section": "",
    "text": "Use the listing below to view lecture recordings for previous iterations of DSAN 5100!\n\n\n\n\n\n\n\n\n\nWeek\n\n\nTitle\n\n\nLast Updated\n\n\n\n\n\n\n14\n\n\nWeek 14 Presentation Recordings\n\n\nThursday Nov 30, 2023\n\n\n\n\n13\n\n\nWeek 13 Lecture Recording\n\n\nThursday Nov 16, 2023\n\n\n\n\n13\n\n\nWeek 13 Guest Lecture: Dr. Kerrie Carfagno\n\n\nThursday Nov 16, 2023\n\n\n\n\n12\n\n\nWeek 12 Lecture Recording\n\n\nThursday Nov 9, 2023\n\n\n\n\n11\n\n\nWeek 11 Lecture Recording\n\n\nThursday Nov 2, 2023\n\n\n\n\n10\n\n\nWeek 10 Lecture Recording\n\n\nThursday Oct 26, 2023\n\n\n\n\n9\n\n\nWeek 09 Lecture Recording\n\n\nThursday Oct 19, 2023\n\n\n\n\n8\n\n\nWeek 08 Lecture Recording\n\n\nThursday Oct 12, 2023\n\n\n\n\n7\n\n\nWeek 07 Lecture Recording\n\n\nThursday Oct 5, 2023\n\n\n\n\n6\n\n\nWeek 06 Lecture Recording\n\n\nThursday Sep 28, 2023\n\n\n\n\n5\n\n\nWeek 05 Lecture Recording\n\n\nThursday Sep 21, 2023\n\n\n\n\n4\n\n\nWeek 04 Lecture Recording\n\n\nThursday Sep 14, 2023\n\n\n\n\n3\n\n\nWeek 03 Lecture Recording\n\n\nTuesday Sep 5, 2023\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recordings/index.html",
    "href": "recordings/index.html",
    "title": "Lecture Recordings",
    "section": "",
    "text": "The lecture recordings for Fall 2024 will be added here as soon as possible after the end of class.\nYou can view recordings of my lectures from previous semesters here\n\n\n\n\n\n\n\n\n\nWeek\n\n\nTitle\n\n\nLast Updated\n\n\n\n\n\n\n1\n\n\nWeek 01 Lecture Recording\n\n\nMonday Sep 30, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "writeups/marginalization/index.html",
    "href": "writeups/marginalization/index.html",
    "title": "Marginal Distributions and “Marginalizing Out” A Variable",
    "section": "",
    "text": "My last writeup was focused on how to get started on Problem 1 of the Lab 5 Assignment, so in this one we will focus on how to do the parts towards the end of this problem: namely, figuring out what marginal distributions are, and what it means to “marginalize out” a variable, to go from some joint pdf \\(f_{X,Y}(x,y)\\) to a marginal pdf \\(f_X(x)\\) (by “marginalizing out” the variable \\(Y\\)) or \\(f_Y(y)\\) (by “marginalizing out” the variable \\(X\\))."
  },
  {
    "objectID": "writeups/marginalization/index.html#marginalization-in-discrete-world",
    "href": "writeups/marginalization/index.html#marginalization-in-discrete-world",
    "title": "Marginal Distributions and “Marginalizing Out” A Variable",
    "section": "Marginalization In Discrete World",
    "text": "Marginalization In Discrete World\nMarginalization provides an example of where, unlike the case for some topics in the course, having a good intuition for the discrete case will actually help us a ton even when considering the continuous case. So, let’s start out by thinking through a joint discrete distribution, and thinking about what a marginal distribution derived from this joint discrete distribution actually represents.\nConsider a case where we are the principal at a “senior high school” (in the US, this means a high school/secondary school that students attend for grades 10, 11, and 12), where each student is in some grade but also has an honor student status which is 1 if the student is an honor student and 0 otherwise. We can represent the discrete frequency distribution (not that it is not a probability distribution, since the numbers in each “bin” are not between 0 and 1) using the following raw frequency table:\n\n\n\n\n\\(H = 0\\)\n\\(H = 1\\)\n\n\n\n\n\\(G = 10\\)\n10\n5\n\n\n\\(G = 11\\)\n6\n4\n\n\n\\(G = 12\\)\n7\n1\n\n\n\nSo far, we can’t use this table for much of anything, since it doesn’t contain probabilities and we also don’t (yet) know the total number of students, so that we don’t know what proportions of all students fall into each bin.\nSo now let’s say someone asks us the probability that, if we randomly select a student from the school (at uniform), the randomly-selected student will be an honor student in 11th grade.\nBy thinking about what specifically they’re asking for here, we can see that they’re asking us a question about the joint distribution of \\(G\\) and \\(H\\) at the school: specifically, they’re asking us for \\(\\Pr(G = 11, H = 1)\\), a question we can answer if we know the joint distribution \\(f_{G,H}(v_G, v_H)\\).\nUsing our naïve definition of probability, we can compute this probability using the frequencies in the table as\n\\[\n\\Pr(G = 11, H = 1) = \\frac{\\#(G = 11, H = 1)}{\\#\\text{ Students Total}}\n\\]\nand, plugging in the values from the above table, we obtain the answer\n\\[\n\\Pr(G = 11, H = 1) = \\frac{4}{33} \\approx 0.121\n\\]\nFollowing the same logic for all remaining cells in the table, we can convert our frequency table into a probability table by normalizing the counts to be proportions of the total. But, let’s think about two different ways we could compute the total number of students, since we’ll need to think about these two ways later on:\n\nComputing Overall Total by Column (Honors-Status Totals)\nStarting from our original frequency table:\n\n\n\n\n\\(H = 0\\)\n\\(H = 1\\)\n\n\n\n\n\\(G = 10\\)\n10\n5\n\n\n\\(G = 11\\)\n6\n4\n\n\n\\(G = 12\\)\n7\n1\n\n\n\nWe could start computing the total number of students by summing columns, to obtain:\n\n\n\n\n\\(H = 0\\)\n\\(H = 1\\)\n\n\n\n\n\\(G = 10\\)\n10\n5\n\n\n\\(G = 11\\)\n6\n4\n\n\n\\(G = 12\\)\n7\n1\n\n\nTotal\n23\n10\n\n\n\nAnd then we could total these two numbers in the totals row to obtain the overall total:\n\n\n\n\n\\(H = 0\\)\n\\(H = 1\\)\nTotal\n\n\n\n\n\\(G = 10\\)\n10\n5\n\n\n\n\\(G = 11\\)\n6\n4\n\n\n\n\\(G = 12\\)\n7\n1\n\n\n\nTotal\n23\n10\n33\n\n\n\n\n\nComputing Overall Total by Row (Grade)\nAs an alternative to starting our computation of the overall totals by summing columns, we could start by summing rows.\nStarting from our original frequency table:\n\n\n\n\n\\(H = 0\\)\n\\(H = 1\\)\n\n\n\n\n\\(G = 10\\)\n10\n5\n\n\n\\(G = 11\\)\n6\n4\n\n\n\\(G = 12\\)\n7\n1\n\n\n\nWe could first sum each row, to obtain:\n\n\n\n\n\\(H = 0\\)\n\\(H = 1\\)\nTotal\n\n\n\n\n\\(G = 10\\)\n10\n5\n15\n\n\n\\(G = 11\\)\n6\n4\n10\n\n\n\\(G = 12\\)\n7\n1\n8\n\n\n\nAnd then we could total the three partial sums in the totals column to obtain the overall total:\n\n\n\n\n\\(H = 0\\)\n\\(H = 1\\)\nTotal\n\n\n\n\n\\(G = 10\\)\n10\n5\n15\n\n\n\\(G = 11\\)\n6\n4\n10\n\n\n\\(G = 12\\)\n7\n1\n8\n\n\nTotal\n\n\n33\n\n\n\n\n\nBringing Both Methods Together\nNotice how we obtained the same overall total, the total number of students, regardless of which dimension we chose to sum over first. So, let’s make a complete frequency table, where we have not only the frequencies of each bin but also the row totals, column totals, and a cell in the bottom-right representing the overall total:\n\n\n\n\n\\(H = 0\\)\n\\(H = 1\\)\nTotal\n\n\n\n\n\\(G = 10\\)\n10\n5\n15\n\n\n\\(G = 11\\)\n6\n4\n10\n\n\n\\(G = 12\\)\n7\n1\n8\n\n\nTotal\n23\n10\n33\n\n\n\n\n\nConverting Frequencies into Probabilities\nNow, before we think about the totals row/column, let’s use the overall total (33) to convert our counts into probabilities:\n\n\n\n\n\\(H = 0\\)\n\\(H = 1\\)\nTotal\n\n\n\n\n\\(G = 10\\)\n\\(\\frac{10}{33}\\)\n\\(\\frac{5}{33}\\)\n\\(\\frac{15}{33}\\)\n\n\n\\(G = 11\\)\n\\(\\frac{6}{33}\\)\n\\(\\frac{4}{33}\\)\n\\(\\frac{10}{33}\\)\n\n\n\\(G = 12\\)\n\\(\\frac{7}{33}\\)\n\\(\\frac{1}{33}\\)\n\\(\\frac{8}{33}\\)\n\n\nTotal\n\\(\\frac{23}{33}\\)\n\\(\\frac{10}{33}\\)\n\\(\\frac{33}{33}\\)\n\n\n\n\n\nMore Than One Distribution Can Be Derived From This Table!\nNow that we see the normalized counts, we can see the different ways we can look at this table to obtain probability distributions:\n\nFirst, there’s the distribution we were originally thinking about: the entries in each non-total cell form a probabiltiy distribution called the joint distribution of \\(G\\) and \\(H\\), since each entry is between 0 and 1 and the entries sum to 1:\n\n\\[\n\\begin{align*}\n&\\underbrace{\\Pr(G = 10, H = 0)}_{10 / 33} + \\underbrace{\\Pr(G = 10, H = 1)}_{5 / 33} + \\\\\n&\\underbrace{\\Pr(G = 11, H = 0)}_{6 / 33} + \\underbrace{\\Pr(G = 11, H = 1)}_{4 / 33} + \\\\\n&\\underbrace{\\Pr(G = 12, H = 0)}_{7 / 33} + \\underbrace{\\Pr(G = 12, H = 1)}_{1 / 33} = 1\n\\end{align*}\n\\]\n\nBut, we can also notice that the two entries in the totals row (excluding the overall total) form a probability distribution, called the marginal distribution of \\(H\\):\n\n\\[\n\\begin{align*}\n\\underbrace{\\Pr(H = 0)}_{23 / 33} + \\underbrace{\\Pr(H = 1)}_{10 / 33} = 1\n\\end{align*}\n\\]\n\nAnd, the three entries in the totals column (excluding the overall total) form a probability distribution, called the marginal distribution of \\(G\\):\n\n\\[\n\\underbrace{\\Pr(G = 10)}_{15 / 33} + \\underbrace{\\Pr(G = 11)}_{10 / 33} + \\underbrace{\\Pr(G = 12)}_{8 / 33} = 1\n\\]\nLet’s consider in a little more detail how we used the joint distribution—the full set of entries in the original table—to derive two different marginal distributions:\nSumming the values in each column:\nWhen we summed up the values in each column, we were summing up all possible pdf values \\(f_{G,H}(v_G, v_H)\\) where \\(v_H = 0\\) and then all possible pdf values \\(f_{G,H}(v_G, v_H)\\) where \\(v_H = 1\\). This gave us two new counts (or probabilities, if used the probability table rather than the frequency table) representing the marginal distribution of \\(H\\), a distribution where the variable \\(G\\) no longer appeared!\nSumming the values in each row:\nWhen we instead summed the values in each row, we summed all possible pdf values \\(f_{G,H}(v_G, v_H)\\) where \\(v_G = 10\\), then all possible pdf values \\(f_{G,H}(v_G, v_H)\\) where \\(v_G = 11\\), and finally all possible pdf values \\(f_{G,H}(v_G, v_H)\\) where \\(v_G = 12\\). This gave us three new counts/probabilities representing the marginal distribution of \\(G\\), a distribution where the variable \\(H\\) no longer appeared!"
  },
  {
    "objectID": "writeups/marginalization/index.html#whats-missing-conditional-distributions",
    "href": "writeups/marginalization/index.html#whats-missing-conditional-distributions",
    "title": "Marginal Distributions and “Marginalizing Out” A Variable",
    "section": "What’s Missing? Conditional Distributions",
    "text": "What’s Missing? Conditional Distributions\nSo far we’ve discussed the interrelationship between joint distributions and marginal distributions: the latter was obtained from the former by summing.\nHowever, there is one type of distribution that is mentioned on your Lab assignment that we haven’t seen yet. The conditional distribution is a bit different from the joint and marginal distributions, in the sense that the conditional distribution does not represent a sum across some dimension of the table but a slice of the table: that is, it represents the new distribution which “pops out” when we consider one particular row or one particular column of the table.\nHowever, just like how above we figured out the marginal distributions by summing over columns and summing over rows separately, and seeing what resulted, let’s explore what conditional distributions look like by slicing the table by column first, then slicing the table by row:\n\nComputing Conditional Distributions as Columns (Honors-Status Values)\nLet’s consider what the table would look like if we just selected a single column: for example, let’s select just the column of values for which \\(H = 1\\). Throwing away all of the other columns in the table, this would give us a one-column table that looks like:\n\n\n\n\n\\(H = 1\\)\n\n\n\n\n\\(G = 10\\)\n5\n\n\n\\(G = 11\\)\n4\n\n\n\\(G = 12\\)\n1\n\n\nTotal\n10\n\n\n\nNow notice that, just like how we were able to convert our frequency table into a probability table representing joint probabilities by dividing each of these values by 33 (the overall total of students across all possible grades and honors-status values), now we can convert this slice of the full table into its own distribution by dividing each of these individual values in the \\(H = 1\\) column by the total number of students in the \\(H = 1\\) column:\n\n\n\n\n\\(H = 1\\)\n\n\n\n\n\\(G = 10\\)\n\\(\\frac{5}{10}\\)\n\n\n\\(G = 11\\)\n\\(\\frac{4}{10}\\)\n\n\n\\(G = 12\\)\n\\(\\frac{1}{10}\\)\n\n\nTotal\n\\(\\frac{10}{10}\\)\n\n\n\nNotice how, unlike in the above two cases, we now have to interpret the numeric value of the probabilities in each cell differently:\n\nJoint and marginal distributions represented some count relative to the total number of students in the school, hence the denominator of all probabilities in either of these cases was 33\nConditional distributions, however, represent a count relative to the number of students within the category we are conditioning on: the denominator is now 10 in each case, since we’re considering the number of students represented by each cell within the column as a proportion of the total number of students across the entire column."
  },
  {
    "objectID": "writeups/marginalization/index.html#discrete-world-summary",
    "href": "writeups/marginalization/index.html#discrete-world-summary",
    "title": "Marginal Distributions and “Marginalizing Out” A Variable",
    "section": "Discrete World Summary",
    "text": "Discrete World Summary\nSo, as long as we notice that conditional distributions are weird in the sense that they require us to renormalize all of our probabilities from our table of joint and marginal distributions, we now have the link between all three types of distributions that get talked about when working with multivariate probability distributions:\n\nA single joint distribution: The normalized counts in each cell of the full 2D table; in other words, the probability that a randomly-selected student from across the entire school will be in a particular bin (when students are binned by grade and honors status).\n\nFor example, the bin in the upper-left corner of our original table represents non-honors students in 10th grade (\\(G = 10, H = 0\\)).\n\nTwo marginal distributions: The normalized totals across each row, or across each column, of the full 2D table; in other words, the probability that a randomly-selected student from across the entire school will be in a particular aggregated bin.\n\nIf we aggregated by summing columns, for example, our first aggregated bin represents all non-honors students, students for whom \\(H = 0\\) (regardless of grade), and our second aggregated bin contains all honors students, students for whom \\(H = 1\\) (regardless of grade).\n\nSix possible conditional distributions: The re-normalized counts within a particular column or a particular row; in other words, the probability that a randomly-selected student from a particular category (the category we’re conditioning on) will be in one of the bins represented by individual slots within this row/column.\n\nIf we conditioned on the \\(H = 1\\) column, for example, then once we renormalize the counts in this column, the first entry represents the probability of a randomly-selected honors student being in grade 10: \\(\\Pr(G = 10 \\mid H = 1)\\)\n\n\nNotice, lastly, how we could use this intuition built from frequency tables to figure out how to go in opposite directions from the directions we derived things above: for example, if we were only given marginal distributions and conditional distributions, we could use this information to derive the full-on joint distribution table. Since we know the definition of conditional probability for example, that\n\\[\n\\Pr(B \\mid A) = \\frac{\\Pr(B, A)}{\\Pr(A)},\n\\]\nwe could re-arrange terms in this equality to obtain\n\\[\n\\Pr(B, A) = \\Pr(B \\mid A)\\Pr(A),\n\\]\nfrom which we can see that if we know the conditional distribution \\(\\Pr(B \\mid A)\\) and the marginal distribution \\(\\Pr(A)\\), we can combine these (via multiplication) to obtain the joint distribution \\(\\Pr(B,A)\\)."
  },
  {
    "objectID": "writeups/marginalization/index.html#moving-to-continuous-world",
    "href": "writeups/marginalization/index.html#moving-to-continuous-world",
    "title": "Marginal Distributions and “Marginalizing Out” A Variable",
    "section": "Moving to Continuous World",
    "text": "Moving to Continuous World\nThe reason I’ve spent so much time focusing on the discrete case here is because the intuitions we just built do indeed translate naturally into good intuitions for reasoning about continuous distributions!\nTaking our discrete table, for example, we can imagine moving into continuous space the same way that we learned how to take discrete rectangles approximating the the space underneath a curve and convert them into integrals: by taking the limit of the area of these rectangles as they got skinnier and skinnier:\n\n\n\n\n\nIn the above figure, we see how we can imagine summing the area of rectangles approximating the area “under” the curve (really, between the curve and the \\(x\\)-axis), then imagine the rectangles becoming skinnier and skinnier such that the sum of the rectangles’ areas converges to the integral of the function.\nSimilarly, now imagine taking the above table and continuizing our variables: imagine that instead of a student’s progress in school being recorded by a discrete random variable \\(G\\) such that \\(\\mathcal{R}_G = \\{10, 11, 12\\}\\), now we record a student’s progress using a continuous random variable \\(G\\) such that \\(\\mathcal{R}_G = [10,12] \\subset \\mathbb{R}\\).\nBy the same logic, rather than tracking each student’s honor status using a discrete random variable \\(H\\) such that \\(\\mathcal{R}_H = \\{0, 1\\}\\), now we keep track of a student’s honor status using a continuous random variable \\(H\\) such that \\(\\mathcal{R}_H = [0, 1] \\subset \\mathbb{R}\\).\nIn this new continuous world, therefore, we might say that a student near the beginning of 10th grade who is towards the “high end” of the “honors spectrum” might be represented by a pair of values \\(G = 10.03\\) and \\(H = 0.95\\), for example.\nHowever, in this case we can still derive all of the distributions from other distributions in the same way, by:\n\nReplacing the sums we computed above with integrals, and\nReplacing the operation of re-normalization (which we performed to ensure that our probability mass values summed to 1) with the operation of ensuring that our probability density values integrate to 1.\n\nSo, for example, now rather than being given a frequency table, we may just be given the information that students’ \\(G\\) values are distributed according to the continuous uniform distribution, \\(G \\sim \\mathcal{U}(10, 12)\\), that their \\(H\\) values are distributed according to the truncated normal distribution \\(\\mathcal{TN}(\\mu = 0.5, \\sigma = 0.1, a = 0, b = 1)\\)1, and that these two variables are independent, which means that \\(\\Pr(G \\mid H) = \\Pr(G)\\) and \\(\\Pr(H \\mid G) = \\Pr(H)\\) (the conditional distributions are the marginal distributions).\nIn this case, then, we know (by the definition of independence) that we can obtain the joint pdf \\(f_{G,H}(v_G, v_H)\\) by just multiplying the pdf of the marginal distribution of \\(G\\), \\(f_G(v_G)\\) and the pdf of the marginal distribution of \\(H\\), \\(f_H(v_H)\\):\n\\[\nf_{G,H}(v_G, v_H) = f_G(v_G) \\cdot f_H(v_H).\n\\]\nSince we know that \\(G\\) has a continuous uniform distribution, \\(G \\sim \\mathcal{U}(10,12)\\), we know (or we could look up) that \\(G\\) has pdf\n\\[\nf_G(v_G) = \\frac{1}{12 - 10} = \\frac{1}{2}.\n\\]\nSince we know that \\(H\\) has a truncated normal distribution, \\(H \\sim \\mathcal{TN}(0.5, 0.1, 0, 1)\\), we know (or we could look up) that \\(H\\) has the pdf\n\\[\nf_H(v_H) = \\frac{1}{\\sigma}\\frac{\\varphi(\\frac{v_H-\\mu}{\\sigma})}{\\Phi(\\frac{b-\\mu}{\\sigma}) - \\Phi(\\frac{a - \\mu}{\\sigma})},\n\\]\nwhere \\(\\varphi\\) is the pdf of the standard normal distribution \\(\\mathcal{N}(0,1)\\) and \\(\\Phi\\) is the CDF of the standard normal distribution \\(\\mathcal{N}(0,1)\\) (note the consistent usage of lowercase letters to describe pdfs and capital letters to describe CDFs, even in Greek!).\nTherefore, given the independence condition, we can obtain the joint pdf \\(f_{G,H}(v_G, v_H)\\) by just multiplying these pdfs:\n\\[\nf_{G,H}(v_G, v_H) = \\frac{1}{2\\sigma}\\frac{\\varphi(\\frac{v_H-\\mu}{\\sigma})}{\\Phi(\\frac{b-\\mu}{\\sigma}) - \\Phi(\\frac{a - \\mu}{\\sigma})}.\n\\]\nAnd, given this joint pdf, we can integrate wherever we took sums in the discrete case to obtain the marginal pdfs:\n\\[\nf_G(v_G) = \\int_{0}^{1}f_{G,H}(v_G,v_H)dv_H\n\\]\nor\n\\[\nf_H(v_H) = \\int_{10}^{12}f_{G,H}(v_G, v_H)dv_G\n\\]\nAnd we can compute conditional pdfs by renormalizing so that the denominator is no longer the integral of the distribution over all its possible values (hence just the number \\(1\\)) but a ratio of joint distribution to marginal distribution values like the following:\n\\[\nf_{H \\mid G}(v_H | v_G) = \\frac{f_{G,H}(v_G, v_H)}{f_G(v_G)}.\n\\]\nSo, while the continuous case does have scarier math than the discrete case, I hope that rather than “lingering” in the continuous world, trying to churn through the meaning of the above equations on their own, you can instead try to link the continuous equations back to their simpler discrete forms given in the previous section, then just convert sums to integrals to complete the picture. That way, you never have to just stare at an integral-filled equation again.\nFor example, given two continuous variables with confusing-looking pdfs or CDFs, start by discretizing (“binning”) the possible values of these continuous values, to obtain a discrete distribution, and build up your intuitions about the relationships between the variables in discrete world.\nIn the previous case, for example, if you were asked to start with the continuous version where \\(G\\) ranges continuously across \\([10,12]\\) and \\(H\\) ranges continuously across \\([0,1]\\), you could start by discretizing these continuous distributions to obtain a table very similar to the table presented at the very beginning of this writeup: if you split the range \\([10,12]\\) into three bins of equal length, and the range \\([0,1]\\) into two bins of equal length, you can start by sampling (say) 1000 \\(G\\) and \\(H\\) values using code, sorting these 1000 samples into these equal-length bins, and reasoning through what the joint/marginal/conditional distributions of this binned distribution look like, before moving back over into continuous world to complete the various portions of the problem…"
  },
  {
    "objectID": "writeups/marginalization/index.html#footnotes",
    "href": "writeups/marginalization/index.html#footnotes",
    "title": "Marginal Distributions and “Marginalizing Out” A Variable",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis may look scary, but \\(\\mathcal{TN}(\\mu, \\sigma, a, b)\\) just says: start by sampling a value \\(x\\) from the normal distribution \\(\\mathcal{N}(\\mu, \\sigma)\\), but then: if \\(x &lt; a\\) transform this value to just be \\(a\\), and if \\(x &gt; b\\) then transform this value to just be \\(b\\). In other words, if \\(x \\sim \\mathcal{N}(\\mu, \\sigma)\\), we can obtain a truncated-normal-distributed variable \\(x'\\) from the normally-distributed variable \\(x\\) as \\(x' = \\begin{cases}x &\\text{if }a &lt; x &lt; b \\\\a &\\text{if }x &lt; a \\\\ b &\\text{if }x &gt; b\\end{cases}\\)↩︎"
  },
  {
    "objectID": "writeups/sample-space-tibbles/index.html",
    "href": "writeups/sample-space-tibbles/index.html",
    "title": "Simulating Sample Spaces With Tibbles",
    "section": "",
    "text": "After working with a good number of students trying to set up a tibble (and/or data.frame) in R to represent the sample space of phone users given at the beginning of Quiz 1, and sort of winging it with a bunch of different approaches each time it came up, I decided to try and figure out some “standardized” way to generate a tibble which will represent a given sample space, with minimal headaches, that will work every time! This is the result."
  },
  {
    "objectID": "writeups/sample-space-tibbles/index.html#step-0-what-do-i-mean-by-simulating-a-sample-space",
    "href": "writeups/sample-space-tibbles/index.html#step-0-what-do-i-mean-by-simulating-a-sample-space",
    "title": "Simulating Sample Spaces With Tibbles",
    "section": "Step 0: What Do I Mean By “Simulating” a Sample Space?",
    "text": "Step 0: What Do I Mean By “Simulating” a Sample Space?\nWhat I’m referring to here is, for example, the information you are given at the top of Quiz 1, stating that a random sample contained:\n\n340 people currently using an iPhone,\n185 people using a different phone who don’t want to switch phones ever, and\n232 people using a different phone but hoping to switch to an iPhone in the future.\n\nThe idea is that we can generate a tibble which “encodes” this information in its rows, by allowing us to use the naïve definition of probability to compute probabilities of particular types of people: for example, say we are interested in the probability that a person in our sample uses an iPhone. Mathematically (by the naïve definition), if we construct a random variable \\(F\\) (for iFone, of course) representing the outcome of a randomly-selected person’s phone type (so that \\(F = 1\\) if the person we ask ends up being an iPhone user, and \\(F = 0\\) otherwise), then we can use the information from our sample to compute \\(\\Pr(F = 1)\\) as\n\\[\n\\Pr(F = 1) = \\frac{\\#\\text{ iPhone users in sample}}{\\#\\text{ People in sample}}\n\\tag{1}\\]\nThis means, therefore, that if we had a tibble called df where each row contained information on one particular person from our sample, then we could “translate” this naive definition formula into code as something like\nprob_F &lt;- nrow(df[df$iphone == 1,]) / nrow(df)\nSince here df[df$iphone == 1,] would subset the full df to keep only those rows corresponding to people with iPhones, while df itself (without any filters applied) would contain a row for each person. Literally, we are applying the following “translation” of the naïve definition-based formula (Equation 1) into code:\n\\[\n\\Pr(F = 1) = \\frac{\\overbrace{\\#}^{\\texttt{nrow}}\\overbrace{\\text{ iPhone users in sample}}^{\\texttt{df[df\\$iphone == 1]}}}{\\underbrace{\\#}_{\\texttt{nrow}}\\underbrace{\\text{ People in sample}}_{\\texttt{df}}}\n\\tag{2}\\]\nSimilarly, say we wanted to compute a conditional probability on the basis of our sample, like the probability of a person liking the feature (the feature mentioned in the quiz) given that they are an iPhone user. In this case, let \\(L\\) be an RV such that \\(L = 1\\) if the person likes the feature and \\(L = 0\\) otherwise. Then we can represent the probability we want in this case mathematically using the definition of conditional probability:\n\\[\n\\Pr(L = 1 \\mid F = 1) = \\frac{\\Pr(L = 1 \\cap F = 1)}{\\Pr(F = 1)}\n\\tag{3}\\]\nAnd then we can interpret this ratio (of \\(\\Pr(L = 1 \\cap F = 1)\\) to \\(\\Pr(F = 1)\\) using the naïve definition of probability to derive a new “naïve definition-based equation for the same conditional probability” written mathematically in Equation 3:\n\\[\n\\Pr(L = 1 \\mid F = 1) = \\frac{\\#\\text{ iPhone users in sample who like feature}}{\\#\\text{ iPhone users in sample}}\n\\tag{4}\\]\nIn this case, since both the numerator and denominator are restricted to only those people in our sample who are iPhone users, we can make our lives easy by creating a new “only iPhone users” tibble, using code like\niphone_df &lt;- df[df$iphone == 1,]\nAnd, with this iPhone-users-only tibble now available to us, Equation 4 can be translated into code similarly to how we translated Equation 1 into code, as\nprob_L_given_F &lt;- nrow(iphone_df[iphone_df$like_feature == 1]) / nrow(iphone_df)\nWhere once again we have just applied the following set of “translations” to the naïve definition (this time the “conditional naïve definition”):\n\\[\n\\Pr(L = 1 \\mid F = 1) = \\frac{\\overbrace{\\#}^{\\texttt{nrow}}\\overbrace{\\text{ iPhone users in sample who like feature}}^{\\texttt{iphone\\_df[iphone\\_df\\$likes\\_feature == 1]}}}{\\underbrace{\\#}_{\\texttt{nrow}}\\underbrace{\\text{ iPhone users in sample}}_{\\texttt{iphone\\_df}}}\n\\tag{5}\\]\nSo, all of the above shows you how to work with a sample-space-simulating tibble, once you have created it. The following steps will show you how to create such a sample-space-simulating tibble using functions from the tidyverse."
  },
  {
    "objectID": "writeups/sample-space-tibbles/index.html#step-1-identifying-types-of-people-in-the-sample",
    "href": "writeups/sample-space-tibbles/index.html#step-1-identifying-types-of-people-in-the-sample",
    "title": "Simulating Sample Spaces With Tibbles",
    "section": "Step 1: Identifying “Types” of People in the Sample",
    "text": "Step 1: Identifying “Types” of People in the Sample\nIn the case of the quiz, since each person in our sample is characterized by their values for 3 binary variables (\\(F\\) for iPhone, \\(W\\) for wants-to-switch, and \\(L\\) for likes-feature), we know that there are \\(2^3 = 8\\) possible types of people that could appear in the dataset:\n\n\n\nTable 1: The 8 possible types of people in our sample, sorted in terms of their binary representations (so that 000 corresponds to the non-iPhone, doesn’t-want-to-switch, and doesn’t-like-the-feature type; 001 corresponds to the non-iPhone, doesn’t-want-to-switch, and does-like-the-feature type; and so on.)\n\n\n\n\n\n\n\n\n\n\n\nType\n\\(F\\)(iPhone)\n\\(W\\)(Wants to switch)\n\\(L\\)(Likes feature)\n\n\n\n\n1\n0\n0\n0\n\n\n2\n0\n0\n1\n\n\n3\n0\n1\n0\n\n\n4\n0\n1\n1\n\n\n5\n1\n0\n0\n\n\n6\n1\n0\n1\n\n\n7\n1\n1\n0\n\n\n8\n1\n1\n1\n\n\n\n\n\n\nWe’ll see why we are splitting people into these types in the next section, but here our next task is to fill out the missing “number in sample” column with the number of people who have each of these combinations of properties, using the information given in the problem.\nFor example, since we know that there are 340 iPhone users in the sample, and we know that the probability of an iPhone user liking the feature is \\(0.8\\), we can compute the number of iPhone users who like the feature as\n\\[\n\\begin{align*}\n\\#\\text{ iPhone users who like feature} &= \\#\\text{ iPhone users} \\cdot 0.8 \\\\\n&= 340 \\cdot 0.8 = 272.\n\\end{align*}\n\\]\nWorking out the numbers for all the other types in a similar way (rounding to the nearest integer in cases where we don’t get integers, and also assuming that all iPhone users just have a \\(W\\) value of \\(0\\), since they don’t want to switch to an iPhone because they already have an iPhone), we arrive at the following counts [see the Appendix below for full details of the computations]:\n\n\n\nTable 2: The same list of types as in Table 1, now with an additional column where we’ve computed the number of times that each type appears in our sample (on the basis of the info given in the assignment)\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\\(F\\)(iPhone)\n\\(W\\)(Wants to switch)\n\\(L\\)(Likes feature)\nNumber in Sample\n\n\n\n\n1\n0\n0\n0\n89\n\n\n2\n0\n0\n1\n96\n\n\n3\n0\n1\n0\n121\n\n\n4\n0\n1\n1\n111\n\n\n5\n1\n0\n0\n68\n\n\n6\n1\n0\n1\n272\n\n\n7\n1\n1\n0\n0\n\n\n8\n1\n1\n1\n0"
  },
  {
    "objectID": "writeups/sample-space-tibbles/index.html#step-2-encoding-each-type-as-a-tibble_row",
    "href": "writeups/sample-space-tibbles/index.html#step-2-encoding-each-type-as-a-tibble_row",
    "title": "Simulating Sample Spaces With Tibbles",
    "section": "Step 2: Encoding Each Type as a tibble_row",
    "text": "Step 2: Encoding Each Type as a tibble_row\nOne nice thing about the tibble library is it explicitly provides a function just for creating individual rows of a full tibble, called tibble_row(). It has the same syntax as the more general tibble() function, but in this case you can just provide a set of key=value pairs as arguments, so that (for example) to create a row representing the first “type” in our dataset \\((F = 0, W = 0, L = 0)\\), we can run\n\nlibrary(tibble)\ntype1 &lt;- tibble_row(iphone=0, wants_switch=0, likes_feature=0)\ntype1\n\n\n\n\n\niphone\nwants_switch\nlikes_feature\n\n\n\n\n0\n0\n0\n\n\n\n\n\n\nNow we can do the same thing for the other 5 non-zero types (we don’t have to explicitly make row objects for the \\((F = 0, W = 1, L = 0)\\) or \\((F = 0, W = 1, L = 1)\\) types, since we’re not going to need any of these in our sample-space-simulating tibble, though we could make these tibble_row objects if we really wanted to for some reason):\n\ntype2 &lt;- tibble_row(iphone=0, wants_switch=0, likes_feature=1)\ntype3 &lt;- tibble_row(iphone=0, wants_switch=1, likes_feature=0)\ntype4 &lt;- tibble_row(iphone=0, wants_switch=1, likes_feature=1)\ntype5 &lt;- tibble_row(iphone=1, wants_switch=0, likes_feature=0)\ntype6 &lt;- tibble_row(iphone=1, wants_switch=0, likes_feature=1)\n\nWith these “types” set up as rows, all that’s left to do is to duplicate these rows however many times we need to in order to reflect the number of each type in our sample (Step 3), and then combine these duplicated rows together into one big tibble (Step 4)!"
  },
  {
    "objectID": "writeups/sample-space-tibbles/index.html#step-3-replicating-types-to-form-subsets-of-the-sample-space",
    "href": "writeups/sample-space-tibbles/index.html#step-3-replicating-types-to-form-subsets-of-the-sample-space",
    "title": "Simulating Sample Spaces With Tibbles",
    "section": "Step 3: Replicating Types To Form Subsets of the Sample Space",
    "text": "Step 3: Replicating Types To Form Subsets of the Sample Space\nThis is the part that I found most difficult—honestly, unless there’s some secret easy way that I don’t know about (please tell me if there is!), the best method I could find for duplicating a given tibble_row object k times was to utilize the following code snippet (which uses the slice() function from the dplyr library) as a template:\ntype |&gt; slice(rep(1:n(), k))\nfor taking the individual tibble_row variable type and repeating it k times.\nSo, using this to make 89 “copies” of the type encoded as the tibble_row called type_1, to represent the 89 people in our sample who are of this type, I used:\n\nlibrary(dplyr) # So we can use slice()\ntype1_rows &lt;- type1 |&gt; slice(rep(1:n(), 89))\n\nWhich produces output that looks as follows (I’m using head() to avoid printing out rows with the exact same values 89 times, but you can see that it worked by glancing at the result of head() that follows along with the result of dim() below that)\n\ntype1_rows |&gt; head()\n\n\n\n\n\niphone\nwants_switch\nlikes_feature\n\n\n\n\n0\n0\n0\n\n\n0\n0\n0\n\n\n0\n0\n0\n\n\n0\n0\n0\n\n\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\ndim(type1_rows)\n\n[1] 89  3\n\n\nGiven that this approach worked to make 89 copies of the first type, I just use the same approach 5 more times, to make the appropriate number of copies of each type based on the “Number in Sample” column from Table 2:\n\ntype2_rows &lt;- type2 |&gt; slice(rep(1:n(), 96))\ntype3_rows &lt;- type3 |&gt; slice(rep(1:n(), 121))\ntype4_rows &lt;- type4 |&gt; slice(rep(1:n(), 111))\ntype5_rows &lt;- type5 |&gt; slice(rep(1:n(), 68))\ntype6_rows &lt;- type6 |&gt; slice(rep(1:n(), 272))\n\nThis leaves only one remaining step of combining these individual collections of rows into a giant tibble."
  },
  {
    "objectID": "writeups/sample-space-tibbles/index.html#step-4-combining-the-type-rows",
    "href": "writeups/sample-space-tibbles/index.html#step-4-combining-the-type-rows",
    "title": "Simulating Sample Spaces With Tibbles",
    "section": "Step 4: Combining the Type-Rows",
    "text": "Step 4: Combining the Type-Rows\nNow that we have these six objects, each representing a particular type of person that could be in our sample, replicated the correct number of times based on our calculations from the information given in the problem, we can combine them to form a single, combined tibble by using the bind_rows() function from dplyr as follows (sorry this line looks so packed/scary: we could have done all this in a loop or something that looks a bit “nicer”, but here I thought writing it all out could help with understanding, especially since there are only 6, rather than 8, possible types):\n\nsample_df &lt;- bind_rows(\n    type1_rows, type2_rows, type3_rows,\n    type4_rows, type5_rows, type6_rows\n)\n\nEven though printing the head() or tail() of this dataset will not be that helpful for ensuring that we created everything correctly, we can at least check that the dimensions are correct, as we’re expecting the sample to contain 757 people (rows) in total, with 3 pieces of information (columns) for each person:\n\ndim(sample_df)\n\n[1] 757   3\n\n\nWe could also check, as a way of starting the type of computations mentioned in Step 0, the number of people within the full sample_df dataset who match some filter—in this case, for example, the number of people who are iPhone users:\n\niphone_df &lt;- sample_df[sample_df$iphone == 1,]\ndim(iphone_df)\n\n[1] 340   3\n\n\nAnd this tells us that again, the number of rows in this subset matches what we expected—more evidence that we’ve constructed things correctly.\nNow that we’ve created the iphone_df that was mentioned as part of Step 0, we can carry out the one final step of computing a conditional probability using this iphone_df object, in precisely the way described in Step 0: to get the conditional probability that someone in the sample likes the feature given that they are an iPhone user, \\(\\Pr(L = 1 \\mid F = 1)\\), we just compute\n\nnrow(iphone_df[iphone_df$likes_feature == 1,]) / nrow(iphone_df)\n\n[1] 0.8\n\n\nwhich once again matches the information given in the problem. To compute the quantities the problem asks you to compute, you can use a similar pattern to this example."
  },
  {
    "objectID": "writeups/sample-space-tibbles/index.html#optional-step-5-wrapping-the-row-duplication-in-a-function",
    "href": "writeups/sample-space-tibbles/index.html#optional-step-5-wrapping-the-row-duplication-in-a-function",
    "title": "Simulating Sample Spaces With Tibbles",
    "section": "(Optional) Step 5: Wrapping the Row Duplication in a Function",
    "text": "(Optional) Step 5: Wrapping the Row Duplication in a Function\nIf you’re like me, and you find that typing slice(rep(1:n(), k)) over and over again is scary and could easily lead to mistakes, you can wrap this whole process in a function to make your life easier. For example, we can define a function called duplicate_row which takes an argument tr representing a tibble_row object and another argument num_reps specifying how many times that tibble_row should be repeated:\n\nduplicate_row &lt;- function(tr, num_reps) {\n    return(tr |&gt; slice(rep(1:n(), num_reps)))\n}\n\nAnd now rather than writing the whole operation over and over again, we can just call duplicate_row as often as needed. If you wanted a tibble which contained the numbers 1 through 5 repeated that many times, for example (that is, the number 1 repeated 1 time, the number 2 repeated 2 times, and so on), this could be done in a loop using our duplicate_row function as follows:\n\n# Create a single row containing the number 1\nfull_tibble &lt;- tibble_row(n = 1)\n# Create 2 rows containing the number 2 and add\n# those 2 rows to full_tibble, then create 3 rows\n# containing the number 3 and add those 3 rows\n# to full_tibble, and so on\nfor (i in 2:5) {\n    row_containing_i &lt;- tibble_row(n = i)\n    repeated_rows &lt;- duplicate_row(row_containing_i, i)\n    full_tibble &lt;- bind_rows(full_tibble, repeated_rows)\n}\ndim(full_tibble)\n\n[1] 15  1\n\nfull_tibble\n\n\n\n\n\nn\n\n\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n\n\n\n\nI hope that helps a bit, in case this scenario ever comes up again on a homework or quiz, or it comes up on your project or at some point in your future data science career!"
  },
  {
    "objectID": "writeups/sample-space-tibbles/index.html#appendix-computing-counts-for-each-type",
    "href": "writeups/sample-space-tibbles/index.html#appendix-computing-counts-for-each-type",
    "title": "Simulating Sample Spaces With Tibbles",
    "section": "Appendix: Computing Counts For Each Type",
    "text": "Appendix: Computing Counts For Each Type\nWe already computed the number of iPhone users who like the feature as 272: \\(\\#(F = 1, W = 0, L = 1) = 272\\). This lets us compute, using the complement rule, that there are\n\\[\n340 - 272 = 68\n\\]\nremaining iPhone users, who do not like the feature: \\(\\#(F = 1, W = 0, L = 0) = 68\\). The counts for the final two rows, \\(\\#(F = 1, W = 1, L = 0)\\) and \\(\\#(F = 1, W = 1, L = 1)\\) are both zero, since we defined \\(W\\) for iPhone users to always be \\(0\\), since they already have the iPhone so cannot want to switch to the iPhone. This gives us the bottom half of the count column.\nFor the top half, we compute as follows: the top half (first four rows) represent all of the non-iPhone users. Since there are\n\\[\n340 + 185 + 232 = 757\n\\]\npeople in total, and 340 are iPhone users, this means that\n\\[\n757 - 340 = 417\n\\]\nmust be non-iPhone users (which we also could have computed by adding the 185 and 232 given in the problem): \\(\\#(F = 0) = 417\\).\nOf these 417, we are also given that 185 don’t want to switch, while 232 do want to switch, so\n\n\\(\\#(F = 0, W = 0) = 185\\) and\n\\(\\#(F = 0, W = 1) = 232\\).\n\nThe final piece of info we need is the info given in the problem that 52% of the no-switch people like the feature, while only 48% of the hope-to-switch people like the feature.\nSince “no-switch people” are people with the properties \\((F = 0, W = 0)\\), and “hope-to-switch people” are people with the properties \\((F = 0, W = 1)\\), we can use these given conditional probabilities to compute the remaining counts in our table as follows:\nNo-switch people who like the feature:\nWe are given the info that, among no-switch people, 52% like the feature. So,\n\\[\n\\begin{align}\n\\#(F = 0, W = 0, L = 1) &= 0.52 \\cdot \\#(F = 0, W = 0) \\\\\n&= 0.52 \\cdot 185 = 96.2,\n\\end{align}\n\\]\nwhich we round to 96 to get an integer number of people.\nNo-switch people who don’t like the feature:\nSince 52% of no-switch people like the feature, this must mean that (100% - 52% = 48%) of no-switch people must not like the feature:\n\\[\n\\begin{align}\n\\#(F = 0, W = 0, L = 0) &= (1 - 0.52) \\cdot \\#(F = 0, W = 0) \\\\\n&= 0.48 \\cdot 185 = 88.8,\n\\end{align}\n\\]\nwhich we round to 89 to get an integer number of people.\nHope-to-switch people who like the feature:\nWe are given the info that, among hope-to-switch people, 48% like the feature. So,\n\\[\n\\begin{align}\n\\#(F = 0, W = 1, L = 1) &= 0.48 \\cdot \\#(F = 0, W = 1) \\\\\n&= 0.48 \\cdot 232 = 111.36,\n\\end{align}\n\\]\nwhich we round to 111 to get an integer number of people.\nHope-to-switch people who don’t like the feature\nSince we are given that 48% of hope-to-switch people like the feature, this must mean that (100% - 48% = 52%) of hope-to-switch people do not like the feature. So,\n\\[\n\\begin{align}\n\\#(F = 0, W = 1, L = 0) &= 0.52 \\cdot \\#(F = 0, W = 1) \\\\\n&= 0.52 \\cdot 232 = 120.64,\n\\end{align}\n\\]\nwhich we round to 121 to get an integer number of people.\nI may have made a mistake in one or more of those calculations, so please let me know if you find one. As a sanity check, however, I did sum the numbers in the “number in sample” column, and the sum does come out to 757 as expected."
  },
  {
    "objectID": "writeups/sampling-from-df/index.html",
    "href": "writeups/sampling-from-df/index.html",
    "title": "Two Ways of Sampling from a Data Frame",
    "section": "",
    "text": "A few students have run into issues when trying to use the sample() function, from base-R, to sample from a full data.frame or tibble. In this writeup I’ll argue that this is a case where using a function from the tidyverse called slice_sample() will make your life much easier, but I will also show how to do this sampling using only base-R functions.\nBefore we start, we make sure to use set.seed(5000) at the beginning, so that your grader gets the same results as you do even when working with random processes!\nset.seed(5000)"
  },
  {
    "objectID": "writeups/sampling-from-df/index.html#creating-a-deck-of-cards-using-expand.grid",
    "href": "writeups/sampling-from-df/index.html#creating-a-deck-of-cards-using-expand.grid",
    "title": "Two Ways of Sampling from a Data Frame",
    "section": "Creating a Deck of Cards Using expand.grid()",
    "text": "Creating a Deck of Cards Using expand.grid()\nThis is done as was introduced in the Bootcamp:\n\n\nCode\nranks &lt;- c(\"Ace\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\",\n           \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\")\nsuits &lt;- c(\"Hearts\", \"Diamonds\", \"Clubs\", \"Spades\")\ndeck_df &lt;- expand.grid(ranks, suits)\ncolnames(deck_df) &lt;- c(\"Rank\", \"Suit\")\nhead(deck_df)\n\n\n\n\n\n\nRank\nSuit\n\n\n\n\nAce\nHearts\n\n\nTwo\nHearts\n\n\nThree\nHearts\n\n\nFour\nHearts\n\n\nFive\nHearts\n\n\nSix\nHearts\n\n\n\n\n\n\nAnd we can check the dimensions of deck_df just to make sure it created the right number of cards:\n\ndim(deck_df)\n\n[1] 52  2\n\n\nNow, since sampling without replacement is the default case for both sample functions, to illustrate how to use parameters to these functions I will be sampling with replacement."
  },
  {
    "objectID": "writeups/sampling-from-df/index.html#using-tidyverse-to-sample-with-replacement",
    "href": "writeups/sampling-from-df/index.html#using-tidyverse-to-sample-with-replacement",
    "title": "Two Ways of Sampling from a Data Frame",
    "section": "Using tidyverse to Sample WITH Replacement",
    "text": "Using tidyverse to Sample WITH Replacement\nThe following code uses the pipe operator %&gt;% to take the data.frame, deck_df, and “pipe it into” the slice_sample() function from the tidyverse. We have to provide two arguments:\n\nn: The number of samples we’d like to take, and\nreplace: If set to TRUE, the sampling is performed with replacement. Otherwise (the default), the sampling is performed without replacement.\n\n\n\nCode\nlibrary(tidyverse)\n# Using the \"fancier\" pipe operator (%&gt;%)\ndeck_sample_df &lt;- deck_df %&gt;% slice_sample(n = 12, replace = TRUE)\ndeck_sample_df\n\n\n\n\n\n\nRank\nSuit\n\n\n\n\nAce\nSpades\n\n\nFive\nHearts\n\n\nFour\nSpades\n\n\nJack\nSpades\n\n\nNine\nClubs\n\n\nSeven\nClubs\n\n\nTen\nClubs\n\n\nQueen\nClubs\n\n\nKing\nSpades\n\n\nTwo\nSpades\n\n\nThree\nSpades\n\n\nJack\nSpades\n\n\n\n\n\n\nHere we can confirm that it sampled with replacement since we see that it selected the Jack of Spades twice (once in slot 4 and once in slot 12).\nNote that, although using the pipe operator %&gt;% is the “standard” way to use tidyverse functions, you can still use the functions without using the pipe operator (long story short, the pipe operator just takes whatever comes before the %&gt;% and “plugs it in” as the first argument to the function that comes after the %&gt;%), by specifying the first argument to the slice_sample() function explicitly:\n\ndeck_sample_df &lt;- slice_sample(deck_df, n = 12, replace = TRUE)\ndeck_sample_df\n\n\n\n\n\nRank\nSuit\n\n\n\n\nFour\nHearts\n\n\nFive\nClubs\n\n\nFour\nClubs\n\n\nFive\nDiamonds\n\n\nKing\nSpades\n\n\nKing\nDiamonds\n\n\nFive\nSpades\n\n\nTwo\nClubs\n\n\nQueen\nHearts\n\n\nEight\nClubs\n\n\nJack\nSpades\n\n\nEight\nClubs\n\n\n\n\n\n\nTo me, one nice aspect of slice_sample() over other base-R functions is (among other things) it ensures that the column names are maintained when you sample, which is not always true for the base-R functions. It’s also possible to do in base-R (without using tidyverse libraries/functions), though, just less straightforwardly."
  },
  {
    "objectID": "writeups/sampling-from-df/index.html#using-base-r-to-sample-with-replacement",
    "href": "writeups/sampling-from-df/index.html#using-base-r-to-sample-with-replacement",
    "title": "Two Ways of Sampling from a Data Frame",
    "section": "Using Base-R to Sample WITH Replacement",
    "text": "Using Base-R to Sample WITH Replacement\nFirst off, note that just applying sample() to the deck will not produce the outcome we expect, or want, which probably unfortunately goes against our intuitions for how this function should work:\n\nsample(deck_df, 5, replace = TRUE)\n\n\n\n\n\nSuit\nRank\nSuit.1\nRank.1\nRank.2\n\n\n\n\nHearts\nAce\nHearts\nAce\nAce\n\n\nHearts\nTwo\nHearts\nTwo\nTwo\n\n\nHearts\nThree\nHearts\nThree\nThree\n\n\nHearts\nFour\nHearts\nFour\nFour\n\n\nHearts\nFive\nHearts\nFive\nFive\n\n\nHearts\nSix\nHearts\nSix\nSix\n\n\nHearts\nSeven\nHearts\nSeven\nSeven\n\n\nHearts\nEight\nHearts\nEight\nEight\n\n\nHearts\nNine\nHearts\nNine\nNine\n\n\nHearts\nTen\nHearts\nTen\nTen\n\n\nHearts\nJack\nHearts\nJack\nJack\n\n\nHearts\nQueen\nHearts\nQueen\nQueen\n\n\nHearts\nKing\nHearts\nKing\nKing\n\n\nDiamonds\nAce\nDiamonds\nAce\nAce\n\n\nDiamonds\nTwo\nDiamonds\nTwo\nTwo\n\n\nDiamonds\nThree\nDiamonds\nThree\nThree\n\n\nDiamonds\nFour\nDiamonds\nFour\nFour\n\n\nDiamonds\nFive\nDiamonds\nFive\nFive\n\n\nDiamonds\nSix\nDiamonds\nSix\nSix\n\n\nDiamonds\nSeven\nDiamonds\nSeven\nSeven\n\n\nDiamonds\nEight\nDiamonds\nEight\nEight\n\n\nDiamonds\nNine\nDiamonds\nNine\nNine\n\n\nDiamonds\nTen\nDiamonds\nTen\nTen\n\n\nDiamonds\nJack\nDiamonds\nJack\nJack\n\n\nDiamonds\nQueen\nDiamonds\nQueen\nQueen\n\n\nDiamonds\nKing\nDiamonds\nKing\nKing\n\n\nClubs\nAce\nClubs\nAce\nAce\n\n\nClubs\nTwo\nClubs\nTwo\nTwo\n\n\nClubs\nThree\nClubs\nThree\nThree\n\n\nClubs\nFour\nClubs\nFour\nFour\n\n\nClubs\nFive\nClubs\nFive\nFive\n\n\nClubs\nSix\nClubs\nSix\nSix\n\n\nClubs\nSeven\nClubs\nSeven\nSeven\n\n\nClubs\nEight\nClubs\nEight\nEight\n\n\nClubs\nNine\nClubs\nNine\nNine\n\n\nClubs\nTen\nClubs\nTen\nTen\n\n\nClubs\nJack\nClubs\nJack\nJack\n\n\nClubs\nQueen\nClubs\nQueen\nQueen\n\n\nClubs\nKing\nClubs\nKing\nKing\n\n\nSpades\nAce\nSpades\nAce\nAce\n\n\nSpades\nTwo\nSpades\nTwo\nTwo\n\n\nSpades\nThree\nSpades\nThree\nThree\n\n\nSpades\nFour\nSpades\nFour\nFour\n\n\nSpades\nFive\nSpades\nFive\nFive\n\n\nSpades\nSix\nSpades\nSix\nSix\n\n\nSpades\nSeven\nSpades\nSeven\nSeven\n\n\nSpades\nEight\nSpades\nEight\nEight\n\n\nSpades\nNine\nSpades\nNine\nNine\n\n\nSpades\nTen\nSpades\nTen\nTen\n\n\nSpades\nJack\nSpades\nJack\nJack\n\n\nSpades\nQueen\nSpades\nQueen\nQueen\n\n\nSpades\nKing\nSpades\nKing\nKing\n\n\n\n\n\n\nA way to avoid this is to make sure that you are using the sample() function NOT on the entire data.frame object, but just to select a subset of the rows of the data.frame, like the following:\n\ndeck_df[sample(nrow(deck_df), 15, replace = TRUE),]\n\n\n\n\n\n\nRank\nSuit\n\n\n\n\n24\nJack\nDiamonds\n\n\n19\nSix\nDiamonds\n\n\n25\nQueen\nDiamonds\n\n\n19.1\nSix\nDiamonds\n\n\n3\nThree\nHearts\n\n\n10\nTen\nHearts\n\n\n31\nFive\nClubs\n\n\n11\nJack\nHearts\n\n\n47\nEight\nSpades\n\n\n39\nKing\nClubs\n\n\n39.1\nKing\nClubs\n\n\n43\nFour\nSpades\n\n\n22\nNine\nDiamonds\n\n\n9\nNine\nHearts\n\n\n27\nAce\nClubs\n\n\n\n\n\n\nFirst off, notice how here we can again confirm that it sampled with replacement since it had to create additional ids like 34.1 and 34.2 to represent the fact that card #34 (the Eight of Clubs) ended up in our sample 3 times.\nAlso note how, rather than sampling from the data.frame, which may be intuitively/linguistically how we would describe what we want, we are actually sampling from the set of indices of the data.frame, then asking R to give us the rows corresponding to those sampled indices. Concretely, to see what’s going on, let’s just look at the row filter we’ve provided (the portion of the full code that is within the square brackets [], before the comma):\n\nsample(nrow(deck_df), 15, replace = TRUE)\n\n [1] 30 32 30 46  7 30 46 27 21 42 25 26 17 35  4\n\n\nWe see that, in fact, we are not really sampling from the data.frame itself, so much as sampling from a list of its indices (from 1 to 52), and then after performing this sample we are going and asking R to give us the rows at the indices that ended up in this sample. Keeping this distinction in mind (between the rows themselves and their indices) can be helpful for debugging code like this."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5100, Section 03 (Tuesdays)",
    "section": "",
    "text": "This page collects slides and relevant links for students in Prof. Jeff’s Tuesday section (Section 03) of DSAN 5100: Probabilistic Modeling and Statistical Computing, Fall 2024 at Georgetown University.\nSections take place in Car Barn 309 on Tuesdays from 6:30pm to 9:00pm.\nThis page is not a replacement for the Main Course Page or the sections’s Canvas Page!\nUse the menu on the left, or the table below, to view resources for a specific week.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1: Welcome to DSAN 5100!\n\n\nFriday, August 30, 2024\n\n\n\n\nWeek 2: Introduction to Probabilistic Modeling\n\n\nTuesday, September 3, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Welcome to DSAN 5100!",
    "section": "",
    "text": "(Week 1 of DSAN 5100 was a joint session across all individual sections, taught on Zoom.)\n\n\n\n\n\n\nToday’s Links\n\n\n\n\nWeek 1 Slides\nWeek 1 Lecture Recording",
    "crumbs": [
      "Week 1: Aug 30"
    ]
  }
]