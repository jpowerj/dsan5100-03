---
title: "Week 4: Discrete Distributions"
subtitle: "*DSAN 5100: Probabilistic Modeling and Statistical Computing*<br>Section 03"
author: "Jeff Jacobs"
institute: "<a href=\"mailto:jj1088@georgetown.edu\" target=\"_blank\">`jj1088@georgetown.edu`</a>"
date: 2024-09-17
date-format: full
lecnum: 4
categories:
  - "Class Sessions"
format:
  revealjs:
    cache: false
    css: "../dsan-globals/jjstyles.css"
    footer: "DSAN 5100-03 W04: Discrete Distributions"
    output-file: slides.html
    df-print: kable
    code-fold: true
    html-math-method: mathjax
    scrollable: true
    slide-number: true
    number-sections: false
    link-external-icon: true
    link-external-newwindow: true
    theme: [default]
    revealjs-plugins:
      - simplemenu
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar'><span style='position: absolute; left: 8; padding-left: 8px;'><a href='./index.html'>&larr; Return to Notes</a></span><ul class='menu'></ul></div>"
      scale: 0.5
    include-in-header:
      text: "<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css' integrity='sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65' crossorigin='anonymous'><link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css'>"
  html:
    cache: false
    output-file: index.html
    df-print: kable
    warning: false
    link-external-icon: true
    link-external-newwindow: true
---

::: {.content-visible unless-format="revealjs"}

<center>
<a class="h2" href="slides.html" target="_blank">Open slides in new window &rarr;</a>
</center>

:::

# "Rules" of Probability {data-stack-name="«Rules»"}

::: {.hidden}

```{r}
source("../dsan-globals/_globals.r")
```

:::

## Summarizing What We Know So Far {.smaller}

* Logic $\rightarrow$ Set Theory $\rightarrow$ Probability Theory
* Entirety of probability theory can be derived from **two axioms**:

::: {.callout-tip icon=false title="The Entirety of Probability Theory Follows From..."}

**Axiom 1** (*Unitarity*): $\Pr(\Omega) = 1$ (The probability that **something** happens is 1)

**Axiom 2** (*$\sigma$-additivity*): For mutually-exclusive events $E_1, E_2, \ldots$,

$$
\underbrace{\Pr\left(\bigcup_{i=1}^{\infty}E_i\right)}_{\Pr(E_1\text{ occurs }\vee E_2\text{ occurs } \vee \cdots)} = \underbrace{\sum_{i=1}^{\infty}\Pr(E_i)}_{\Pr(E_1\text{ occurs}) + \Pr(E_2\text{ occurs}) + \cdots}
$$
  
:::

* But what does "mutually exclusive" mean...?

## Venn Diagrams: Sets {.smaller}

:::: {.columns}
::: {.column width="50%"}

$$
\begin{align*}
&A = \{1, 2, 3\}, \; B = \{4, 5, 6\} \\
&\implies A \cap B = \varnothing
\end{align*}
$$

![Mutually-exclusive (disjoint) sets](images/venn_disjoint.svg){fig-align="center" width="100%"}

:::
::: {.column width="50%"}

$$
\begin{align*}
&A = \{1, 2, 3, 4\}, \; B = \{3, 4, 5, 6\} \\
&\implies A \cap B = \{3, 4\}
\end{align*}
$$

![Non-mutually-exclusive sets](images/venn_two_circs.svg){fig-align="center" width="75%"}

:::
:::

## Venn Diagrams: Events (Dice) {.smaller}

$$
\begin{align*}
A &= \{\text{Roll is even}\} = \{2, 4, 6\} \\
B &= \{\text{Roll is odd}\} = \{1, 3, 5\} \\
C &= \{\text{Roll is in Fibonnaci sequence}\} = \{1, 2, 3, 5\}
\end{align*}
$$

| Set 1 | Set 2 | Intersection | Mutually Exclusive? | Can Happen Simultaneously? |
|:-:|:-:| - | - | - |
| $A$ | $B$ | $A \cap B = \varnothing$ | **Yes** | **No** |
| $A$ | $C$ | $A \cap C = \{2\}$ | **No** | **Yes** |
| $B$ | $C$ | $B \cap C = \{1, 3, 5\}$ | **No** | **Yes** |

: {tbl-colwidths="[8,8,30,27,27]"}

## "Rules" of Probability {.smaller}

(Remember: not *"rules"* but *"facts resulting from the logic $\leftrightarrow$ probability connection"*)

::: {.callout-tip icon="false" title="\"Rules\" of Probability"}

For logical predicates $p, q \in \{T, F\}$, events $P, Q$ defined so $P$ = event that $p$ becomes true, $Q$ = event that $q$ becomes true,

1. **Logical AND = Probabilistic Multiplication**

$$
\Pr(p \wedge q) = \Pr(P \cap Q) = \Pr(P) \cdot \Pr(Q)
$$

2. **Logical OR = Probabilistic Addition**

$$
\Pr(p \vee q) = \Pr(P \cup Q) = \Pr(P) + \Pr(Q) - \underbrace{\Pr(P \cap Q)}_{\text{(see rule 1)}}
$$

3. **Logical NOT = Probabilistic Complement**

$$
\Pr(\neg p) = \Pr(P^c) = 1 - \Pr(P)
$$

:::

# What is Conditional Probability? {data-stack-name="Conditional Prob"}

## Conditional Probability {.smaller .crunch-title .crunch-li-8 .inline-90}

* Usually if someone asks you probabilistic questions, like
  * "What is the likelihood that [our team] wins?"
  * "Do you think it will rain tomorrow?"
    
* You don't guess a random number, you **consider** and **incorporate evidence**.
* Example: $\Pr(\text{rain})$ on its own, no other info? Tough question... maybe $0.5$?
* In reality, we would think about
  * $\Pr(\text{rain} \mid \text{month of the year})$
  * $\Pr(\text{rain} \mid \text{where we live})$
  * $\Pr(\text{rain} \mid \text{did it rain yesterday?})$
* Psychologically, breaks down into two steps: (1) Think of **baseline probability**, (2) **Update** baseline to incorporate **relevant evidence** (more on this in a bit...)
* Also recall: **all** probability is **conditional** probability, even if just conditioned on "something happened" ($\Omega$, the thing defined so $\Pr(\Omega) = 1$)

## Naïve Definition 2.0 {.smaller}

::: {.callout-tip icon="false" title="[Slightly Less] Naïve Definition of Probability"}

$$
\Pr(A \mid B) = \frac{\text{\# of Desired Outcomes in world where }B\text{ happened}}{\text{\# Total outcomes in world where }B\text{ happened}} = \frac{|B \cap A|}{|B|}
$$

:::

| World Name | Weather in World | Likelihood of Rain Today |
| - | - |:-:|
| $R$ | **Rained for the past 5 days** | $\Pr(\text{rain} \mid R) > 0.5$ |
| $M$ | **Mix of rain and non-rain over past 5 days** | $\Pr(\text{rain} \mid M) \approx 0.5$ |
| $S$ | **Sunny for the past 5 days** | $\Pr(\text{rain} \mid S) < 0.5$ |

: {tbl-colwidths="[16,50,34]"}


## Law of Total Probability {.smaller .crunch-title .math-80}

* Suppose the events $B_1, \ldots, B_k$ form a partition of the space $\Omega$ and $\Pr(B_j) > 0 \forall j$.
* Then, for every event $A$ in $\Omega$,

    $$
    \Pr(A) = \sum_{i=1}^k \Pr(B_j)\Pr(A \mid B_j)
    $$

* Probability of an event is the sum of its conditional probabilities across all conditions.
* In other words: $A$ is some event, $B_1, \ldots, B_n$ are mutually exclusive events filling entire sample-space, then

    $$
    \Pr(A) = \Pr(A \mid B_1)\Pr(B_1) + \Pr(A \mid B_2)\Pr(B_2) + \cdots + \Pr(A \mid B_n)\Pr(B_n)
    $$

    i.e. Compute the probability by **summing over all possible cases**.

::: {.notes}

Draw pic on board!

:::

## Example {.smaller .crunch-title}

* Probability of completing job on time with and without rain: 0.42 and 0.9.
* Probability of rain is 0.45. What is probability job will be completed on time?
* $A$ = job will be completed on time, $B$ = rain

$$
\Pr(B) = 0.45 \implies \Pr(B^c) = 1 - \Pr(B) = 0.55.
$$

* Note: Events $B$ and $B^c$ are exclusive and form partitions of the sample space $S$
* We know $\Pr(A \mid B) = 0.24$, $\Pr(A \mid B^c) = 0.9$.
* By the Law of Total Probability, we have

$$
\begin{align*}
\Pr(A) &= \Pr(B)\Pr(A \mid B) + \Pr(B^c)\Pr(A \mid B^c) \\
&= 0.45(0.42) + 0.55(0.9) = 0.189 + 0.495 = 0684.
\end{align*}
$$

So, the probability that the job will be completed on time is **0.684**. [(source)](https://byjus.com/maths/total-probability-theorem/){target="_blank"}

# Bayes' Theorem and its Implications {data-stack-name="Bayes"}

## Deriving Bayes' Theorem {.smaller .crunch-math}

* Literally just a re-writing of the conditional probability definition (don't be scared)!

::: columns
::: {.column width="50%"}

1. For two events $A$ and $B$, definition of conditional probability says that

$$
\begin{align*}
\Pr(A \mid B) &= \frac{\Pr(A \cap B)}{\Pr(B)} \tag{1} \\
\Pr(B \mid A) &= \frac{\Pr(B \cap A)}{\Pr(A)} \tag{2}
\end{align*}
$$

2. Multiply to get rid of fractions

$$
\begin{align*}
\Pr(A \mid B)\Pr(B) &= \Pr(A \cap B) \tag{1*} \\
\Pr(B \mid A)\Pr(A) &= \Pr(B \cap A) \tag{2*}
\end{align*}
$$

:::
::: {.column width="50%"}

3. But **set intersection** is **associative** (just like multiplication...), $A \cap B = B \cap A$! So, we know LHS of $(\text{1*})$ = LHS of $(\text{2*})$:

$$
\Pr(A \mid B)\Pr(B) = \Pr(B \mid A)\Pr(A)
$$

4. Divide both sides by $\Pr(B)$ to get a **new definition** of $\Pr(A \mid B)$, **Bayes' Theorem**!

::: {#fig-bayes-thm}

$$
\boxed{\Pr(A \mid B) = \frac{\Pr(B \mid A)\Pr(A)}{\Pr(B)}}
$$

Bayes' Theorem
:::

:::
:::

## Why Is This Helpful? {.smaller .crunch-title .crunch-math .crunch-li-5 .inline-90}

::: {.callout-tip icon="false" title="Bayes' Theorem"}

For any two events $A$ and $B$,
$$
\Pr(A \mid B) = \frac{\Pr(B \mid A)\Pr(A)}{\Pr(B)}
$$

:::

* In words (as exciting as I can make it, for now): **Bayes' Theorem allows us to take information about $B \mid A$ and use it to infer information about $A \mid B$**
* It isn't until you work through some examples that this becomes mind-blowing, the most powerful equation we have for inferring **unknowns** from **knowns**...
* Consider $A = \{\text{person has disease}\}$, $B = \{\text{person tests positive for disease}\}$
  1. Is $A$ observable on its own? **No**, but...
  2.
    (a) Is $B$ observable on its own? **Yes**, and
    (b) Can we **infer** info about $A$ from knowing $B$? **Also Yes, thx Bayes!**
  3. Therefore, we can **use $B$ to infer information about $A$**, i.e., calculate $\Pr(A \mid B)$

## Why Is This Helpful for *Data Science*? {.smaller}

* It merges **probability theory** and **hypothesis testing** into a single framework:

$$
\Pr(\text{hypothesis} \mid \text{data}) = \frac{\Pr(\text{data} \mid \text{hypothesis})\Pr(\text{hypothesis})}{\Pr(\text{data})}
$$


## Probability Forwards and Backwards {.nostretch .smaller}

* Two discrete RVs:
  * Weather on a given day, $W \in \{\textsf{Rain},\textsf{Sun}\}$
  * Action that day, $A \in \{\textsf{Go}, \textsf{Stay}\}$: go to party or stay in and watch movie
* Data-generating process: if $\textsf{Sun}$, rolls a die $R$ and goes out unless $R = 6$. If $\textsf{Rain}$, flips a coin and goes out if $\textsf{H}$.

* Probabilistic Graphical Model (PGM):

![](images/pgm.svg){fig-align="center" width="60%"}

## Probability Forwards and Backwards {.unnumbered .nostretch}

* So, if we know $W = \textsf{Sun}$, what is $P(A = \textsf{Go})$?
$$
\begin{align*}
P(A = \textsf{Go} \mid W) &= 1 - P(R = 6) \\
&= 1 - \frac{1}{6} = \frac{5}{6}
\end{align*}
$$

* Conditional probability lets us go **forwards** (left to right):

![](images/pgm_forwards.svg){fig-align="center" width="60%"}

::: {.notes}
But what if we want to perform inference going **backwards**?
:::

## Probability Forwards and Backwards {.unnumbered .smaller .smallishmath}

::: columns
::: {.column width="50%"}

  * If we see Ana at the party, we know $A = \textsf{Go}$
  * What does this tell us about the weather?
  * Intuitively, we should increase our degree of belief that $W = \textsf{Sun}$. But, by how much?
  * We don't know $P(W \mid A)$, only $P(A \mid W)$...

:::
::: {.column width="50%"}

![](images/koolaid_bayes.png){fig-align="center" width="100%"}

:::
:::

## Probability Forwards and Backwards {.unnumbered .smaller}

$$
P(W = \textsf{Sun} \mid A = \textsf{Go}) = \frac{\overbrace{P(A = \textsf{Go} \mid W = \textsf{Sun})}^{5/6~ ✅}\overbrace{P(W = \textsf{Sun})}^{❓}}{\underbrace{P(A = \textsf{Go})}_{❓}}
$$

* We've seen $P(W = \textsf{Sun})$ before, it's our **prior**: the probability without having any additional relevant knowledge. So, let's say 50/50. $P(W = \textsf{Sun}) = \frac{1}{2}$
* If we lived in Seattle, we could pick $P(W = \textsf{Sun}) = \frac{1}{4}$

## Probability Forwards and Backwards {.unnumbered .smaller .small-math}

$$
P(W = \textsf{Sun} \mid A = \textsf{Go}) = \frac{\overbrace{P(A = \textsf{Go} \mid W = \textsf{Sunny})}^{5/6~ ✅}\overbrace{P(W = \textsf{Sun})}^{1/2~ ✅}}{\underbrace{P(A = \textsf{Go})}_{❓}}
$$

* $P(A = \textsf{Go})$ is trickier: the probability that Ana goes out **regardless of what the weather is**. But there are only two possible weather outcomes! So we just compute

$$
\begin{align*}
&P(A = \textsf{Go}) = \sum_{\omega \in S(W)}P(A = \textsf{Go}, \omega) = \sum_{\omega \in S(W)}P(A = \textsf{Go} \mid \omega)P(\omega) \\
&= P(A = \textsf{Go} \mid W = \textsf{Rain})P(W = \textsf{Rain}) + P(A = \textsf{Go} \mid W = \textsf{Sun})P(W = \textsf{Sun}) \\
&= \left( \frac{1}{2} \right)\left( \frac{1}{2} \right) + \left( \frac{5}{6} \right)\left( \frac{1}{2} \right) = \frac{1}{4} + \frac{5}{12} = \frac{2}{3}
\end{align*}
$$

## Putting it All Together {.math-80}

$$
\begin{align*}
P(W = \textsf{Sun} \mid A = \textsf{Go}) &= \frac{\overbrace{P(A = \textsf{Go} \mid W = \textsf{Sunny})}^{3/4~ ✅}\overbrace{P(W = \textsf{Sun})}^{1/2~ ✅}}{\underbrace{P(A = \textsf{Go})}_{1/2~ ✅}} \\
&= \frac{\left(\frac{3}{4}\right)\left(\frac{1}{2}\right)}{\frac{1}{2}} = \frac{\frac{3}{8}}{\frac{1}{2}} = \frac{3}{4}.
\end{align*}
$$

* Given that we see Ana at the party, we should **update our beliefs**, so that $P(W = \textsf{Sun}) = \frac{3}{4}, P(W = \textsf{Rain}) = \frac{1}{4}$.

## A Scarier Example {.smaller}

* Bo worries he has a **rare disease**. He takes a test with **99% accuracy** and tests **positive**. What's the probability Bo has the disease? (Intuition: 99%? ...Let's do the math!)

::: columns
::: {.column width="50%"}

* $H \in \{\textsf{sick}, \textsf{healthy}\}, T \in \{\textsf{T}^+, \textsf{T}^-\}$
* **The test**: 99% accurate. $\Pr(T = \textsf{T}^+ \mid H = \textsf{sick}) = 0.99$, $\Pr(T = \textsf{T}^- \mid H = \textsf{healthy}) = 0.99$.
* **The disease**: 1 in 10K. $\Pr(H = \textsf{sick}) = \frac{1}{10000}$
* **What do we want to know?** $\Pr(H = \textsf{sick} \mid T = \textsf{T}^+)$
* **How do we get there?**

:::
::: {.column width="50%"}

![This photo, originally thought to be of Thomas Bayes, turns out to be [probably someone else](https://www.york.ac.uk/depts/maths/histstat/bayespic.htm)... $\Pr(\textsf{Bayes})$?](images/bayes.jpg){fig-align="center" width="65%"}

:::
:::

::: {.notes}
$H$ for health, $T$ for test result

Photo credit: https://thedatascientist.com/wp-content/uploads/2019/04/reverend-thomas-bayes.jpg
:::

## A Scarier Example {.unnumbered .math-80}

$$
\begin{align*}
\Pr(H = \textsf{sick} \mid T = \textsf{T}^+) &= \frac{\Pr(T = \textsf{T}^+ \mid H = \textsf{sick})\Pr(H = \textsf{sick})}{\Pr(T = \textsf{T}^+)} \\
&= \frac{(0.99)\left(\frac{1}{10000}\right)}{(0.99)\left( \frac{1}{10000} \right) + (0.01)\left( \frac{9999}{10000} \right)}
\end{align*}
$$

```{r}
#| label: prob-sick-bayes
#| echo: true
p_sick <- 1 / 10000
p_healthy <- 1 - p_sick
p_pos_given_sick <- 0.99
p_neg_given_sick <- 1 - p_pos_given_sick
p_neg_given_healthy <- 0.99
p_pos_given_healthy <- 1 - p_neg_given_healthy
numer <- p_pos_given_sick * p_sick
denom1 <- numer
denom2 <- p_pos_given_healthy * p_healthy
final_prob <- numer / (denom1 + denom2)
final_prob
```

* ... Less than 1% 😱

## Proof in the Pudding {.smaller}

* Let's generate a dataset of **5,000** people, using $\Pr(\textsf{Disease}) = \frac{1}{10000}$

```{r}
#| label: bayes-sick-mc
#| echo: true
#| code-fold: true
library(tibble)
library(dplyr)
# Disease rarity
p_disease <- 1 / 10000
# 1K people
num_people <- 10000
# Give them ids
ppl_df <- tibble(id=seq(1,num_people))
# Whether they have the disease or not
has_disease <- rbinom(num_people, 1, p_disease)
ppl_df <- ppl_df %>% mutate(has_disease=has_disease)
ppl_df |> head()
```

## Binary Variable Trick {.smaller .crunch-code}

* Since `has_disease` $\in \{0, 1\}$, we can use
  * `sum(has_disease)` to obtain the **count** of people with the disease, or
  * `mean(has_disease)` to obtain the **proportion** of people who have the disease
* To see this (or, if you forget in the future), just make a fake dataset with a binary variable and 3 rows, and think about **sums** vs. **means** of that variable:

::: columns
::: {.column width="40%"}

```{r}
#| label: fake-binary-data
#| echo: true
#| code-fold: show
binary_df <- tibble(
  id=c(1,2,3),
  x=c(0,1,0)
)
binary_df
```

:::
::: {.column width="60%"}

Taking the **sum** tells us: **one** row where `x == 1`:

```{r}
#| label: binary-df-sum
#| echo: true
#| code-fold: show
sum(binary_df$x)
```

Taking the **mean** tells us: **1/3** of rows have `x == 1`:

```{r}
#| label: binary-df-mean
#| echo: true
#| code-fold: show
mean(binary_df$x)
```

:::
:::

## Applying This to the Disease Data {.smaller}

* If we want the **number** of people who have the disease:

```{r}
#| echo: true
#| code-fold: show
#| label: compute-disease-count
# Compute the *number* of people who have the disease
sum(ppl_df$has_disease)
```

* If we want the **proportion** of people who have the disease:

```{r}
#| echo: true
#| code-fold: show
#| label: compute-disease-rate
# Compute the *proportion* of people who have the disease
mean(ppl_df$has_disease)
```

* (And if you dislike scientific notation like I do...)

```{r}
#| label: compute-disease-rate-decimal
#| echo: true
#| code-fold: show
format(mean(ppl_df$has_disease), scientific = FALSE)
```

::: {.notes}
(Foreshadowing Monte Carlo methods)
:::

## Data-Generating Process: Test Results {.smaller}

```{r}
#| label: count-positive-tests
#| echo: true
#| code-fold: show
library(dplyr)
# Data Generating Process
take_test <- function(is_sick) {
  if (is_sick) {
    return(rbinom(1,1,p_pos_given_sick))
  } else {
    return(rbinom(1,1,p_pos_given_healthy))
  }
}
ppl_df['test_result'] <- unlist(lapply(ppl_df$has_disease, take_test))
num_positive <- sum(ppl_df$test_result)
p_positive <- mean(ppl_df$test_result)
writeLines(paste0(num_positive," positive tests / ",num_people," total = ",p_positive))
```

```{r}
#| label: df-test-result-var
#disp(ppl_df %>% head(50), obs_per_page = 3)
ppl_df |> head()
```

## Zooming In On Positive Tests {.smaller}

::: columns
::: {.column width="50%"}

```{r}
#| label: filter-positives-only
pos_ppl <- ppl_df %>% filter(test_result == 1)
#disp(pos_ppl, obs_per_page = 10)
pos_ppl |> head()
```

:::
::: {.column width="50%"}

* Bo doesn't have it, and neither do 110 of the 111 total people who tested positive!
* But, in the real world, we only observe $T$

![](images/pgm_sick.svg){fig-align="center" width="100%"}

:::
:::

## Zooming In On Disease-Havers {.smaller}

* What if we look at only those who actually have the disease? Maybe the cost of 111 people panicking is worth it if we correctly catch those who do have it?

```{r}
#| label: disease-only
#| echo: true
#| code-fold: show
#disp(ppl_df[ppl_df$has_disease == 1,])
ppl_df[ppl_df$has_disease == 1,]
```

Is this always going to be the case?

::: columns
::: {.column width="50%"}

```{r}
#| label: repeating-disease-simulation
#| echo: false
#| code-fold: true
simulate_disease <- function(num_people, p_disease, verbose = TRUE, return_df = TRUE, return_full_df = FALSE, return_all_detected = FALSE, return_info = FALSE) {
  # Give them ids
  sim_df <- tibble(id=seq(1,num_people))
  # Whether they have the disease or not
  has_disease <- rbinom(num_people, 1, p_disease)
  sim_df['has_disease'] <- has_disease
  # Find the number / proportion with the disease
  num_with_disease <- sum(sim_df$has_disease)
  prop_with_disease <- mean(sim_df$has_disease)
  prop_with_disease_decimal <- format(prop_with_disease, scientific = FALSE)
  if (verbose) {
    writeLines(paste0("Num with disease: ",num_with_disease))
    writeLines(paste0("Proportion with disease: ",prop_with_disease_decimal))
  }
  # Simulate giving everyone the test
  sim_df['test_result'] <- unlist(lapply(sim_df$has_disease, take_test))
  if (return_full_df) {
    return(sim_df)
  }
  # Find the number with positive tests
  num_pos_tests <- sum(sim_df$test_result)
  if (verbose) {
    writeLines(paste0("Number of positive tests: ",num_pos_tests))
  }
  # Extract just the people *with* the disease
  disease_df <- sim_df %>% filter(has_disease == 1)
  # And print the proportion of these who got correct result
  if (nrow(disease_df) == 0) {
    # (We define P(correct) as 1 in this case.)
    # (Intuition: we correctly detected "all" 0 people with the disease)
    prop_correct <- 1
  } else {
    prop_correct <- mean(disease_df$test_result)
  }
  #if (verbose) {
  #  writeLines(paste0("Proportion of disease-havers with correct test: ",prop_correct))
  #  print(disease_df)
  #}
  if (return_all_detected) {
    return(prop_correct == 1)
  }
  if (return_df) {
    return(disease_df)
  }
  if (return_info) {
    return(list(
      num_people=num_people,
      df=disease_df,
      prop_correct=prop_correct,
      all_detected=(prop_correct == 1)
    ))
  }
}
#disp(simulate_disease(5000, 1/10000))
simulate_disease(5000, 1/10000)
```

:::
::: {.column width="50%"}

```{r}
#| label: sim-disease-again
#disp(simulate_disease(5000, 1/10000))
simulate_disease(5000, 1/10000)
```

:::
:::

## Worst-Case Worlds {.smaller}

::: columns
::: {.column width="50%"}

```{r}
#| label: worse-case-sim
for (i in seq(1,1000)) {
  sim_result <- simulate_disease(5000, 1/10000, verbose = FALSE, return_all_detected = FALSE, return_df = FALSE, return_info = TRUE)
  if (!sim_result$all_detected) {
    writeLines(paste0("World #",i," / 1000 (",sim_result$num_people," people):"))
    print(sim_result$df)
    writeLines('\n')
  }
}
format(4 / 5000000, scientific = FALSE)
```

:::
::: {.column width="50%"}

How unlikely is this? Math:

$$
\begin{align*}
\Pr(\textsf{T}^- \cap \textsf{Sick}) &= \Pr(\textsf{T}^- \mid \textsf{Sick})\Pr(\textsf{Sick}) \\
&= (0.01)\frac{1}{10000} \\
&= \frac{1}{1000000}
\end{align*}
$$

Computers:

```{r}
#| label: how-unlikely-sim
result_df <- simulate_disease(1000000, 1/10000, verbose = FALSE, return_full_df = TRUE)
false_negatives <- result_df[result_df$has_disease == 1 & result_df$test_result == 0,]
num_false_negatives <- nrow(false_negatives)
writeLines(paste0("False Negatives: ",num_false_negatives,", Total Cases: ", nrow(result_df)))
false_negative_rate <- num_false_negatives / nrow(result_df)
false_negative_rate_decimal <- format(false_negative_rate, scientific = FALSE)
writeLines(paste0("False Negative Rate: ", false_negative_rate_decimal))
```

(Perfect match!)

:::
:::

## Bayes: Takeaway

* Bayesian approach **allows new evidence to be weighed against existing evidence**, with **statistically principled** way to derive these weights:

$$
\begin{array}{ccccc}
\Pr_{\text{post}}(\mathcal{H}) &\hspace{-6mm}\propto &\hspace{-6mm} \Pr(X \mid \mathcal{H}) &\hspace{-6mm} \times &\hspace{-6mm} \Pr_{\text{pre}}(\mathcal{H}) \\
\text{Posterior} &\hspace{-6mm}\propto &\hspace{-6mm}\text{Evidence} &\hspace{-6mm} \times &\hspace{-6mm} \text{Prior}
\end{array}
$$


## Monte Carlo Methods: Overview {data-name="Monte Carlo"}

* You already saw an example, in our rare disease simulation!
* Generally, **using computers** (rather than math, "by hand") **to estimate probabilistic quantities**

::: columns
::: {.column width="50%" .smaller}

**Pros:**

* Most real-world processes have no analytic solution
* Step-by-step breakdown of complex processes

:::
::: {.column width="50%"}

**Cons:**

* Can require immense computing power
* ⚠️ Can generate **incorrect answers** ⚠️

:::
:::

::: {.notes}
By step-by-step I mean, a lot of the time you are just walking through, generating the next column using previously-generated columns. Like we did in the example above, generating `test_result` based on `has_disease`.
:::

## Birthday Problem

::: columns
::: {.column width="50%"}

* 30 people gather in a room together. What is the probability that two of them share the same birthday?
* Analytic solution is fun, but requires some thought... Monte Carlo it!

:::
::: {.column width="50%"}

```{r}
#| label: monty-hall-mc
#| echo: true
#| code-fold: true
gen_bday_room <- function(room_num=NULL) {
  num_people <- 30
  num_days <- 366
  ppl_df <- tibble(id=seq(1,num_people))
birthdays <- sample(1:num_days, num_people,replace = T)
  ppl_df['birthday'] <- birthdays
  if (!is.null(room_num)) {
    ppl_df <- ppl_df %>% mutate(room_num=room_num) %>% relocate(room_num)
  }
  return(ppl_df)
}
ppl_df <- gen_bday_room(1)
#disp(ppl_df %>% head())
ppl_df |> head()
```

:::
:::

## Birthday Problem {.unnumbered}

```{r}
#| label: shared-bdays-fn
# Inefficient version (return_num=FALSE) is for: if you want tibbles of *all* shared bdays for each room
get_shared_bdays <- function(df, is_grouped=NULL, return_num=FALSE, return_bool=FALSE) {
  bday_pairs <- tibble()
  for (i in 1:(nrow(df)-1)) {
    i_data <- df[i,]
    i_bday <- i_data$birthday
    for (j in (i+1):nrow(df)) {
      j_data <- df[j,]
      j_bday <- j_data$birthday
      # Check if they're the same
      same_bday <- i_bday == j_bday
      if (same_bday) {
        if (return_bool) {
          return(1)
        }
        pair_data <- tibble(i=i,j=j,bday=i_bday)
        if (!is.null(is_grouped)) {
          i_room <- i_data$room_num
          pair_data['room'] <- i_room
        }
        bday_pairs <- bind_rows(bday_pairs, pair_data)
      }
    }
  }
  if (return_bool) {
    return(0)
  }
  if (return_num) {
    return(nrow(bday_pairs))
  }
  return(bday_pairs)
}
#get_shared_bdays(ppl_df)
get_shared_bdays(ppl_df)
```

## {.unnumbered .smaller}

Let's try more rooms...

::: columns
::: {.column width="48%"}

```{r,echo=TRUE}
#| label: bday-mc-multiple-rooms
#| echo: true
# Get tibbles for each room
library(purrr)
gen_bday_rooms <- function(num_rooms) {
  rooms_df <- tibble()
  for (r in seq(1, num_rooms)) {
      cur_room <- gen_bday_room(r)
      rooms_df <- bind_rows(rooms_df, cur_room)
  }
  return(rooms_df)
}
num_rooms <- 10
rooms_df <- gen_bday_rooms(num_rooms)
rooms_df %>% group_by(room_num) %>% group_map(~ get_shared_bdays(.x, is_grouped=TRUE))
```

:::
::: {.column width="48%"}

Number of shared birthdays per room:

```{r}
#| label: get-shared-bdays
#| echo: true
# Now just get the # shared bdays
shared_per_room <- rooms_df %>%
    group_by(room_num) %>%
    group_map(~ get_shared_bdays(.x, is_grouped = TRUE, return_num=TRUE))
shared_per_room <- unlist(shared_per_room)
shared_per_room
```

* $\widehat{\Pr}(\text{shared})$

```{r}
#| label: shared-bday-rooms-pct
sum(shared_per_room > 0) / num_rooms
```

:::
:::

## {.unnumbered .smaller}

* How about **A THOUSAND ROOMS**?

```{r}
#| label: bday-1k-rooms
num_rooms_many <- 100
many_rooms_df <- gen_bday_rooms(num_rooms_many)
anyshared_per_room <- many_rooms_df %>%
    group_by(room_num) %>%
    group_map(~ get_shared_bdays(.x, is_grouped = TRUE, return_bool = TRUE))
anyshared_per_room <- unlist(anyshared_per_room)
anyshared_per_room
```

* $\widehat{\Pr}(\text{shared bday})$?

```{r}
#| label: bday-1k-rooms-prob
# And now the probability estimate
sum(anyshared_per_room > 0) / num_rooms_many
```

* The analytic solution: $\Pr(\text{shared} \mid k\text{ people in room}) = 1 - \frac{366!}{366^{k}(366-k)!}$
* In our case: $1 - \frac{366!}{366^{30}(366-30)!} = 1 - \frac{366!}{366^{30}336!} = 1 - \frac{\prod_{i=337}^{366}i}{366^{30}}$
* `R` can juust barely handle these numbers:

```{r}
#| label: bday-exact-soln
(exact_solution <- 1 - (prod(seq(337,366))) / (366^30))
```

## Wrapping Up {.smaller .nostretch}

```{=html}
<style>
#bday-solns-plot {
  width: 50% !important;
}
</style>
```

<center>

{{< embed _bday-solutions-plot.ipynb#bday-solns-plot width="100%" height="75%" >}}

</center>

## Final Note: Functions of Random Variables

* $X \sim U[0,1], Y \sim U[0,1]$.
* $P(Y < X^2)$?
* The hard way: solve analytically
* The easy way: simulate!

# Probability Distributions in General {data-stack-name="Probability Distributions"}

## Discrete vs. Continuous {.smaller .math-80 .crunch-ul .crunch-img .crunch-title .crunch-math .crunch-cell-output}

::: columns
::: {.column width="50%"}

* **Discrete** = "Easy mode": Based (intuitively) on **sets**
* $\Pr(A)$: Four marbles $\{A, B, C, D\}$ in box, all equally likely, what is the probability I pull out $A$?

```{r}
#| label: discrete-prob-plot
#| fig-height: 5.5
library(tibble)
library(ggplot2)
disc_df <- tribble(
  ~x, ~y, ~label,
  0, 0, "A",
  0, 1, "B",
  1, 0, "C",
  1, 1, "D"
)
ggplot(disc_df, aes(x=x, y=y, label=label)) +
    geom_point(size=g_pointsize) +
    geom_text(
      size=g_textsize,
      hjust=1.5,
      vjust=-0.5
    ) +
    xlim(-0.5,1.5) + ylim(-0.5,1.5) +
    coord_fixed() +
    dsan_theme("quarter") +
    labs(
      title="Discrete Probability Space in N"
    )
```

$$
\Pr(A) = \underbrace{\frac{|\{A\}|}{|\Omega|}}_{\mathclap{\small \text{Probability }\textbf{mass}}} = \frac{1}{|\{A,B,C,D\}|} = \frac{1}{4}
$$

:::
::: {.column width="50%"}

* **Continuous** = "Hard mode": Based (intuitively) on **areas**
* $\Pr(A)$: If I throw a dart at this square, what is the probability that I hit region $A$?

```{r}
#| label: continuous-prob-plot
#| fig-height: 6
library(ggforce)
ggplot(disc_df, aes(x=x, y=y, label=label)) +
    xlim(-0.5,1.5) + ylim(-0.5,1.5) +
    geom_rect(aes(xmin = -0.5, xmax = 1.5, ymin = -0.5, ymax = 1.5), fill=cbPalette[1], color="black", alpha=0.3) +
    geom_circle(aes(x0=x, y0=y, r=0.25), fill=cbPalette[2]) +
    coord_fixed() +
    dsan_theme("quarter") +
    geom_text(
      size=g_textsize,
      #hjust=1.75,
      #vjust=-0.75
    ) +
    geom_text(
      data=data.frame(label="Ω"),
      aes(x=-0.4,y=1.39),
      parse=TRUE,
      size=g_textsize
    ) +
    labs(
      title=expression("Continuous Probability Space in "*R^2)
    )
```

$$
\Pr(A) = \underbrace{\frac{\text{Area}(\{A\})}{\text{Area}(\Omega)}}_{\mathclap{\small \text{Probability }\textbf{density}}} = \frac{\pi r^2}{s^2} = \frac{\pi \left(\frac{1}{4}\right)^2}{4} = \frac{\pi}{64}
$$

:::
:::

## The Technical Difference tl;dr {.smaller .smallish-math .crunch-title .crunch-math-less}

* **Countable** Sets: Can be put into 1-to-1 correspondence with the **natural numbers** $\mathbb{N}$
  * What are you doing when you're counting? Saying "first", "second", "third", ...
  * You're pairing each object with a natural number! $\{(\texttt{a},1),(\texttt{b},2),\ldots,(\texttt{z},26)\}$
  <!-- f(\texttt{a}) = 1, f^{-1}(1) = \texttt{a}$, $f(\texttt{b}) = 2, f^{-1}(2) = \texttt{b}$, ... -->
* **Uncountable** Sets: Cannot be put into 1-to-1 correspondence with the natural numbers.
  * **$\mathbb{R}$ is uncountable**. *Intuition*: Try counting the real numbers. *Proof*[^cantor]
  $$
  \text{Assume }\exists (f: \mathbb{R} \leftrightarrow \mathbb{N}) = 
  \begin{array}{|c|c|c|c|c|c|c|}\hline
  \mathbb{R} & & & & & & \Leftrightarrow \mathbb{N} \\ \hline
  \color{orange}{3} & . & 1 & 4 & 1 & \cdots & \Leftrightarrow 1 \\\hline
  4 & . & \color{orange}{9} & 9 & 9 & \cdots & \Leftrightarrow 2 \\\hline
  0 & . & 1 & \color{orange}{2} & 3 & \cdots &\Leftrightarrow 3 \\\hline
  1 & . & 2 & 3 & \color{orange}{4} & \cdots & \Leftrightarrow 4 \\\hline
 \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\\hline
  \end{array} \overset{\color{blue}{y_{[i]}} = \color{orange}{x_{[i]}} \overset{\mathbb{Z}_{10}}{+} 1}{\longrightarrow} \color{blue}{y = 4.035 \ldots} \Leftrightarrow \; ?
  $$

[^cantor]: The method used in this proof, if you haven't seen it before, is called [**Cantor diagonalization**](https://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument){target="_blank"}, and it is extremely fun and applicable to a wide variety of levels-of-infinity proofs

::: {.aside}
**Fun math challenge**: Is $\mathbb{Q}$ countable? See [this appendix slide](#appendix-countability-of-mathbbq) for why the answer is yes, despite the fact that $\forall x, y \in \mathbb{Q} \left[ \frac{x+y}{2} \in \mathbb{Q} \right]$
:::

## The Practical Difference {.smaller}

* This part of the course (**discrete** probability): $\Pr(X = v), v \in \mathcal{R}_X \subseteq \mathbb{N}$
  * Example: $\Pr($<span><i class="bi bi-dice-3"></i></span>$) = \Pr(X = 3), 3 \in \{1,2,3,4,5,6\} \subseteq \mathbb{N}$
* Next part of the course (**continuous** probability): $\Pr(X \in V), v \subseteq \mathbb{R}$
  * Example: $\Pr(X \geq 2\pi) = \Pr(X \in [\pi,\infty)), [\pi,\infty) \subseteq \mathbb{R}$
* Why do they have to be in separate parts?

$$
\Pr(X = 2\pi) = \frac{\text{Area}(\overbrace{2\pi}^{\mathclap{\small \text{Single point}}})}{\text{Area}(\underbrace{\mathbb{R}}_{\mathclap{\small \text{(Uncountably) Infinite set of points}}})} = 0
$$


## Probability Mass vs. Probability Density {.smaller .small-title}

* **Cumulative Distribution Function** (**CDF**): $F_X(v) = \Pr(X \leq v)$
* For **discrete** RV $X$, **Probability Mass Function** (**pmf**) $p_X(v)$:
  $$
  \begin{align*}
  p_X(v) &= \Pr(X = v) = F_X(v) - F_X(v-1) \\
  \implies F_X(v) &= \sum_{\{w \in \mathcal{R}_X: \; w \leq v\}}p_X(w)
  \end{align*}
  $$
* For **continuous** RV $X$ ($\mathcal{R}_X \subseteq \mathbb{R}$), **Probability Density Function** (**pdf**) $f_X(v)$:
  $$
  \begin{align*}
  f_X(v) &= \frac{d}{dx}F_X(v) \\
  \implies F_X(v) &= \int_{-\infty}^v f_X(w)dw
  \end{align*}
  $$

::: {.aside}

Frustratingly, the CDF/pmf/pdf is usually written using $X$ and $x$, like $F_X(x) = \Pr(X \leq x)$. To me this is extremely confusing, since the capitalized $X$ is a random variable (not a number) while the lowercase $x$ is some particular value, like $3$. So, to emphasize this difference, I use $X$ for the RV and $v$ for the **value** at which we're checking the CDF/pmf/pdf.

Also note the capitalized CDF but lowercase pmf/pdf, matching the mathematical notation where $f_X(v)$ is the derivative of $F_X(v)$.

:::

## Probability Density $\neq$ Probability {.smaller}

* **BEWARE**: $f_X(v) \neq \Pr(X = v)$!
* Long story short, for continuous variables, $\Pr(X = v) = 0$[^measurezero]
* Hence, we instead construct a PDF $f_X(v)$ that enables us to calculate $\Pr(X \in [a,b])$ by integrating: $f_X(v)$ is whatever function satisfies $\Pr(X \in [a,b]) = \int_{a}^bf_X(v)dv$.
* i.e., instead of $p_X(v) = \Pr(X = v)$ from discrete world, the relevant function here is $f_X(v)$, the probability **density** of $X$ at $v$.
* If we **really** want to get something like the "probability of a value" in a continuous space 😪, we can get something kind of like this by using fancy limits
  $$
  f_X(v) = \lim_{\varepsilon \to 0}\frac{P(X \in [v-\varepsilon, v + \varepsilon])}{2\varepsilon} = \lim_{\varepsilon \to 0}\frac{F(v + \varepsilon) - F(v - \varepsilon)}{2\varepsilon} = \frac{d}{dx}F_X(v)
  $$

[^measurezero]: For intuition: $X \sim U[0,10] \implies \Pr(X = \pi) = \frac{|\{v \in \mathbb{R}:\; v = \pi\}|}{|\mathbb{R}|} = \frac{1}{2^{\aleph_0}} \approx 0$. That is, finding the $\pi$ needle in the $\mathbb{R}$ haystack is a one-in-$\left(\infty^\infty\right)$ event. A similar issue occurs if $S$ is countably-infinite, like $S = \mathbb{N}$: $\Pr(X = 3) = \frac{|\{x \in \mathbb{N} : \; x = 3\}|}{|\mathbb{N}|} = \frac{1}{\aleph_0}$.

# Common Discrete Distributions {data-stack-name="Discrete Distributions"}

* Bernoulli
* Binomial
* Geometric

## Bernoulli Distribution

* Single trial with two outcomes, **"success"** (**1**) or **"failure"** (**0**): basic model of a **coin flip** (heads = 1, tails = 0)
* $X \sim \text{Bern}({\color{purple} p}) \implies \mathcal{R}_X = \{0,1\}, \; \Pr(X = 1) = {\color{purple}p}$.

```{r}
#| label: bernoulli-demo
#| fig-align: center
library(ggplot2)
library(tibble)
bern_tibble <- tribble(
  ~Outcome, ~Probability, ~Color,
  "Failure", 0.2, cbPalette[1],
  "Success", 0.8, cbPalette[2]
)
ggplot(data = bern_tibble, aes(x=Outcome, y=Probability)) +
  geom_bar(aes(fill=Outcome), stat = "identity") +
  dsan_theme("half") +
  labs(
    y = "Probability Mass"
  ) +
  scale_fill_manual(values=c(cbPalette[1], cbPalette[2])) +
  remove_legend()
```

## Binomial Distribution

* **Number of successes** in ${\color{purple}N}$ Bernoulli trials. $X \sim \text{Binom}({\color{purple}N},{\color{purple}k},{\color{purple}p}) \implies \mathcal{R}_X = \{0, 1, \ldots, N\}$
  * $P(X = k)  = \binom{N}{k}p^k(1-p)^{N-k}$: probability of $k$ successes out of $N$ trials.
  * $\binom{N}{k} = \frac{N!}{k!(N-k)!}$: "Binomial coefficient". How many groups of size $k$ can be formed?[^binom]

[^binom]: A fun way to never have to memorize or compute these: imagine a pyramid like $\genfrac{}{}{0pt}{}{}{\boxed{\phantom{1}}}\genfrac{}{}{0pt}{}{\boxed{\phantom{1}}}{}\genfrac{}{}{0pt}{}{}{\boxed{\phantom{1}}}$, where the boxes are slots for numbers, and put a $1$ in the box at the top. In the bottom row, fill each slot with the sum of the two numbers above-left and above-right of it. Since $1 + \text{(nothing)} = 1$, this looks like: $\genfrac{}{}{0pt}{}{}{1}\genfrac{}{}{0pt}{}{1}{}\genfrac{}{}{0pt}{}{}{1}$. Continue filling in the pyramid this way, so the next row looks like $\genfrac{}{}{0pt}{}{}{1}\genfrac{}{}{0pt}{}{1}{}\genfrac{}{}{0pt}{}{}{2}\genfrac{}{}{0pt}{}{1}{}\genfrac{}{}{0pt}{}{}{1}$, then $\genfrac{}{}{0pt}{}{}{1}\genfrac{}{}{0pt}{}{1}{}\genfrac{}{}{0pt}{}{}{3}\genfrac{}{}{0pt}{}{2}{}\genfrac{}{}{0pt}{}{}{3}\genfrac{}{}{0pt}{}{1}{}\genfrac{}{}{0pt}{}{}{1}$, and so on. The $k$th number in the $N$th row (counting from $0$) is $\binom{N}{k}$. For the triangle written out to the 7th row, see Appendix I at end of slideshow.

## Visualizing the Binomial {.smaller}

```{r}
#| label: binomial-demo
#| echo: true
#| code-fold: show
#| fig-align: center
k <- seq(0, 10)
prob <- dbinom(k, 10, 0.5)
bar_data <- tibble(k, prob)
ggplot(bar_data, aes(x=k, y=prob)) +
  geom_bar(stat="identity", fill=cbPalette[1]) +
  labs(
    title="Binomial Distribution, N = 10, p = 0.5",
    y="Probability Mass"
  ) +
  scale_x_continuous(breaks=seq(0,10)) +
  dsan_theme("half")
```

::: {.notes}
So who can tell me, from this plot, the approximate probability of getting 4 heads when flipping a coin 10 times?
:::


## Multiple Classes: Multinomial Distribution {.smaller}

* Bernoulli only allows two outcomes: success or failure.
* What if we're predicting soccer match outcomes?
  * $X_i \in \{\text{Win}, \text{Loss}, \text{Draw}\}$
* **Categorical Distribution**: Generalization of Bernoulli to $k$ outcomes. $X \sim \text{Categorical}(\mathbf{p} = \{p_1, p_2, \ldots, p_k\}), \sum_{i=1}^kp_i = 1$.
  * $P(X = k) = p_k$
* **Multinomial Distribution**: Generalization of Binomial to $k$ outcomes.
* $\mathbf{X} \sim \text{Multinom}(N,k,\mathbf{p}=\{p_1,p_2,\ldots,p_k\}), \sum_{i=1}^kp_i=1$
  * $P(\mathbf{X} = \{x_1,x_2\ldots,x_k\}) = \frac{N!}{x_1!x_2!\cdots x_k!}p_1^{x_1}p_2^{x_2}\cdots p_k^{x_k}$
  * $P(\text{30 wins}, \text{4 losses}, \text{4 draws}) = \frac{38!}{30!4!4!}p_{\text{win}}^{30}p_{\text{lose}}^4p_{\text{draw}}^4$.

## Geometric Distribution

* **Geometric**: Likelihood that we need ${\color{purple}k}$ trials to get our first success. $X \sim \text{Geom}({\color{purple}k},{\color{purple}p}) \implies \mathcal{R}_X = \{0, 1, \ldots\}$
  * $P(X = k) = \underbrace{(1-p)^{k-1}}_{\small k - 1\text{ failures}}\cdot \underbrace{p}_{\mathclap{\small \text{success}}}$
  * Probability of $k-1$ failures followed by a success

```{r}
#| label: geometric-demo
#| fig-align: center
library(ggplot2)
k <- seq(0, 8)
prob <- dgeom(k, 0.5)
bar_data <- tibble(k, prob)
ggplot(bar_data, aes(x = k, y = prob)) +
    geom_bar(stat = "identity", fill = cbPalette[1]) +
    labs(
        title = "Geometric Distribution, p = 0.5",
        y = "Probability Mass"
    ) +
    scale_x_continuous(breaks = seq(0, 8)) +
    dsan_theme("half")
```

## Less Common (But Important) Distributions {.smaller}

* **Discrete Uniform**: $N$ equally-likely outcomes
  * $X \sim U\{{\color{purple}a},{\color{purple}b}\} \implies \mathcal{R}_X = \{a, a+1, \ldots, b\}, P(X = k) = \frac{1}{{\color{purple}b} - {\color{purple}a} + 1}$
* **Beta**: $X \sim \text{Beta}({\color{purple}\alpha}, {\color{purple}\beta})$: *conjugate prior* for Bernoulli, Binomial, and Geometric dists.
  * Intuition: If we use Beta to encode our prior hypothesis, then observe data drawn from Binomial, distribution of our *updated* hypothesis is still Beta.
  * $\underbrace{\Pr(\text{biased}) = \Pr(\text{unbiased})}_{\text{Prior: }\text{Beta}({\color{purple}\alpha}, {\color{purple}\beta})} \rightarrow$ Observe $\underbrace{\frac{8}{10}\text{ heads}}_{\text{Data}} \rightarrow \underbrace{\Pr(\text{biased}) = 0.65}_{\text{Posterior: }\text{Beta}({\color{purple}\alpha + 8}, {\color{purple}\beta + 2})}$
* **Dirichlet**: $\mathbf{X} = (X_1, X_2, \ldots, X_K) \sim \text{Dir}({\color{purple} \boldsymbol\alpha})$
  * $K$-dimensional extension of Beta (thus, conjugate prior for Multinomial)

<!-- TODO: see why \param doesn't work for the bolded \alpha -->

::: {.notes}
We can now use $\text{Beta}(\alpha + 8, \beta + 2)$ as a prior for our next set of trials (encoding our knowledge up to that point), and update further once we know the results (to yet another Beta distribution).
:::

## Interactive Visualizations!

<a href="https://seeing-theory.brown.edu/probability-distributions/index.html" target="_blank">Seeing Theory</a>, Brown University

<!-- APPENDIX SLIDES -->


## Appendix: Countability of $\mathbb{Q}$ {.smaller .small-math}

* **Bad** definition: "$\mathbb{N}$ is countable because no $x \in \mathbb{N}$ between $0$ and $1$. $\mathbb{R}$ is uncountable because infinitely-many $x \in \mathbb{R}$ between $0$ and $1$." ($\implies \mathbb{Q}$ uncountable)
* And yet, $\mathbb{Q}$ **is** countable...

::: columns
::: {.column width="45%"}

![](images/rationals_countable.png){fig-align="center"}

:::
::: {.column width="55%"}

$$
\begin{align*}
\begin{array}{ll}
s: \mathbb{N} \leftrightarrow \mathbb{Z} & s(n) = (-1)^n \left\lfloor \frac{n+1}{2} \right\rfloor \\
h_+: \mathbb{Z}^+ \leftrightarrow \mathbb{Q}^+ & p_1^{a_1}p_2^{a_2}\cdots \mapsto p_1^{s(a_1)}p_2^{s(a_2)}\cdots \\
h: \mathbb{Z} \leftrightarrow \mathbb{Q} & h(n) = \begin{cases}h_+(n) &n > 0 \\ 0 & n = 0 \\
-h_+(-n) & n < 0\end{cases} \\
(h \circ s): \mathbb{N} \leftrightarrow \mathbb{Q} & ✅🤯
\end{array}
\end{align*}
$$

:::
:::

::: {.aside}
Image credit: <a href="https://math.stackexchange.com/a/659373" target="_blank">Rebecca J. Stones, Math StackExchange</a>. Math credit: <a href="https://math.stackexchange.com/a/1067928" target="_blank">Thomas Andrews, Math StackExchange</a>
:::
